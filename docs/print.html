<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>AWS Graviton technical guide</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="Technical guidance for developers to use the Arm based AWS Graviton instances">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">AWS Graviton technical guide</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/aws/aws-graviton-getting-started" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="aws-graviton-technical-guide"><a class="header" href="#aws-graviton-technical-guide">AWS Graviton Technical Guide</a></h1>
<p>This repository provides technical guidance for users and developers using <a href="https://aws.amazon.com/ec2/graviton/">Amazon EC2 instances powered by AWS Graviton processors</a> (including the latest generation Graviton4 processors). While it calls out specific features of the Graviton processors themselves, this repository is also generally useful for anyone running code on Arm-based systems.</p>
<h1 id="contents"><a class="header" href="#contents">Contents</a></h1>
<ul>
<li><a href="index.html#transitioning-to-graviton">Transitioning to Graviton</a></li>
<li><a href="index.html#building-for-graviton2-graviton3-and-graviton3e">Building for Graviton</a></li>
<li><a href="optimizing.html">Optimizing for Graviton</a></li>
<li><a href="SIMD_and_vectorization.html">Taking advantage of Arm Advanced SIMD instructions</a></li>
<li><a href="index.html#recent-software-updates-relevant-to-graviton">Recent software updates relevant to Graviton</a></li>
<li>Language-specific considerations
<ul>
<li><a href="c-c++.html">C/C++</a></li>
<li><a href="golang.html">Go</a></li>
<li><a href="java.html">Java</a></li>
<li><a href="dotnet.html">.NET</a></li>
<li><a href="nodejs.html">Node.JS</a></li>
<li><a href="php.html">PHP</a></li>
<li><a href="python.html">Python</a></li>
<li><a href="rust.html">Rust</a></li>
</ul>
</li>
<li><a href="containers.html">Containers on Graviton</a></li>
<li><a href="software/ChromeAndPuppeteer.html">Headless website testing with Chrome and Puppeteer on Graviton</a></li>
<li><a href="index.html#lambda-on-graviton">Lambda on Graviton</a></li>
<li><a href="os.html">Operating Systems support</a></li>
<li><a href="isv.html">Third-party Software Vendors</a></li>
<li><a href="amis_cf_sm.html">Finding and managing AMIs for Graviton, with AWS SystemManager or CloudFormation</a></li>
<li><a href="dpdk_spdk.html">DPDK, SPDK, and other datapath software</a></li>
<li><a href="machinelearning/pytorch.html">PyTorch</a></li>
<li><a href="machinelearning/llama.cpp.html">llama.cpp</a></li>
<li><a href="R.html">R</a></li>
<li><a href="machinelearning/tensorflow.html">TensorFlow</a></li>
<li><a href="DataAnalytics.html">Spark on Graviton</a></li>
<li><a href="index.html#known-issues-and-workarounds">Known issues and workarounds</a></li>
<li><a href="managed_services.html">AWS Managed Services available on Graviton</a></li>
<li><a href="perfrunbook/README.html">Graviton Performance Runbook</a></li>
<li><a href="arm64-assembly-optimization.html">Assembly Optimization Guide for Graviton Arm64 Processors</a></li>
<li><a href="index.html#additional-resources">Additional resources</a></li>
<li><a href="howtoresources.html">How To Resources</a></li>
<li><a href="index.html#blog-posts">Blog Posts</a></li>
<li><a href="index.html#case-studies">Case Studies</a></li>
</ul>
<h1 id="transitioning-to-graviton"><a class="header" href="#transitioning-to-graviton">Transitioning to Graviton</a></h1>
<p>If you are new to Graviton and want to understand how to identify target workloads, how to plan a transition project, how to test your workloads on AWS Graviton and finally how deploy in production, please read <a href="transition-guide.html">the key considerations to take into account when transitioning workloads to AWS Graviton based Amazon EC2 instances</a>.</p>
<h1 id="building-for-graviton"><a class="header" href="#building-for-graviton">Building for Graviton</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Processor</th><th>Graviton2</th><th>Graviton3(E)</th><th>Graviton4</th></tr></thead><tbody>
<tr><td>Instances</td><td><a href="https://aws.amazon.com/ec2/instance-types/m6g/">M6g/M6gd</a>, <a href="https://aws.amazon.com/ec2/instance-types/c6g/">C6g/C6gd/C6gn</a>, <a href="https://aws.amazon.com/ec2/instance-types/r6g/">R6g/R6gd</a>, <a href="https://aws.amazon.com/ec2/instance-types/t4g">T4g</a>, <a href="https://aws.amazon.com/ec2/instance-types/x2/">X2gd</a>, <a href="https://aws.amazon.com/ec2/instance-types/g5g/">G5g</a>, and <a href="https://aws.amazon.com/ec2/instance-types/i4g/">I4g/Im4gn/Is4gen</a></td><td><a href="https://aws.amazon.com/ec2/instance-types/c7g/">C7g/C7gd/C7gn</a>, <a href="https://aws.amazon.com/ec2/instance-types/m7g/">M7g/M7gd</a>, <a href="https://aws.amazon.com/ec2/instance-types/r7g/">R7g/R7gd</a>, and <a href="https://aws.amazon.com/ec2/instance-types/hpc7g/">Hpc7g</a></td><td><a href="https://aws.amazon.com/ec2/instance-types/c8g/">C8g</a>, <a href="https://aws.amazon.com/ec2/instance-types/m8g/">M8g</a>, <a href="https://aws.amazon.com/ec2/instance-types/r8g/">R8g</a>, <a href="https://aws.amazon.com/ec2/instance-types/x8g/">X8g</a>, and <a href="https://aws.amazon.com/ec2/instance-types/i8g/">I8g</a></td></tr>
<tr><td>Core</td><td><a href="https://developer.arm.com/documentation/100616/0301">Neoverse-N1</a></td><td><a href="https://developer.arm.com/documentation/101427/latest/">Neoverse-V1</a></td><td><a href="https://developer.arm.com/documentation/102375/latest">Neoverse-V2</a></td></tr>
<tr><td>Frequency</td><td>2500MHz</td><td>2600MHz</td><td>2800MHz (2700MHz for 48xlarge)</td></tr>
<tr><td>Turbo supported</td><td>No</td><td>No</td><td>No</td></tr>
<tr><td>Software Optimization Guide (Instruction Throughput and Latency)</td><td><a href="https://developer.arm.com/documentation/pjdoc466751330-9707/latest/">SWOG</a></td><td><a href="https://developer.arm.com/documentation/pjdoc466751330-9685/latest/">SWOG</a></td><td><a href="https://developer.arm.com/documentation/PJDOC-466751330-593177/latest/">SWOG</a></td></tr>
<tr><td>Interconnect</td><td>CMN-600</td><td>CMN-650</td><td>CMN-700</td></tr>
<tr><td>Architecture revision</td><td>ARMv8.2-a</td><td>ARMv8.4-a</td><td>Armv9.0-a</td></tr>
<tr><td><a href="https://developer.arm.com/documentation/ka006014/latest/">32bit Backward Compatibility</a></td><td>Userspace only</td><td>Userspace only</td><td>No support</td></tr>
<tr><td>Additional  features</td><td>fp16, rcpc, dotprod, crypto</td><td>sve, rng, bf16, int8</td><td>sve2, sve-int8, sve-bf16, sve-bitperm, sve-crypto</td></tr>
<tr><td>Recommended <code>-mcpu</code> flag	(<a href="c-c++.html#enabling-arm-architecture-specific-features">more information</a>)</td><td><code>neoverse-n1</code></td><td><code>neoverse-512tvb</code></td><td><code>neoverse-512tvb</code></td></tr>
<tr><td>RNG Instructions</td><td>No</td><td>Yes</td><td>Yes</td></tr>
<tr><td>SIMD instructions</td><td>2x Neon 128bit vectors</td><td>4x Neon 128bit vectors / 2x SVE 256bit</td><td>4x Neon/SVE 128bit vectors</td></tr>
<tr><td>LSE (atomic mem operations)</td><td>yes</td><td>yes</td><td>yes</td></tr>
<tr><td>Pointer Authentication</td><td>no</td><td>yes</td><td>yes</td></tr>
<tr><td>Branch Target Identification</td><td>no</td><td>no</td><td>yes</td></tr>
<tr><td>Cores</td><td>64</td><td>64</td><td>96 per socket (192 for 2-socket 48xlarge)</td></tr>
<tr><td>L1 cache (per core)</td><td>64kB inst / 64kB data</td><td>64kB inst / 64kB data</td><td>64kB inst / 64kB data</td></tr>
<tr><td>L2 cache (per core)</td><td>1MB</td><td>1MB</td><td>2MB</td></tr>
<tr><td>LLC (shared)</td><td>32MB</td><td>32MB</td><td>36MB</td></tr>
<tr><td>Memory (NUMA) nodes</td><td>1</td><td>1</td><td>1  (2 for 48xlarge)</td></tr>
<tr><td>DRAM</td><td>8x DDR4</td><td>8x DDR5</td><td>12x DDR5 (24x for 48xlarge)</td></tr>
<tr><td>DDR Encryption</td><td>yes</td><td>yes</td><td>yes</td></tr>
</tbody></table>
</div>
<h1 id="optimizing-for-graviton"><a class="header" href="#optimizing-for-graviton">Optimizing for Graviton</a></h1>
<p>Please refer to <a href="optimizing.html">optimizing</a> for general debugging and profiling information.  For detailed checklists on optimizing and debugging performance on Graviton, see our <a href="perfrunbook/README.html">performance runbook</a>.</p>
<p>Different architectures and systems have differing capabilities, which means some tools you might be familiar with on one architecture don't have equivalent on AWS Graviton. Documented <a href="Monitoring_Tools_on_Graviton.html">Monitoring Tools</a> with some of these utilities.</p>
<p>Furthermore, different generations of Graviton support different features, as noted in the table above. For example,
Graviton3 supports SVE but Graviton2 does not. Graviton4 supports SVE2 and SVE. For some applications it may be
advantageous to implement performance critical kernels to make use of the highest performing feature set available which
may not be known until runtime. For this, the best practice is to consult HWCAPS. For details on how to do this, see
<a href="runtime-feature-detection.html">our guide on HWCAPS runtime feature detection</a>.</p>
<h1 id="recent-software-updates-relevant-to-graviton"><a class="header" href="#recent-software-updates-relevant-to-graviton">Recent software updates relevant to Graviton</a></h1>
<p>There is a huge amount of activity in the Arm software ecosystem and improvements are being
made on a daily basis. As a general rule later versions of compilers, language runtimes, and applications
should be used whenever possible. The table below includes known recent changes to popular
packages that improve performance (if you know of others please let us know).</p>
<div class="table-wrapper"><table><thead><tr><th>Package</th><th style="text-align: center">Version</th><th>Improvements</th></tr></thead><tbody>
<tr><td>bazel</td><td style="text-align: center"><a href="https://github.com/bazelbuild/bazel/releases">3.4.1+</a></td><td>Pre-built bazel binary for Graviton/Arm64. <a href="index.html#bazel-on-linux">See below</a> for installation.</td></tr>
<tr><td>Cassandra</td><td style="text-align: center">4.0+</td><td>Supports running on Java/Corretto 11, improving overall performance</td></tr>
<tr><td>FFmpeg</td><td style="text-align: center">6.0+</td><td>Improved performance of libswscale by 50% with better NEON vectorization which improves the performance and scalability of FFmpeg multi-threaded encoders. The changes are available in FFmpeg version 4.3, with further improvements to scaling and motion estimation available in 5.1. Additional improvements to both are available in 6. For encoding h.265, build with the master branch of x265 because the released version of 3.5 does not include important optimizations for Graviton. For more information about FFmpeg on Graviton, read the blog post on AWS Open Source Blog, <a href="https://aws.amazon.com/blogs/opensource/optimized-video-encoding-with-ffmpeg-on-aws-graviton-processors/">Optimized Video Encoding with FFmpeg on AWS Graviton Processors</a>.</td></tr>
<tr><td>HAProxy</td><td style="text-align: center">2.4+</td><td>A <a href="https://github.com/haproxy/haproxy/issues/958">serious bug</a> was fixed. Additionally, building with <code>CPU=armv81</code> improves HAProxy performance by 4x so please rebuild your code with this flag.</td></tr>
<tr><td>MariaDB</td><td style="text-align: center">10.4.14+</td><td>Default build now uses -moutline-atomics, general correctness bugs for Graviton fixed.</td></tr>
<tr><td>mongodb</td><td style="text-align: center">4.2.15+ / 4.4.7+ / 5.0.0+</td><td>Improved performance on graviton, especially for internal JS engine. LSE support added in <a href="https://jira.mongodb.org/browse/SERVER-56347">SERVER-56347</a>.</td></tr>
<tr><td>MySQL</td><td style="text-align: center">8.0.23+</td><td>Improved spinlock behavior, compiled with -moutline-atomics if compiler supports it.</td></tr>
<tr><td>PostgreSQL</td><td style="text-align: center">15+</td><td>General scalability improvements plus additional <a href="https://commitfest.postgresql.org/37/3527/">improvements to spin-locks specifically for Arm64</a></td></tr>
<tr><td>.NET</td><td style="text-align: center"><a href="https://dotnet.microsoft.com/download/dotnet/5.0">5+</a></td><td><a href="https://devblogs.microsoft.com/dotnet/Arm64-performance-in-net-5/">.NET 5 significantly improved performance for ARM64</a>. Here's an associated <a href="https://aws.amazon.com/blogs/compute/powering-net-5-with-aws-graviton2-benchmark-results/">AWS Blog</a> with some performance results.</td></tr>
<tr><td>OpenH264</td><td style="text-align: center"><a href="https://github.com/cisco/openh264/releases/tag/v2.1.1">2.1.1+</a></td><td>Pre-built Cisco OpenH264 binary for Graviton/Arm64.</td></tr>
<tr><td>PCRE2</td><td style="text-align: center">10.34+</td><td>Added NEON vectorization to PCRE's JIT to match first and pairs of characters. This may improve performance of matching by up to 8x. This fixed version of the library now is shipping with Ubuntu 20.04 and PHP 8.</td></tr>
<tr><td>PHP</td><td style="text-align: center">7.4+</td><td>PHP 7.4 includes a number of performance improvements that increase perf by up to 30%</td></tr>
<tr><td>pip</td><td style="text-align: center">19.3+</td><td>Enable installation of python wheel binaries on Graviton</td></tr>
<tr><td>PyTorch</td><td style="text-align: center">2.0+</td><td>Optimize Inference latency and throughput on Graviton. <a href="machinelearning/pytorch.html">AWS DLCs and python wheels are available</a>.</td></tr>
<tr><td>ruby</td><td style="text-align: center">3.0+</td><td>Enable arm64 optimizations that improve performance by as much as 40%. These changes have also been back-ported to the Ruby shipping with AmazonLinux2, Fedora, and Ubuntu 20.04.</td></tr>
<tr><td>Spark</td><td style="text-align: center">3.0+</td><td>Supports running on Java/Corretto 11, improving overall performance.</td></tr>
<tr><td>zlib</td><td style="text-align: center">1.2.8+</td><td>For the best performance on Graviton please use <a href="https://github.com/cloudflare/zlib">zlib-cloudflare</a>.</td></tr>
</tbody></table>
</div>
<h1 id="containers-on-graviton"><a class="header" href="#containers-on-graviton">Containers on Graviton</a></h1>
<p>You can run Docker, Kubernetes, Amazon ECS, and Amazon EKS on Graviton. Amazon ECR supports multi-arch containers.
Please refer to <a href="containers.html">containers</a> for information about running container-based workloads on Graviton.</p>
<h1 id="lambda-on-graviton"><a class="header" href="#lambda-on-graviton"><a href="/aws-lambda/README.html">Lambda on Graviton</a></a></h1>
<p><a href="https://aws.amazon.com/lambda/">AWS Lambda</a> now allows you to configure new and existing functions to run on Arm-based AWS Graviton2 processors in addition to x86-based functions. Using this processor architecture option allows you to get up to 34% better price performance. Duration charges are 20 percent lower than the current pricing for x86 with <a href="https://aws.amazon.com/blogs/aws/new-for-aws-lambda-1ms-billing-granularity-adds-cost-savings/">millisecond granularity</a>. This also applies to duration charges when using <a href="https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/">Provisioned Concurrency</a>. Compute <a href="https://aws.amazon.com/blogs/aws/savings-plan-update-save-up-to-17-on-your-lambda-workloads/">Savings Plans</a> supports Lambda functions powered by Graviton2.</p>
<p>The <a href="/aws-lambda/README.html">Lambda</a> page highlights some of the migration considerations and also provides some simple to deploy demos you can use to explore how to build and migrate to Lambda functions using Arm/Graviton2.</p>
<h1 id="operating-systems"><a class="header" href="#operating-systems">Operating Systems</a></h1>
<p>Please check <a href="os.html">os.md</a> for more information about which operating system to run on Graviton based instances.</p>
<h1 id="known-issues-and-workarounds"><a class="header" href="#known-issues-and-workarounds">Known issues and workarounds</a></h1>
<h2 id="postgres"><a class="header" href="#postgres">Postgres</a></h2>
<p>Postgres performance can be heavily impacted by not using <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/c-c%2B%2B.md#large-system-extensions-lse">LSE</a>.
Today, postgres binaries from distributions (e.g. Ubuntu) are not built with <code>-moutline-atomics</code> or <code>-march=armv8.2-a</code> which would enable LSE.  Note: Amazon RDS for PostgreSQL isn't impacted by this.</p>
<p>In November 2021 PostgreSQL started to distribute Ubuntu 20.04 packages optimized with <code>-moutline-atomics</code>.
For Ubuntu 20.04, we recommend using the PostgreSQL PPA instead of the packages distributed by Ubuntu Focal.
Please follow <a href="https://www.postgresql.org/download/linux/ubuntu/">the instructions to set up the PostgreSQL PPA.</a></p>
<h2 id="python-installation-on-some-linux-distros"><a class="header" href="#python-installation-on-some-linux-distros">Python installation on some Linux distros</a></h2>
<p>The default installation of pip on some Linux distributions is old (&lt;19.3) to install binary wheel packages released for Graviton.  To work around this, it is recommended to upgrade your pip installation using:</p>
<pre><code>sudo python3 -m pip install --upgrade pip
</code></pre>
<h2 id="bazel-on-linux"><a class="header" href="#bazel-on-linux">Bazel on Linux</a></h2>
<p>The <a href="https://www.bazel.build/">Bazel build tool</a> now releases a pre-built binary for arm64. As of October 2020, this is not available in their custom Debian repo, and Bazel does not officially provide an RPM. Instead, we recommend using the <a href="https://docs.bazel.build/versions/master/install-bazelisk.html">Bazelisk installer</a>, which will replace your <code>bazel</code> command and <a href="https://github.com/bazelbuild/bazelisk/blob/master/README.md">keep bazel up to date</a>.</p>
<p>Below is an example using the <a href="https://github.com/bazelbuild/bazelisk/releases/latest">latest Arm binary release of Bazelisk</a> as of October 2020:</p>
<pre><code>wget https://github.com/bazelbuild/bazelisk/releases/download/v1.7.1/bazelisk-linux-arm64
chmod +x bazelisk-linux-arm64
sudo mv bazelisk-linux-arm64 /usr/local/bin/bazel
bazel
</code></pre>
<p>Bazelisk itself should not require further updates, as its only purpose is to keep Bazel updated.</p>
<h2 id="zlib-on-linux"><a class="header" href="#zlib-on-linux">zlib on Linux</a></h2>
<p>Linux distributions, in general, use the original zlib without any optimizations. zlib-cloudflare has been updated to provide better and faster compression on Arm and x86. To use zlib-cloudflare:</p>
<pre><code>git clone https://github.com/cloudflare/zlib.git
cd zlib
./configure --prefix=$HOME
make
make install
</code></pre>
<p>Make sure to have the full path to your lib at $HOME/lib in /etc/ld.so.conf and run ldconfig.</p>
<p>For JDKs that dynamically link to the system zlib, you can set LD_LIBRARY_PATH to point to the directory where your newly built version of zlib-cloudflare is located or load that library with LD_PRELOAD.</p>
<p>You can check the libz that JDK is dynamically linked against with:</p>
<pre><code>$ ldd /Java/jdk-11.0.8/lib/libzip.so | grep libz
libz.so.1 =&gt; /lib/x86_64-linux-gnu/libz.so.1 (0x00007ffff7783000)
</code></pre>
<p>NOTE: Linux versions of OpenJDK and Amazon Corretto 17+ dynamically link to zlib. Earlier versions of Corretto bundled zlib directly and this behavior can vary by JDK vendor and platform.</p>
<h1 id="blog-posts"><a class="header" href="#blog-posts">Blog Posts</a></h1>
<h2 id="hpc"><a class="header" href="#hpc">HPC</a></h2>
<ul>
<li><a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/">Application deep-dive into the AWS Graviton3E-based Amazon EC2 Hpc7g instance</a></li>
<li><a href="https://rescale.com/blog/rescale-automates-the-deployment-of-ansys-ls-dyna-and-ansys-fluent-workloads-on-amazon-ec2-hpc7g-instances/">Rescale Automates the Deployment of Ansys LS-DYNA and Ansys Fluent Workloads on Amazon EC2 Hpc7g Instances</a></li>
<li><a href="https://aws.amazon.com/blogs/hpc/lattice-boltzmann-simulation-with-palabos-on-aws-using-graviton-based-amazon-ec2-hpc7g-instances/">Lattice Boltzmann simulation with Palabos on AWS using Graviton-based Amazon EC2 Hpc7g instances</a></li>
<li><a href="https://aws.amazon.com/blogs/hpc/instance-sizes-in-the-amazon-ec2-hpc7-family-a-different-experience/">Instance sizes in the Amazon EC2 Hpc7 family – a different experience</a></li>
<li><a href="https://aws.amazon.com/blogs/hpc/checkpointing-hpc-applications-using-the-spot-instance-two-minute-notification-from-amazon-ec2/">Checkpointing HPC applications using the Spot Instance two-minute notification from Amazon EC2</a></li>
<li><a href="https://aws.amazon.com/blogs/hpc/best-practices-for-running-molecular-dynamics-simulations-on-aws-graviton3e/">Best practices for running molecular dynamics simulations on AWS Graviton3E</a></li>
</ul>
<h2 id="machine-learning"><a class="header" href="#machine-learning">Machine Learning</a></h2>
<ul>
<li><a href="https://aws.amazon.com/blogs/machine-learning/optimized-pytorch-2-0-inference-with-aws-graviton-processors/">Optimized PyTorch 2.0 inference with AWS Graviton processors</a></li>
<li><a href="https://aws.amazon.com/blogs/machine-learning/reduce-amazon-sagemaker-inference-cost-with-aws-graviton/">Reduce Amazon SageMaker inference cost with AWS Graviton</a></li>
<li><a href="https://pytorch.org/blog/optimized-pytorch-w-graviton/">PyTorch blog: Optimized PyTorch 2.0 Inference with AWS Graviton processors</a></li>
<li><a href="https://pytorch.org/tutorials/recipes/inference_tuning_on_aws_graviton.html">PyTorch Inference Performance Tuning on AWS Graviton Processors</a></li>
<li><a href="https://pytorch.org/blog/accelerated-pytorch-inference/">PyTorch blog: Accelerated PyTorch inference with torch.compile on AWS Graviton processors</a></li>
<li><a href="https://aws.amazon.com/blogs/machine-learning/sprinklr-improves-performance-by-20-and-reduces-cost-by-25-for-machine-learning-inference-on-aws-graviton3/">Sprinklr improves performance by 20% and reduces cost by 25% for machine learning inference on AWS Graviton3</a></li>
<li><a href="https://aws.amazon.com/blogs/machine-learning/run-machine-learning-inference-workloads-on-aws-graviton-based-instances-with-amazon-sagemaker/">Run machine learning inference workloads on AWS Graviton-based instances with Amazon SageMaker</a></li>
<li><a href="https://aws.amazon.com/blogs/machine-learning/accelerate-nlp-inference-with-onnx-runtime-on-aws-graviton-processors">Accelerate NLP inference with ONNX Runtime on AWS Graviton processors</a></li>
<li><a href="https://community.arm.com/arm-community-blogs/b/infrastructure-solutions-blog/posts/best-in-class-llm-performance-on-arm-neoverse-v1-based-aws-graviton3-servers">Best-in-class LLM Performance on Arm Neoverse V1 based AWS Graviton3 CPUs</a></li>
<li><a href="https://community.arm.com/arm-community-blogs/b/infrastructure-solutions-blog/posts/accelerating-sentiment-analysis-on-arm-neoverse-cpus">Accelerating Popular Hugging Face Models using Arm Neoverse</a></li>
<li><a href="https://community.aws/content/2eazHYzSfcY9flCGKsuGjpwqq1B/run-llms-on-cpu-with-amazon-sagemaker-real-time-inference?lang=en">Run LLMs on CPU with Amazon SageMaker Real-time Inference</a></li>
<li><a href="https://aws.amazon.com/blogs/machine-learning/accelerating-large-scale-neural-network-training-on-cpus-with-thirdai-and-aws-graviton/">Accelerating large-scale neural network training on CPUs with ThirdAI and AWS Graviton</a></li>
<li><a href="https://github.com/pytorch/serve/tree/usecase/rag_based_llm/examples/usecases/RAG_based_LLM_serving#enhancing-llm-serving-with-torch-compiled-rag-on-aws-graviton">Enhancing LLM Serving with Torch Compiled RAG on AWS Graviton</a></li>
<li><a href="https://dev.to/aws-heroes/intro-to-llama-on-graviton-1dc">Intro to Llama on Graviton</a></li>
<li><a href="https://community.aws/content/2pViH36qEfqDYwv0kEysJ0kqDp4/small-language-models-slms-inference-with-llama-cpp-on-graviton4">Small Language Models (SLMs) inference with llama.cpp on Graviton4</a></li>
<li><a href="machinelearning/llama.cpp.html">Run DeepSeek R1 LLM Inference on AWS Graviton</a></li>
<li><a href="https://community.aws/content/2rhRJI6cxBa1Ib5f3TjsfPadpXs/deploying-deepseek-r1-distill-llama-70b-for-batch-inference-on-aws-graviton4">DeepSeek-R1 Distill Model on CPU with AWS Graviton4 for batch inference</a></li>
</ul>
<h2 id="other"><a class="header" href="#other">Other</a></h2>
<ul>
<li><a href="https://aws.amazon.com/blogs/opensource/optimized-video-encoding-with-ffmpeg-on-aws-graviton-processors/">Optimized Video Encoding with FFmpeg on AWS Graviton Processors</a></li>
<li><a href="https://aws.amazon.com/blogs/opensource/video-encoding-on-graviton-in-2025/">Video Encoding on Graviton in 2025</a></li>
<li><a href="https://aws.amazon.com/blogs/compute/using-amazon-aperf-to-go-from-50-below-to-36-above-performance-target/">Using Amazon APerf to go from 50% below to 36% above performance target</a></li>
</ul>
<h1 id="case-studies"><a class="header" href="#case-studies">Case Studies</a></h1>
<h2 id="hpc-1"><a class="header" href="#hpc-1">HPC</a></h2>
<ul>
<li><a href="https://aws.amazon.com/solutions/case-studies/encored-technologies-case-study/">Encored Technologies Successfully Built an HPC on AWS for Weather Research &amp; Forecasting (WRF)</a></li>
<li><a href="https://aws.amazon.com/solutions/case-studies/arm-ltd-case-study/">Arm Scales Performance for Chip Design Using Amazon FSx for NetApp ONTAP</a></li>
<li><a href="https://aws.amazon.com/solutions/case-studies/case-study-institut-pasteur/?nc1=h_ls">The Institut Pasteur and AWS are analysing the world's DNA, using a public database</a></li>
</ul>
<h2 id="other-1"><a class="header" href="#other-1">Other</a></h2>
<ul>
<li><a href="https://aws.amazon.com/solutions/case-studies/sprinklr-case-study/">Lower Latency and Costs Using AWS Graviton2–Based Instances with Sprinklr</a></li>
</ul>
<h1 id="additional-resources"><a class="header" href="#additional-resources">Additional resources</a></h1>
<ul>
<li><a href="https://aws.amazon.com/ec2/graviton/">AWS Graviton</a></li>
<li><a href="https://developer.arm.com/documentation/pjdoc466751330-9707/latest">Neoverse N1 Software Optimization Guide</a></li>
<li><a href="https://documentation-service.arm.com/static/60119835773bb020e3de6fee">Armv8 reference manual</a></li>
<li><a href="https://pkgs.org/">Package repository search tool</a></li>
</ul>
<p><strong>Feedback?</strong> ec2-arm-dev-feedback@amazon.com</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="considerations-when-transitioning-workloads-to-aws-graviton-based-amazon-ec2-instances"><a class="header" href="#considerations-when-transitioning-workloads-to-aws-graviton-based-amazon-ec2-instances">Considerations when transitioning workloads to AWS Graviton based Amazon EC2 instances</a></h1>
<p>AWS Graviton processors power Amazon EC2 general purpose (M8g, M7g, M7gd, M6g, M6gd, T4g), compute optimized (C8g, C7g, C7gd, C7gn, C6g, C6gd, C6gn), memory optimized (X8g, R8g, R7g, R7gd, R6g, R6gd) instances, storage optimized (I8g, I4g, Im4gn, Is4gen), HPC optimized (Hpc7g), and GPU-powered (G5g) instances that provide the best price-performance for a wide variety of Linux-based workloads. Examples include application servers, micro-services, high-performance computing, CPU-based machine learning inference, video encoding, electronic design automation, gaming, open-source databases, and in-memory caches. In most cases transitioning to AWS Graviton is as simple as updating your infrastructure-as-code to select the new instance type and associated Operating System (OS) Amazon Machine Image (<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html">AMI</a>). However, because AWS Graviton processors implement the arm64 instruction set, there can be additional software implications. This transition guide provides a step-by-step approach to assess your workload to identify and address any potential software changes that might be needed.</p>
<h2 id="introduction---identifying-target-workloads"><a class="header" href="#introduction---identifying-target-workloads">Introduction - identifying target workloads</a></h2>
<p>The quickest and easiest workloads to transition are Linux-based, and built using open-source components or in-house applications where you control the source code. Many open source projects already support Arm64 and by extension Graviton, and having access to the source code allows you to build from source if pre-built artifacts do not already exist. There is also a large and growing set of Independent Software Vendor (ISV) software available for Graviton (a non-exhaustive list can be found <a href="isv.html">here</a>. However if you license software you’ll want to check with the respective ISV to ensure they already, or have plans to, support the Arm64 instruction set.</p>
<p>Another set of opportunities, not covered in this guide, are all the managed services offered by AWS which leverage AWS Graviton processors. Those services, listed on <a href="managed_services.html">AWS Graviton-based Manage Services page</a>, tend to be straightforward to adopt and require a process similar to moving to a new version of underlying instance of the same architecture, or in some cases do not require any modification.</p>
<p>In this guide, we will focus on applications you operate on Amazon EC2 instances, either directly or in the context of orchestration engine operating on top of Amazon EC2 instances, such as Amazon ECS or Amazon EKS.</p>
<p>The following transition guide is organized into a logical sequence of steps as follows:</p>
<ul>
<li><a href="transition-guide.html#learning-and-exploring">Learning and exploring</a>
<ul>
<li>Step 1 -  [Optional] Understand the Graviton Processor and review key documentation</li>
<li>Step 2 - Explore your workload, and inventory your current software stack</li>
</ul>
</li>
<li><a href="transition-guide.html#plan-your-workload-transition">Plan your workload transition</a>
<ul>
<li>Step 3 - Install and configure your application environment</li>
<li>Step 4 - [Optional] Build your application(s) and/or container images</li>
</ul>
</li>
<li><a href="transition-guide.html#test-and-optimize-your-workload">Test and optimize your workload</a>
<ul>
<li>Step 5 - Testing and optimizing your workload</li>
<li>Step 6 - Performance testing</li>
</ul>
</li>
<li><a href="transition-guide.html#infrastructure-and-deployment">Infrastructure and deployment</a>
<ul>
<li>Step 7 - Update your infrastructure as code</li>
<li>Step 8 - Perform canary or Blue-Green deployment</li>
</ul>
</li>
</ul>
<h3 id="learning-and-exploring"><a class="header" href="#learning-and-exploring">Learning and exploring</a></h3>
<p><strong>Step 1 - [Optional] Understand the Graviton Processor and review key documentation</strong></p>
<ol>
<li>[Optional] Start by watching <a href="https://www.youtube.com/watch?v=W4dnUvJJ_Sg">re:Invent 2024 - AWS Graviton: The best price performance for your AWS workloads</a>, <a href="https://www.youtube.com/watch?v=YKZbNcOU77c">re:Invent 2024 - Dive deep into the AWS Nitro System</a> and <a href="https://www.youtube.com/watch?v=WDKwwFQKfSI">re:Invent 2021 - Deep dive into AWS Graviton3 and Amazon EC2 C7g instances</a>, which will give you an overview of the Graviton-based instances and some insights on how to run applications depending on their operating system, languages and runtimes.</li>
<li>[Optional] Keep on learning by watching <a href="https://www.youtube.com/watch?v=p62DLuSCNOw">AWS Summit SF 2022 - The journey of silicon innovation at AWS</a> to better understand Amazon long-term commitment to innovate with custom silicon.</li>
<li>Get familiar with the rest of this <a href="README.html">Getting started with AWS Graviton repository</a> which will act as a useful reference throughout your workload transition.</li>
</ol>
<p><strong>Step 2 -  Explore your workload, and inventory your current software stack</strong></p>
<p>Before starting the transition, you will need to inventory your current software stack so you can identify the path to equivalent software versions that support Graviton. At this stage it can be useful to think in terms of software you download (e.g. open source packages, container images, libraries), software you build and software you procure/license (e.g. monitoring or security agents). Areas to review:</p>
<ul>
<li><a href="os.html">Operating system</a>, pay attention to specific versions that support Graviton (usually more recent are better)</li>
<li>If your workload is container based, check container images you consume for Arm64 support. Keep in mind many container images now support multiple architectures which simplifies consumption of those images in a mixed-architecture environment. See the <a href="https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/">ECR multi-arch support announcement</a> for more details on multi-arch images.</li>
<li>All the libraries, frameworks and runtimes used by the application.</li>
<li>Tools used to build, deploy and test your application (e.g. compilers, test suites, CI/CD pipelines, provisioning tools and scripts). Note there are language specific sections in the getting started guide with useful pointers to getting the best performance from Graviton processors.</li>
<li>Tools and/or agents used to deploy and manage the application in production (e.g. monitoring tools or security agents)</li>
<li><a href="https://aws.amazon.com/q/developer/transform/">Amazon Transform</a> helps quickly and easily migrate software environments that provide a path to Graviton adoption - for example:
<ul>
<li><a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/transform-java.html">Transforming Java applications with Amazon Q Developer</a></li>
<li><a href="https://docs.aws.amazon.com/amazonq/latest/qdeveloper-ug/transform-dotnet-IDE.html">Transforming .NET applications with Amazon Q Developer</a></li>
</ul>
</li>
<li>This guide contains language specifics sections where you'll find additional per-language guidance:
<ul>
<li><a href="c-c++.html">C/C++</a></li>
<li><a href="golang.html">Go</a></li>
<li><a href="java.html">Java</a></li>
<li><a href="nodejs.html">Node.js</a></li>
<li><a href="dotnet.html">.NET</a></li>
<li><a href="php.html">PHP</a></li>
<li><a href="python.html">Python</a></li>
<li><a href="R.html">R</a></li>
<li><a href="rust.html">Rust</a></li>
</ul>
</li>
</ul>
<p>As a rule the more current your software environment the more likely you will obtain the full performance entitlement from Graviton.</p>
<p>For each component of your software stack, check for arm64/Graviton support. A large portion of this can be done using existing configuration scripts, as your scripts run and install packages you will get messages for any missing components, some may build from source automatically while others will cause the script to fail. Pay attention to software versions as in general the more current your software is the easier the transition, and the more likely you’ll achieve the full performance entitlement from Graviton processors. If you do need to perform upgrades prior to adopting Graviton then it is best to do that using an existing x86 environment to minimize the number of changed variables. We have seen examples where upgrading OS version on x86 was far more involved and time consuming than transitioning to Graviton after the upgrade. For more details on checking for software support please see Appendix A.</p>
<p>Note: When locating software be aware that some tools, including  GCC, refer to the architecture as AArch64, others including the Linux Kernel, call it arm64. When checking packages across various repositories, you’ll find those different naming conventions.</p>
<h3 id="plan-your-workload-transition"><a class="header" href="#plan-your-workload-transition">Plan your workload transition</a></h3>
<p><strong>Step 3-  Install and configure your application environment</strong></p>
<p>To transition and test your application, you will need a suitable Graviton environment. Depending on your execution environment, you may need to:</p>
<ul>
<li>Obtain or create an arm64 AMI to boot your Graviton instance(s) from. Depending on how you manage your AMIs, you can either start directly from an existing reference AMI for Arm64, or you can build a Golden AMI with your specific dependencies from one of the reference images (see <a href="os.html">here</a> for a full list of supported operating systems with AMI links) ;</li>
<li>If you operate a container based environment, you’ll need to build or extend an existing cluster with support for Graviton based instances. Both Amazon ECS and EKS support adding Graviton-based instances to an existing x86-based cluster. For ECS, you can add Graviton-based instances to your ECS cluster, launching them with either the <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html">AWS ECS-optimized AMI for arm64</a> or your own AMI after you’ve installed the ECS agent. For EKS, you will need to create a node-group with Graviton-based instances launched with the <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html">EKS optimized AMI for arm64</a>.
<ul>
<li>Note: you can support Graviton and x86 instances in the same Auto Scaling Group, this <a href="https://aws.amazon.com/blogs/compute/supporting-aws-graviton2-and-x86-instance-types-in-the-same-auto-scaling-group/">blog</a> details the process using the launch template override feature.</li>
</ul>
</li>
<li>Complete the installation of your software stack based on the inventory created in step 2.
<ul>
<li>Note: In many cases your installation scripts can be used as-is or with minor modifications to reference architecture specific versions of components where necessary. The first time through this may be an iterative process as you resolve any remaining dependencies.</li>
</ul>
</li>
</ul>
<p><strong>Step 4 - [Optional] Build your application(s) and/or container images</strong></p>
<p>Note: If you are not building your application or component parts of your overall application stack you may skip this step.</p>
<p>For applications built using interpreted or JIT’d languages, including Java, PHP or Node.js, they should run as-is or with only minor modifications. The repository contains language specific sections with recommendations, for example <a href="java.html">Java</a>, <a href="python.html">Python</a>, <a href="c-c++.html">C/C++</a>, <a href="golang.html">Golang</a>, <a href="php.html">PHP</a>, <a href="r.html">R</a>, <a href="nodejs.html">Node.js</a>, <a href="rust.html">Rust</a> or <a href="dotnet.html">.Net</a>. Note: if there is no language specific section, it is because there is no specific guidance beyond using a suitably current version of the language as documented <a href="README.html#recent-software-updates-relevant-to-graviton">here</a>. .NET-core is a great way to benefit from Graviton-based instances, this <a href="https://aws.amazon.com/blogs/dotnet/powering-net-8-with-aws-graviton3-benchmarks/">blog post</a> covers .NET 8 performance.</p>
<p>Applications using compiled languages including C, C++ or Go, need to be compiled for the Arm64 architecture. Most modern builds (e.g. using Make) will just work when run natively on Graviton-based instances, however, you’ll find language specific compiler recommendations in this repository: <a href="c-c++.html">C/C++</a>, <a href="golang.html">Go</a>, and <a href="rust.html">Rust</a>.</p>
<p>Just like an operating system, container images are architecture specific. You will need to build arm64 container image(s), to make the transition easier we recommend building multi-arch container image(s) that can run automatically on either x86-64 or arm64. Check out the <a href="containers.html">container section</a> of this repository for more details and this <a href="https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/">blog post</a> provides a detailed overview of multi-architecture container image support, which is considered a best practice for establishing and maintaining a multi-architecture environment.</p>
<p>You will also need to review any functional and unit test suite(s) to ensure you can test the new build artifacts with the same test coverage you have already for x86 artifacts.</p>
<h3 id="test-and-optimize-your-workload"><a class="header" href="#test-and-optimize-your-workload">Test and optimize your workload</a></h3>
<p><strong>Step 5 - Testing and optimizing your workload</strong></p>
<p>Now that you have your application stack on Graviton, you should run your test suite to ensure all regular unit and functional tests pass. Resolve any test failures in the application(s) or test suites until you are satisfied everything is working as expected. Most errors should be related to the modifications and updated software versions you have installed during the transition (tip: when upgrading software versions first test them using an existing x86 environment to minimize the number of variables changed at a time. If issues occur then resolve them using the current x86 environment before continuing with the new Graviton environment). If you suspect architecture specific issue(s) please have a look to our <a href="c-c++.html">C/C++ section </a> which documents them and give advice on how to solve them. If there are still details that seem unclear, please reach out to your AWS account team, or to the AWS support for assistance.</p>
<p><strong>Step 6 - Performance testing</strong></p>
<p>With your fully functional application, it is time to establish a performance baseline on Graviton. In many cases, Graviton will provide performance and/or capacity improvements over x86-based instances.</p>
<p>One of the major differences between AWS Graviton instances and other Amazon EC2 instances is their vCPU-to-physical-core mappings. Every vCPU on a Graviton processor maps to a physical core, and there is no Simultaneous Multi-Threading (SMT). Consequently, Graviton provides better linear performance scalability in most cases. When comparing to existing x86 instances, we recommend fully loading both instance types to determine the maximum sustainable load before the latency or error rate exceeds acceptable bounds. For horizontally-scalable, multi-threaded workloads that are CPU bound, you may find that the Graviton instances are able to sustain a significantly higher transaction rate before unacceptable latencies or error rates occur.</p>
<p>During the transition to Graviton, if you are using Amazon EC2 Auto Scaling, you may be able to increase the threshold values for the CloudWatch alarms that invoke the scaling process. This may reduce the number of EC2 instances now needed to serve a given level of demand.</p>
<p>Important: This repository has sections dedicated to <a href="optimizing.html">Optimization</a> and a <a href="perfrunbook/README.html">Performance Runbook</a> for you to follow during this stage.</p>
<p>If after reading the documentation in this repository and following the recommendations you do not observe expected performance then please reach out to your AWS account team, or send email to <a href="mailto:ec2-arm-dev-feedback@amazon.com">ec2-arm-dev-feedback@amazon.com</a> with details so we can assist you with your performance observations.</p>
<h3 id="infrastructure-and-deployment"><a class="header" href="#infrastructure-and-deployment">Infrastructure and deployment</a></h3>
<p><strong>Step 7 - Update your infrastructure as code</strong></p>
<p>Now you have a tested and performant application, its time to update your infrastructure as code to add support for Graviton-based instances. This typically includes updating instance types, AMI IDs, ASG constructs to support multi-architecture (see <a href="https://aws.amazon.com/about-aws/whats-new/2020/11/amazon-ec2-auto-scaling-announces-support-for-multiple-launch-templates-for-auto-scaling-groups/">Amazon EC2 ASG support for multiple Launch Templates</a>), and finally deploying or redeploying your infrastructure.</p>
<p><strong>Step 8 - Perform canary or Blue-Green deployment</strong></p>
<p>Once your infrastructure is ready to support Graviton-based instances, you can start a Canary or Blue-Green deployment to re-direct a portion of application traffic to the Graviton-based instances. Ideally initial tests will run in a development environment to load test with production traffic patterns. Monitor the application closely to ensure expected behavior. Once your application is running as expected on Graviton you can define and execute your transition strategy and begin to enjoy the benefits of increased price-performance.</p>
<h3 id="appendix-a---locating-packages-for-arm64graviton"><a class="header" href="#appendix-a---locating-packages-for-arm64graviton"><em>Appendix A - locating packages for Arm64/Graviton</em></a></h3>
<p>Remember: When locating software be aware that some tools, including  GCC, refer to the architecture as AArch64, others including the Linux Kernel, call it arm64. When checking packages across various repositories, you’ll find those different naming conventions, and in some cases just “ARM”.</p>
<p>The main ways to check and places to look for will be:</p>
<ul>
<li>Package repositories of your chosen Linux distribution(s). Arm64 support within Linux distributions is largely complete: for example, Debian, which has the largest package repository, has over 98% of its packages built for the arm64 architecture.</li>
<li>Container image registry. Amazon ECR now offers <a href="https://docs.aws.amazon.com/AmazonECR/latest/public/public-repositories.html">public repositories</a> that you can search for <a href="https://gallery.ecr.aws/?architectures=ARM+64&amp;page=1">arm64 images</a>. DockerHub allows you to search for a specific architecture (<a href="https://hub.docker.com/search?type=image&amp;architecture=arm64">e.g. arm64</a>).
<ul>
<li>Note: Specific to containers you may find an amd64 (x86-64) container image you currently use transitioned to a multi-architecture container image when adding Arm64 support. This means you may not find an explicit arm64 container, so be sure to check for both as projects may chose to vend discrete images for x86-64 and arm64 while other projects chose to vend a multi-arch image supporting both architectures.</li>
</ul>
</li>
<li>On GitHub, you can check for arm64 versions in the release section. However, some projects don’t use the release section, or only release source archives, so you may need to visit the main project webpage and check the download section. You can also search the GitHub project for “arm64” or “AArch64” to see whether the project has any arm64 code contributions or issues. Even if a project does not currently produce builds for arm64, in many cases an Arm64 version of those packages will be available through Linux distributions or additional package repositories. You can search for packages using a package search tool such as <a href="https://pkgs.org/">pkgs.org</a>.</li>
<li>The download section or platform support matrix of your software vendors, look for references to arm64, AArch64 or Graviton.</li>
</ul>
<p>Categories of software with potential issues:</p>
<ul>
<li>Packages or applications sourced from an ISV may not yet be available for Graviton. AWS is working with lots of software partners to offer technical guidance as they add support for Graviton, but some are still missing or in the process of adding support. A non-exhaustive list of some ISV software can be found in <a href="isv.html">here</a>.</li>
<li>The Python community vend lots of modules built using low level languages (e.g. C/C++) that need to be compiled for the Arm64 architecture. You may use modules that are not currently available as pre-built binaries from the Python Package Index. AWS is actively working with open-source communities to ensure the most popular modules are available. In the meantime we provide specific instructions to resolve the build-time dependencies for missing packages in the <a href="python.html#1-installing-python-packages">Python section</a> of the Graviton Getting Started Guide.</li>
</ul>
<p>If you find other software lacking support for Arm64, please let your AWS team know, or send email to <a href="mailto:ec2-arm-dev-feedback@amazon.com">ec2-arm-dev-feedback@amazon.com</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimizing-for-graviton-1"><a class="header" href="#optimizing-for-graviton-1">Optimizing for Graviton</a></h1>
<h2 id="debugging-problems"><a class="header" href="#debugging-problems">Debugging Problems</a></h2>
<p>It's possible that incorrect code will work fine on an existing system, but
produce an incorrect result when using a new compiler. This could be because
it relies on undefined behavior in the language (e.g. assuming char is signed in C/C++,
or the behavior of signed integer overflow), contains memory management bugs that
happen to be exposed by aggressive compiler optimizations, or incorrect ordering.
Below are some techniques / tools we have used to find issues
while migrating our internal services to newer compilers and Graviton based instances.</p>
<h3 id="using-sanitizers"><a class="header" href="#using-sanitizers">Using Sanitizers</a></h3>
<p>The compiler may generate code and layout data slightly differently on Graviton
compared to an x86 system and this could expose latent memory bugs that were previously
hidden. On GCC, the easiest way to look for these bugs is to compile with the
memory sanitizers by adding the below to standard compiler flags:</p>
<pre><code>    CFLAGS += -fsanitize=address -fsanitize=undefined
    LDFLAGS += -fsanitize=address  -fsanitize=undefined
</code></pre>
<p>Then run the resulting binary, any bugs detected by the sanitizers will cause
the program to exit immediately and print helpful stack traces and other
information.</p>
<h3 id="ordering-issues"><a class="header" href="#ordering-issues">Ordering issues</a></h3>
<p>Arm is weakly ordered, similar to POWER and other modern architectures. While
x86 is a variant of total-store-ordering (TSO).
Code that relies on TSO may lack barriers to properly order memory references.
Armv8 based systems, including all Gravitons are <a href="https://www.cl.cam.ac.uk/~pes20/armv8-mca/armv8-mca-draft.pdf">weakly ordered
multi-copy-atomic</a>.</p>
<p>While TSO allows reads to occur out-of-order with writes and a processor to
observe its own write before it is visible to others, the Armv8 memory model has
further relaxations for performance and power efficiency.
Code relying on pthread mutexes or locking abstractions
found in C++, Java or other languages shouldn't notice any difference. Code that
has a bespoke implementation of lockless data structures or implements its own
synchronization primitives will have to use the proper intrinsics and
barriers to correctly order memory transactions. If you run into an issue with
memory ordering please feel free to open an issue in this GitHub repo, and one
of our AWS experts will contact you.</p>
<h3 id="architecture-specific-optimization"><a class="header" href="#architecture-specific-optimization">Architecture specific optimization</a></h3>
<p>Sometimes code will have architecture specific optimizations. These can take many forms:
sometimes the code is optimized in assembly using specific instructions for
<a href="https://github.com/php/php-src/commit/2a535a9707c89502df8bc0bd785f2e9192929422">CRC</a>,
other times the code could be enabling a <a href="https://github.com/lz4/lz4/commit/605d811e6cc94736dd609c644404dd24c013fd6f">feature</a>
that has been shown to work well on particular architectures. A quick way to see if any optimizations
are missing for Arm is to grep the code for <code>__x86_64__</code> <code>ifdef</code>s and see if there
is corresponding Arm code there too. If not, that might be something to improve.
We welcome suggestions by opening an issue in this repo.</p>
<h3 id="locksynchronization-intensive-workload"><a class="header" href="#locksynchronization-intensive-workload">Lock/Synchronization intensive workload</a></h3>
<p>Graviton2 processors and later support the Arm Large Scale Extensions (LSE). LSE based locking and synchronization
is an order of magnitude faster for highly contended locks with high core counts (e.g. up to 192 cores on Graviton4).
For workloads that have highly contended locks, compiling with <code>-march=armv8.2-a</code> will enable LSE based atomics and can substantially increase performance. However, this will prevent the code
from running on an Arm v8.0 system such as AWS Graviton-based EC2 A1 instances.
With GCC 10 and newer an option <code>-moutline-atomics</code> will not inline atomics and
detect at run time the correct type of atomic to use. This is slightly worse
performing than <code>-march=armv8.2-a</code> but does retain backwards compatibility.</p>
<h3 id="network-intensive-workloads"><a class="header" href="#network-intensive-workloads">Network intensive workloads</a></h3>
<p>In some workloads, the packet processing capability of Graviton is both faster and
lower-latency than other platforms, which reduces the natural “coalescing”
capability of Linux kernel and increases the interrupt rate.
Depending on the workload it might make sense to enable adaptive RX interrupts
(e.g. <code>ethtool -C &lt;interface&gt; adaptive-rx on</code>).</p>
<h2 id="profiling-the-code"><a class="header" href="#profiling-the-code">Profiling the code</a></h2>
<p>If you aren't getting the performance you expect, one of the best ways to understand what is
going on in the system is to compare profiles of execution and understand where the CPUs are
spending time. This will frequently point to a hot function or sub-system that could be optimized. A crutch
is comparing a profile between a system that is performing well and one that isn't to see the
relative difference in execution time. Feel free to open an issue in this
GitHub repo for advice or help.</p>
<p>Using <a href="https://github.com/aws/aperf">AWS APerf</a> tool:</p>
<pre><code class="language-bash"># Graviton
wget -qO- https://github.com/aws/aperf/releases/download/v0.1.10-alpha/aperf-v0.1.10-alpha-aarch64.tar.gz | tar -xvz -C /target/directory

# x86
wget -qO- https://github.com/aws/aperf/releases/download/v0.1.10-alpha/aperf-v0.1.10-alpha-x86_64.tar.gz | tar -xvz -C /target/directory

## Record a profile and generate a report
cd /target/directory/
./aperf record -r &lt;RUN_NAME&gt; -i &lt;INTERVAL_NUMBER&gt; -p &lt;COLLECTION_PERIOD&gt;
./aperf report -r &lt;COLLECTOR_DIRECTORY&gt; -n &lt;REPORT_NAME&gt;

## The resulting report can be viewed with a web-browser by opening the index.html file
</code></pre>
<p>Using the Linux perf tool:</p>
<pre><code class="language-bash"># Amazon Linux 2
sudo yum install perf

# Ubuntu
sudo apt-get install linux-tools-$(uname -r)
</code></pre>
<p>Record a profile:</p>
<pre><code># If the program is run interactively
$ sudo perf record -g -F99 -o perf.data ./your_program

# If the program is a service, sample all cpus (-a) and run for 60 seconds while the system is loaded
$  sudo perf record -ag -F99 -o perf.data  sleep 60
</code></pre>
<p>Look at the profile:</p>
<pre><code>$ perf report
</code></pre>
<p>Additionally, there is a tool that will generate a visual representation of the output which can sometimes
be more useful:</p>
<pre><code>git clone https://github.com/brendangregg/FlameGraph.git
perf script -i perf.data | FlameGraph/stackcollapse-perf.pl | FlameGraph/flamegraph.pl &gt; flamegraph.svg
</code></pre>
<p>For example, in March 2020, we committed a patch to
<a href="http://ffmpeg.org/pipermail/ffmpeg-devel/2019-November/253385.html">ffmpeg</a> to
improve performance. Comparing the execution time of a C5 vs an M6g
immediately uncovered an outlier function <code>ff_hscale_8_to_15_neon</code>.  Once we
identified this as the outlier we could focus on improving this function.</p>
<pre><code>C5.4XL	                        M6g.4XL
19.89% dv_encode_video_segment	19.57% ff_hscale_8_to_15_neon
11.21% decode_significance_x86	18.02% get_cabac
8.68% get_cabac	                15.08% dv_encode_video_segment
8.43% ff_h264_decode_mb_cabac	5.85% ff_jpeg_fdct_islow_8
8.05% ff_hscale8to15_X4_ssse3	5.01% ff_yuv2planeX_8_neon
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="cc-on-graviton"><a class="header" href="#cc-on-graviton">C/C++ on Graviton</a></h1>
<h3 id="enabling-arm-architecture-specific-features"><a class="header" href="#enabling-arm-architecture-specific-features">Enabling Arm Architecture Specific Features</a></h3>
<p>TLDR: To target all current generation Graviton instances (Graviton2,
Graviton3, and Graviton4), use <code>-march=armv8.2-a</code>.</p>
<p>C and C++ code can be built for Graviton with a variety of flags, depending on
the goal. If the goal is to get the best performance for a specific generation,
select a flag from the table column "performance". If the goal is to get a good
compromise of feature availability and performance balanced across generations,
select the flag from the "balanced" column. If you want to target multiple
generations of Graviton, select the "balanced" flag for the oldest generation
planned for deployment, since code built for a newer generation may not run on
an older generation. On arm64 <code>-mcpu=</code> acts as both specifying the appropriate
architecture and tuning and it's generally better to use that vs <code>-march</code> if
you're building for a specific CPU.</p>
<div class="table-wrapper"><table><thead><tr><th>CPU</th><th>Flag (performance)</th><th>Flag (balanced)</th><th>GCC version</th><th>LLVM version</th></tr></thead><tbody>
<tr><td>Graviton2</td><td><code>-mcpu=neoverse-n1</code> ¹</td><td><code>-march=armv8.2-a</code></td><td>GCC-9</td><td>Clang/LLVM 10+</td></tr>
<tr><td>Graviton3(E)</td><td><code>-mcpu=neoverse-v1</code></td><td><code>-mcpu=neoverse-512tvb</code> ²</td><td>GCC 11</td><td>Clang/LLVM 14+</td></tr>
<tr><td>Graviton4</td><td><code>-mcpu=neoverse-v2</code></td><td><code>-mcpu=neoverse-512tvb</code> ²</td><td>GCC 13</td><td>Clang/LLVM 16+</td></tr>
</tbody></table>
</div>
<p>¹ Requires GCC-9 or later (or GCC-7 for Amazon Linux 2); otherwise we suggest
using <code>-mcpu=cortex-a72</code></p>
<p>² If your compiler doesn't support <code>neoverse-512tvb</code>, please use the Graviton2
tuning.</p>
<p>For some applications, it may be necessary to support a broad range of Arm64
targets while still making use of more advanced features such as LSE (Large
System Extensions) or SVE (Scalable Vector Extension). For this case choose a
more conservative build flag, such as <code>-march=armv8-a</code> and make use of runtime
CPU support detection of features such as SVE. You can enable runtime detection
and use of LSE atomics instructions by adding the additional compiler flag,
<code>-moutline-atomics</code>.</p>
<h3 id="compilers"><a class="header" href="#compilers">Compilers</a></h3>
<p>Newer compilers provide better support and optimizations for Graviton processors.
We have seen 15% better performance on Graviton2 when using gcc-10 instead of Amazon Linux 2 system's compiler gcc-7.
When possible please use the latest compiler version available on your system.
The table shows GCC and LLVM compiler versions available in Linux distributions.
Starred version marks the default system compiler.</p>
<div class="table-wrapper"><table><thead><tr><th>Distribution</th><th>GCC</th><th>Clang/LLVM</th></tr></thead><tbody>
<tr><td>Amazon Linux 2023</td><td>11*</td><td>15*</td></tr>
<tr><td>Amazon Linux 2</td><td>7*, 10</td><td>7, 11*</td></tr>
<tr><td>Ubuntu 24.04</td><td>9, 10, 11, 12, 13*, 14</td><td>14, 15, 16, 17, 18*</td></tr>
<tr><td>Ubuntu 22.04</td><td>9, 10, 11*, 12</td><td>11, 12, 13, 14*</td></tr>
<tr><td>Ubuntu 20.04</td><td>7, 8, 9*, 10</td><td>6, 7, 8, 9, 10, 11, 12</td></tr>
<tr><td>Ubuntu 18.04</td><td>4.8, 5, 6, 7*, 8</td><td>3.9, 4, 5, 6, 7, 8, 9, 10</td></tr>
<tr><td>Debian10</td><td>7, 8*</td><td>6, 7, 8</td></tr>
<tr><td>Red Hat EL8</td><td>8*, 9, 10</td><td>10</td></tr>
<tr><td>SUSE Linux ES15</td><td>7*, 9, 10</td><td>7</td></tr>
</tbody></table>
</div>
<h3 id="large-system-extensions-lse"><a class="header" href="#large-system-extensions-lse">Large-System Extensions (LSE)</a></h3>
<p>All Graviton processors after Graviton1 have support for the Large-System Extensions (LSE)
which was first introduced in vArmv8.1. LSE provides low-cost atomic operations which can
improve system throughput for CPU-to-CPU communication, locks, and mutexes.
The improvement can be up to an order of magnitude when using LSE instead of
load/store exclusives.</p>
<p>POSIX threads library needs LSE atomic instructions.  LSE is important for
locking and thread synchronization routines.  The following systems distribute
a libc compiled with LSE instructions:</p>
<ul>
<li>Amazon Linux 2</li>
<li>Amazon Linux 2023</li>
<li>Ubuntu 18.04 (needs <code>apt install libc6-lse</code>)</li>
<li>Ubuntu 20.04</li>
<li>Ubuntu 22.04</li>
<li>Ubuntu 24.04</li>
</ul>
<p>The compiler needs to generate LSE instructions for applications that use atomic
operations.  For example, the code of databases like PostgreSQL contain atomic
constructs; c++11 code with std::atomic statements translate into atomic
operations.  GCC's <code>-march=armv8.2-a</code> flag enables all instructions supported by
Graviton2, including LSE.  To confirm that LSE instructions are created,
the output of <code>objdump</code> command line utility should contain LSE instructions:</p>
<pre><code>$ objdump -d app | grep -i 'cas\|casp\|swp\|ldadd\|stadd\|ldclr\|stclr\|ldeor\|steor\|ldset\|stset\|ldsmax\|stsmax\|ldsmin\|stsmin\|ldumax\|stumax\|ldumin\|stumin' | wc -l
</code></pre>
<p>To check whether the application binary contains load and store exclusives:</p>
<pre><code>$ objdump -d app | grep -i 'ldxr\|ldaxr\|stxr\|stlxr' | wc -l
</code></pre>
<h3 id="porting-codes-with-sseavx-intrinsics-to-neon"><a class="header" href="#porting-codes-with-sseavx-intrinsics-to-neon">Porting codes with SSE/AVX intrinsics to NEON</a></h3>
<p>When programs contain code with x64 intrinsics, the following procedure can help
to quickly obtain a working program on Arm, assess the performance of the
program running on Graviton processors, profile hot paths, and improve the
quality of code on the hot paths.</p>
<p>To quickly get a prototype running on Arm, one can use
https://github.com/DLTcollab/sse2neon a translator of x64 intrinsics to NEON.
sse2neon provides a quick starting point in porting performance critical codes
to Arm.  It shortens the time needed to get an Arm working program that then
can be used to extract profiles and to identify hot paths in the code.  A header
file <code>sse2neon.h</code> contains several of the functions provided by standard x64
include files like <code>xmmintrin.h</code>, only implemented with NEON instructions to
produce the exact semantics of the x64 intrinsic.  Once a profile is
established, the hot paths can be rewritten directly with NEON intrinsics to
avoid the overhead of the generic sse2neon translation.</p>
<h3 id="signed-vs-unsigned-char"><a class="header" href="#signed-vs-unsigned-char">Signed vs. Unsigned char</a></h3>
<p>The C standard doesn't specify the signedness of char. On x86 char is signed by
default while on Arm it is unsigned by default. This can be addressed by using
standard int types that explicitly specify the signedness (e.g. <code>uint8_t</code> and <code>int8_t</code>)
or compile with <code>-fsigned-char</code>.
When using the <code>getchar</code> function, instead of the commonly used but incorrect:</p>
<pre><code>char c;
while((c = getchar()) != EOF) {
    // Do something with the character c
}
// Assume we have reached the end of file here
</code></pre>
<p>you should use an <code>int</code> type and the standard function <code>feof</code> and <code>ferror</code> to
check for the end of file, as follows:</p>
<pre><code>int c;

while ((c = getchar()) != EOF) {
    // Do something with the character c
}
// Once we get EOF, we should check if it is actually an EOF or an error

if (feof(stdin)) {
    // End of file has been reached
} else if (ferror(stdin)) {
    // Handle the error (check errno, etc)
}
</code></pre>
<h3 id="using-arm-instructions-to-speed-up-machine-learning"><a class="header" href="#using-arm-instructions-to-speed-up-machine-learning">Using Arm instructions to speed-up Machine Learning</a></h3>
<p>Graviton2 and later processors been optimized for performance and power efficient machine learning by enabling <a href="https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/exploring-the-arm-dot-product-instructions">Arm dot-product instructions</a> commonly used for Machine Learning (quantized) inference workloads, and enabling <a href="https://developer.arm.com/documentation/100067/0612/Other-Compiler-specific-Features/Half-precision-floating-point-intrinsics">Half precision floating point - _float16</a> to double the number of operations per second, reducing the memory footprint compared to single precision floating point (_float32), while still enjoying large dynamic range.</p>
<h3 id="using-sve"><a class="header" href="#using-sve">Using SVE</a></h3>
<p>The scalable vector extensions (SVE) require both a new enough tool-chain to
auto-vectorize to SVE (GCC 11+, LLVM 14+) and a 4.15+ kernel that supports SVE.
One notable exception is that Amazon Linux 2 with a 4.14 kernel doesn't support SVE;
please upgrade to a 5.4+ AL2 kernel.  Graviton3 and Graviton4 support SVE, earlier Gravitons does not.</p>
<h3 id="using-arm-instructions-to-speed-up-common-code-sequences"><a class="header" href="#using-arm-instructions-to-speed-up-common-code-sequences">Using Arm instructions to speed-up common code sequences</a></h3>
<p>The Arm instruction set includes instructions that can be used to speedup common
code sequences. The table below lists common operations and links to code sequences:</p>
<div class="table-wrapper"><table><thead><tr><th>Operation</th><th>Description</th></tr></thead><tbody>
<tr><td><a href="sample-code/crc.c">crc</a></td><td>Graviton processors support instructions to accelerate both CRC32 which is used by Ethernet, media and compression and CRC32C (Castagnoli) which is used by filesystems.</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="go-on-graviton"><a class="header" href="#go-on-graviton">Go on Graviton</a></h1>
<p>Go is a statically typed, compiled programming language originally designed at Google. Go supports arm64 out of the box, and available in all common distributions, with recent changes that improve performance, so make sure to use the latest version of the Go compiler and toolchain.</p>
<h2 id="noteworthy-performance-upgrades"><a class="header" href="#noteworthy-performance-upgrades">Noteworthy performance upgrades</a></h2>
<h3 id="go-118-released-20220314"><a class="header" href="#go-118-released-20220314">Go 1.18 [released 2022/03/14]</a></h3>
<p>The main implementation of the Go compiler, <a href="https://github.com/golang/go">golang/go</a>, has improved
performance on Arm by implementing a new way of passing function arguments and results using registers instead of the stack. This change has been available on x86-64 since 1.17, where it brought performance improvements of about 5%. On Arm this change typically gives even higher performance improvements of 10% or more.</p>
<p>To learn more about the use cases benefiting from Go 1.18's performance improvements, check the blog post: <a href="https://aws.amazon.com/blogs/compute/making-your-go-workloads-up-to-20-faster-with-go-1-18-and-aws-graviton/">Making your Go workloads up to 20% faster with Go 1.18 and AWS Graviton</a>.</p>
<h3 id="go-117-released-20210816"><a class="header" href="#go-117-released-20210816">Go 1.17 [released 2021/08/16]</a></h3>
<p>The main implementation of the Go compiler, <a href="https://github.com/golang/go">golang/go</a>, has improved
performance for the following standard library packages:</p>
<ul>
<li>crypto/ed25519 - the package has been rewritten, and all operations are now approximately twice as fast on both arm64 and amd64.</li>
<li>crypto/elliptic - CurveParams methods now automatically invoke faster and safer dedicated implementations for known curves (P-224, P-256, and P-521) when available. The P521 curve implementation has also been rewritten and is now constant-time and three times faster on amd64 and arm64.</li>
</ul>
<h3 id="go-116-released-20210216"><a class="header" href="#go-116-released-20210216">Go 1.16 [released 2021/02/16]</a></h3>
<p>The main implementation of the Go compiler, <a href="https://github.com/golang/go">golang/go</a>, has improved
performance on Arm with couple of changes listed below. Building your project with Go 1.16 will give you these improvements:</p>
<ul>
<li><a href="https://go-review.googlesource.com/c/go/+/234217">ARMv8.1-A Atomics instructions</a>, which dramatically improve mutex fairness and speed on Graviton 2, and modern Arm core with v8.1 and newer instruction set.</li>
<li><a href="https://go-review.googlesource.com/c/go/+/243357">copy performance improvements</a>, especially when the addresses are unaligned.</li>
</ul>
<h3 id="recently-updated-packages"><a class="header" href="#recently-updated-packages">Recently updated packages</a></h3>
<p>Changes to commonly used packages that improve performance on Arm can make a noticeable difference in
some cases. Here is a partial list of packages to be aware of.</p>
<div class="table-wrapper"><table><thead><tr><th>Package</th><th>Version</th><th>Improvements</th></tr></thead><tbody>
<tr><td><a href="https://github.com/golang/snappy">Snappy</a></td><td>as of commit <a href="https://github.com/golang/snappy/commit/196ae77b8a26000fa30caa8b2b541e09674dbc43">196ae77</a></td><td>assembly implementations of the hot path functions were ported from amd64 to arm64</td></tr>
</tbody></table>
</div>
<h2 id="using-go-in-a-container-with-cpu-limits"><a class="header" href="#using-go-in-a-container-with-cpu-limits">Using Go in a Container with CPU Limits</a></h2>
<p>Go automatically assigns a sensible value to <code>GOMAXPROCS</code> based on the number of
CPU cores available. However, using a container with a limitation on how much
CPU is available to that container can lead to problems. For example, using the
<a href="https://docs.docker.com/engine/containers/resource_constraints/#configure-the-default-cfs-scheduler">CFS scheduler</a>
option in Docker, <code>--cpus=1</code> can limit the available CPU time to the
equivalent of 1 CPU while still exposing all of the actually available CPUs to
the container. If you use CPU limits in this way, it may make sense to also
manually set <code>GOMAXPROCS</code> to an equivalent value.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="java-on-graviton"><a class="header" href="#java-on-graviton">Java on Graviton</a></h1>
<p>Java is a general-purpose programming language. Compiled Java code can run on all platforms that support Java, without the need for recompilation. Java applications are typically compiled to bytecode that can run on any Java virtual machine (JVM) regardless of the underlying computer architecture. Note: there are multiple programming languages such as Kotlin, Scala, and Groovy that compile to byte code and run on top of the JVM, so those also tend to be highly portable across architectures. <em><a href="https://en.wikipedia.org/wiki/Java_(programming_language)">Wikipedia</a></em></p>
<p>Java is well supported and generally performant out-of-the-box on arm64. <a href="https://aws.amazon.com/corretto/">Amazon Corretto</a>, a no-cost, multiplatform, production-ready distribution of the Open Java Development Kit (OpenJDK) supports Graviton-powered instances.
While Java 8 is fully supported on Arm
processors, some customers haven't been able to obtain Graviton's full
performance benefit until they switched to Java 11.</p>
<p>This page includes specific details about building and tuning Java application on Graviton.</p>
<h3 id="java-graviton-migration-checklist"><a class="header" href="#java-graviton-migration-checklist">Java Graviton Migration Checklist</a></h3>
<p>This checklist summarizes portions of the Java on Graviton section and can be helpful for getting started.  It is not a comprehensive summary, so we also recommend reading through all the content below.</p>
<ul>
<li>
<p>Check AMI &amp; Kernel Version support</p>
<ul>
<li>Pre-2020 Linux distributions are unlikely to contain the right optimizations.</li>
<li>Amazon Linux:  AL2023 is ideal.  AL2 is fine with a recent kernel (i.e. not 4.14).  AL2 is EOL in June 2025.</li>
<li>Ubuntu:  Use at least Ubuntu 20.04.  More recent versions are even better.</li>
<li>Red Hat Linux:  RHEL9 is ideal.  Use at least RHEL8.2 (be aware kernel uses unusual 64KB memory pages).</li>
<li>For the full list, see <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/os.md">Operating Systems available for Graviton based instances</a>.</li>
</ul>
</li>
<li>
<p>Check JVM Version &amp; Flavor</p>
<ul>
<li>Java:  The more recent the better. AWS recommends at least JDK11, but ideally JDK17 or newer.  JDK8 is the minimum version supporting Arm64 but likely won’t provide the best performance.</li>
<li><a href="https://aws.amazon.com/corretto">Amazon Corretto</a> (Amazon’s distribution of OpenJDK):  Typically provides the best performance and is recommended for Graviton workloads.  Corretto 11 and above support LSE extensions, a set of atomic memory operations that improve performance for lock-contended workloads and reduce garbage collection time.  Some customers have seen even better performance on Corretto 17 and Corretto 21.</li>
</ul>
</li>
<li>
<p>Check JARs and shared objects for architecture specific code (compiled other than Java byte code)</p>
<ul>
<li>See guidance for <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/java.md#looking-for-x86-shared-objects-in-jars">manual scanning process</a>.</li>
<li><a href="https://github.com/aws/porting-advisor-for-graviton">Porting Advisor for Graviton</a> can scan/flag them and is useful for <a href="https://maven.apache.org/index.html">Maven</a>-based projects.</li>
<li>JNI extensions usually exist to implement performance critical functions in a language other than Java.  Without an Arm64 version the code may not work or can fall back on a slower pure Java implementation.  Check for Arm64 versions or later versions of the package to see if the JNI has been superseded by more performant native Java implementations.</li>
<li>Follow <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/java.md#building-multi-arch-jars">these instructions</a> for building multi-arch JAR’s that support both x86 and Arm64/Graviton.</li>
</ul>
</li>
<li>
<p>Java Crypto operations:</p>
<ul>
<li>Review <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/java.md#java-jvm-options">Java JVM options</a> for Crypto optimizations and recommendations.</li>
<li>AES/GCM benefits when using AES hardware instructions, which can improve performance by up to 5x for this algorithm.  Corretto &amp; OpenJDK 18 support this by default and have been back-ported to Corretto &amp; OpenJDK 11 and 17 which can be enabled using <code>XX:+UnlockDiagnosticVMOptions -XX:+UseAESCTRIntrinsics</code>.</li>
<li><a href="https://github.com/corretto/amazon-corretto-crypto-provider">Amazon-corretto-crypto-provider</a> is another option that offers optimizations for a large number of cryptographic operations.</li>
</ul>
</li>
<li>
<p>Application Testing &amp; Performance Evaluation</p>
<ul>
<li>Be sure to run Graviton instances “hotter”: vCPUs are mapped to physical cores instead of Hyperthreads and performance often flatlines at a much higher CPU utilization than with x86 based instances.  Testing at low levels of load can lead to misleading results.  The most realistic test results are usually achieved when testing close to breaking latency.</li>
<li>See the <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/perfrunbook/README.md">Graviton Performance Runbook</a> for more info.</li>
<li><a href="https://github.com/aws/aperf">Aperf</a> is a CLI tool for gathering &amp; visualizing performance data that can be helpful.</li>
</ul>
</li>
</ul>
<h3 id="java-versions"><a class="header" href="#java-versions">Java versions</a></h3>
<p>JDK binaries for arm64 are available from a number of
different sources.  <a href="https://aws.amazon.com/corretto/">Amazon Corretto</a> is
continuing to improve performance of Java workloads running on Graviton processors and
if you have the choice of a JDK to use we recommend using Corretto as it
provides the fastest way to get access to the performance improvements AWS is making.</p>
<p>Versions of Corretto released since October 2020 are built to use the
most optimal atomic operations within the JVM: Corretto11 (all
variants); Correto8 (on Amazon Linux 2 only). This has shown to reduce
GC time in some workloads, and avoids contention in net-intensive workloads like Apache Kafka.</p>
<p>Versions of Corretto11 (&gt;=11.0.12) come with additional enhancements to improve
performance on workloads with light to moderate lock-contention: improved spin-lock behavior inside the JVM,
enhanced implementation of <code>Thread.onSpinWait()</code> on Graviton.</p>
<h3 id="java-jvm-options"><a class="header" href="#java-jvm-options">Java JVM Options</a></h3>
<p>There are numerous options that control the JVM and may lead to better performance.</p>
<ul>
<li>
<p>Flags <code>-XX:-TieredCompilation -XX:ReservedCodeCacheSize=64M -XX:InitialCodeCacheSize=64M</code>
have shown large (1.5x) improvements in some Java workloads. Corretto 17 needs two additional flags:
<code>-XX:CICompilerCount=2 -XX:CompilationMode=high-only</code>. <code>ReservedCodeCacheSize</code>/<code>InitialCodeCacheSize</code>
should be equal and can be in range: 64M...127M.
The JIT compiler stores generated code in the code cache. The flags change the size of the code cache
from the default 240M to the smaller one. The smaller code cache may help CPU
to improve the caching and prediction of jitted code. The flags disable the tiered compilation
to make the JIT compiler able to use the smaller code cache.
These are helpful on some workloads but can hurt on others so testing with and without
them is essential.</p>
</li>
<li>
<p>Crypto algorithm AES/GCM used by TLS has been optimized for Graviton.
On Graviton2 GCM encrypt/decrypt
<a href="https://github.com/aws/aws-graviton-getting-started/issues/110#issuecomment-989442948">performance improves by 3.5x to 5x</a>.
The optimization is enabled by default in Corretto and OpenJDK 18 and later.
The optimization has been backported to Corretto and OpenJDK 11 and 17
and can be enabled with the flags <code>-XX:+UnlockDiagnosticVMOptions -XX:+UseAESCTRIntrinsics</code>.
As an alternative, you can use <a href="https://github.com/corretto/amazon-corretto-crypto-provider">Amazon Corretto Crypto Provider</a> JNI libraries.</p>
</li>
</ul>
<h3 id="java-stack-size"><a class="header" href="#java-stack-size">Java Stack Size</a></h3>
<p>The default stack size for Java threads (i.e. <code>ThreadStackSize</code>) is 2mb on aarch64 (compared to 1mb on x86_64). You can check the default with:</p>
<pre><code>$ java -XX:+PrintFlagsFinal -version | grep ThreadStackSize
     intx CompilerThreadStackSize = 2048  {pd product} {default}
     intx ThreadStackSize         = 2048  {pd product} {default}
     intx VMThreadStackSize       = 2048  {pd product} {default}
</code></pre>
<p>The default can be easily changed on the command line with either <code>-XX:ThreadStackSize=&lt;kbytes&gt;</code> or <code>-Xss&lt;bytes&gt;</code>. Notice that <code>-XX:ThreadStackSize</code> interprets its argument as kilobytes whereas <code>-Xss</code> interprets it as bytes. So <code>-XX:ThreadStackSize=1024</code> and <code>-Xss1m</code> will both set the stack size for Java threads to 1 megabyte:</p>
<pre><code>$ java -Xss1m -XX:+PrintFlagsFinal -version | grep ThreadStackSize
     intx CompilerThreadStackSize                  = 2048                                   {pd product} {default}
     intx ThreadStackSize                          = 1024                                   {pd product} {command line}
     intx VMThreadStackSize                        = 2048                                   {pd product} {default}
</code></pre>
<p>Usually, there's no need to change the default, because the thread stack will be committed lazily as it grows. So no matter what's the default, the thread will always only commit as much stack as it really uses (at page size granularity). However there's one exception to this rule if <a href="https://www.kernel.org/doc/html/latest/admin-guide/mm/transhuge.html">Transparent Huge Pages</a> (THP) are turned on by default on a system. In such a case the THP page size of 2mb matches exactly with the 2mb default stack size on aarch64 and most stacks will be backed up by a single huge page of 2mb. This means that the stack will be completely committed to memory right from the start. If you're using hundreds or even thousands of threads, this memory overhead can be considerable.</p>
<p>To mitigate this issue, you can either manually change the stack size on the command line (as described above) or you can change the default for THP from <code>always</code> to <code>madvise</code> on Linux distributions like AL2 (with Linux kernel 5 and higher) on which the setting defaults to <code>always</code>:</p>
<pre><code># cat /sys/kernel/mm/transparent_hugepage/enabled
[always] madvise never
# echo madvise &gt; /sys/kernel/mm/transparent_hugepage/enabled
# cat /sys/kernel/mm/transparent_hugepage/enabled
always [madvise] never
</code></pre>
<p>Notice that even if the the default is changed from <code>always</code> to <code>madvise</code>, the JVM can still use THP for the Java heap and code cache if you specify <code>-XX:+UseTransparentHugePages</code> on the command line.</p>
<h3 id="looking-for-x86-shared-objects-in-jars"><a class="header" href="#looking-for-x86-shared-objects-in-jars">Looking for x86 shared-objects in JARs</a></h3>
<p>Java JARs can include shared-objects that are architecture specific. Some Java libraries check
if these shared objects are found and if they are they use a JNI to call to the native library
instead of relying on a generic Java implementation of the function. While the code might work,
without the JNI the performance can suffer.</p>
<p>A quick way to check if a JAR contains such shared objects is to simply unzip it and check if
any of the resulting files are shared-objects and if an aarch64 (arm64) shared-object is missing:</p>
<pre><code>$ unzip foo.jar
$ find . -name "*.so" -exec file {} \;
</code></pre>
<p>For each x86-64 ELF file, check there is a corresponding aarch64 ELF file
in the binaries. With some common packages (e.g. commons-crypto) we've seen that
even though a JAR can be built supporting Arm manually, artifact repositories such as
Maven don't have updated versions. To see if a certain artifact version may have Arm support,
consult our <a href="CommonNativeJarsTable.html">Common JARs with native code Table</a>.
Feel free to open an issue in this GitHub repo or contact us at ec2-arm-dev-feedback@amazon.com
for advice on getting Arm support for a required Jar.</p>
<h3 id="building-multi-arch-jars"><a class="header" href="#building-multi-arch-jars">Building multi-arch JARs</a></h3>
<p>Java is meant to be a write once, and run anywhere language.  When building Java artifacts that
contain native code, it is important to build those libraries for each major architecture to provide
a seamless and optimally performing experience for all consumers.  Code that runs well on both Graviton and x86
based instances increases the package's utility.</p>
<p>There is nominally a multi-step process to build the native shared objects for each supported
architecture before doing the final packaging with Maven, SBT, Gradle etc. Below is an example
of how to create your JAR using Maven that contains shared libraries for multiple distributions
and architectures for running your Java application interchangeably on AWS EC2 instances
based on x86 and Graviton processors:</p>
<pre><code># Create two build instances, one x86 and one Graviton instance.
# Pick one instance to be the primary instance.
# Log into the secondary instance
$ cd java-lib
$ mvn package
$ find target/ -name "*.so" -type f -print

# Note the directory this so file is in, it will be in a directory
# such as: target/classes/org/your/class/hierarchy/native/OS/ARCH/lib.so

# Log into the primary build instance
$ cd java-lib
$ mvn package

# Repeat the below two steps for each OS and ARCH combination you want to release
$ mkdir target/classes/org/your/class/hierarchy/native/OS/ARCH
$ scp slave:~/your-java-lib/target/classes/org/your/class/hierarchy/native/OS/ARCH/lib.so target/classes/org/your/class/hierarchy/native/OS/ARCH/

# Create the jar packaging with maven.  It will include the additional
# native libraries even though they were not built directly by this maven process.
$ mvn package

# When creating a single Jar for all platform native libraries, 
# the release plugin's configuration must be modified to specify 
# the plugin's `preparationGoals` to not include the clean goal.
# See http://maven.apache.org/maven-release/maven-release-plugin/prepare-mojo.html#preparationGoals
# For more details.

# To do a release to Maven Central and/or Sonatype Nexus:
$ mvn release:prepare
$ mvn release:perform
</code></pre>
<p>This is one way to do the JAR packaging with all the libraries in a single JAR.  To build all the JARs, we recommend to build on native
machines, but it can also be done via Docker using the buildx plug-in, or by cross-compiling inside your build-environment.</p>
<p>Additional options for releasing jars with native code is to: use a manager plugin such as the <a href="https://maven-nar.github.io/">nar maven plugin</a>
to manage each platform specific Jar.  Release individual architecture specific jars, and then use the primary
instance to download these released jars and package them into a combined Jar with a final <code>mvn release:perform</code>.
An example of this methd can be found in the <a href="https://github.com/fusesource/leveldbjni">Leveldbjni-native</a> <code>pom.xml</code> files.</p>
<h3 id="remove-anti-patterns"><a class="header" href="#remove-anti-patterns">Remove Anti-patterns</a></h3>
<p>Anti-patterns can affect the performance on any instance family, but the level
of impact can be different.  Below is a list of
anti-patterns we have found to be particularly impactful on Graviton:</p>
<ol>
<li><strong>Excessive exceptions</strong>: Throwing exceptions and generating stack-traces
has been observed to cost up to 2x more on Graviton platforms compared to x86.
We recommend not to use Java exceptions as control flow, and to remove
exceptions when they appear in the hot-code path. Identifying hot exceptions can
be done using function profilers like <a href="https://github.com/aws/aperf">Aperf</a>,
<a href="https://github.com/async-profiler/async-profiler">Async-profiler</a>, or Linux <code>perf</code>.
Overhead can be mitigated some by using the <code>-XX:+OmitStackTraceInFastThrow</code> JVM
flag to allow the Java runtime to optimize the exception flow for some hot
paths. The best solution is to avoid the exceptions as much as possible.</li>
</ol>
<h3 id="profiling-java-applications"><a class="header" href="#profiling-java-applications">Profiling Java applications</a></h3>
<h3 id="aperf--async-profiler"><a class="header" href="#aperf--async-profiler">APerf + Async-profiler</a></h3>
<p>To profile Java we recommend using <a href="https://github.com/aws/aperf">Aperf</a> to gather profiles and view them
via a static webpage.  On your test system, follow the below directions to profile your
Java code using APerf:</p>
<pre><code class="language-bash"># Get latest APerf release onto the machine you are profiling.  
# As of Oct 28, 2024 the latest release is v0.13.0.
wget https://github.com/aws/aperf/releases/download/v0.1.13-alpha/aperf-v0.1.13-alpha-aarch64.tar.gz
tar -zxf aperf-v0.1.13-alpha-aarch64.tar.gz

# Get the latest Async profiler
wget https://github.com/async-profiler/async-profiler/releases/download/v3.0/async-profiler-3.0-linux-arm64.tar.gz
tar -zxf async-profiler-3.0-linux-arm64.tar.gz
cd async-profiler-3.0-linux-arm64
sudo mkdir -p /opt/bin
sudo mkdir -p /opt/lib
sudo cp -a bin/* /opt/bin
sudo cp -a lib/* /opt/lib

export PATH=/opt/bin:$PATH
export LD_LIBRARY_PATH=/opt/lib:$LD_LIBRARY_PATH
sudo sysctl -w kernel.kptr_restrict=0
sudo sysctl -w kernel.perf_event_paranoid=-1
cd aperf-v0.1.13-alpha-aarch64

# While the application is running
./aperf record --profile --profile-java --period 300 -r java_record
./aperf report -r java_record -n java_report

# In java_report folder, open index.html in a browser to view report
</code></pre>
<h3 id="linux-perf-and-libperf-jvmtiso"><a class="header" href="#linux-perf-and-libperf-jvmtiso">Linux <code>perf</code> and <code>libperf-jvmti.so</code></a></h3>
<p>If prefering to use the standard Linux <code>perf</code> tool, we can capture information about symbols
on Java JIT'ed code leveraging the <code>libperf-jvmti.so</code> agent.
Follow the below steps to use the <code>libperf-jvmti.so</code> to dump symbols for
JITed code as the JVM runs.</p>
<pre><code class="language-bash"># Compile your Java application with -g

# find where libperf-jvmti.so is on your distribution

# Run your java app with -agentpath:/path/to/libperf-jvmti.so added to the command line
# Launch perf record on the system
$ perf record -g -k 1 -a -o perf.data sleep 5

# Inject the generated methods information into the perf.data file
$ perf inject -j -i perf.data -o perf.data.jit

# View the perf report with symbol info
$ perf report -i perf.data.jit

# Process the new file, for instance via Brendan Gregg's Flamegraph tools
$ perf script -i perf.data.jit | ./FlameGraph/stackcollapse-perf.pl | ./FlameGraph/flamegraph.pl &gt; ./flamegraph.svg
</code></pre>
<h3 id="build-libperf-jvmtiso-on-amazon-linux-2"><a class="header" href="#build-libperf-jvmtiso-on-amazon-linux-2">Build libperf-jvmti.so on Amazon Linux 2</a></h3>
<p>Amazon Linux 2 does not package <code>libperf-jvmti.so</code> by default with the <code>perf</code> yum package for kernel versions &lt;5.10.
Build the <code>libperf-jvmti.so</code> shared library using the following steps:</p>
<pre><code class="language-bash">$ sudo amazon-linux-extras enable corretto8
$ sudo yum install -y java-1.8.0-amazon-corretto-devel

$ cd $HOME
$ sudo yumdownloader --source kernel

$ cat &gt; .rpmmacros &lt;&lt; __EOF__
%_topdir    %(echo $HOME)/kernel-source
__EOF__

$ rpm -ivh ./kernel-*.amzn2.src.rpm
$ sudo yum-builddep kernel
$ cd kernel-source/SPECS
$ rpmbuild -bp kernel.spec

$ cd ../BUILD
$ cd kernel-*.amzn2
$ cd linux-*.amzn2.aarch64
$ cd tools/perf
$ make
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h2 id="commonly-used-jars-that-package-native-artifacts"><a class="header" href="#commonly-used-jars-that-package-native-artifacts">Commonly used Jars that package native artifacts</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Org</th><th>jar</th><th>Builds on Arm</th><th>Arm Artifact available</th><th>Minimum Version</th></tr></thead><tbody>
<tr><td>com.github.luben</td><td><a href="https://github.com/luben/zstd-jni">zstd-jni</a></td><td>yes</td><td><a href="https://mvnrepository.com/artifact/com.github.luben/zstd-jni">yes</a></td><td>1.2.0</td></tr>
<tr><td>org.lz4</td><td><a href="https://github.com/lz4/lz4-java">lz4-java</a></td><td>yes</td><td><a href="https://mvnrepository.com/artifact/org.lz4/lz4-java">yes</a></td><td>1.4.0</td></tr>
<tr><td>org.xerial.snappy</td><td><a href="https://github.com/xerial/snappy-java">snappy-java</a></td><td>yes</td><td><a href="https://mvnrepository.com/artifact/org.xerial.snappy/snappy-java">yes</a></td><td>1.1.4</td></tr>
<tr><td>org.rocksdb</td><td><a href="https://github.com/facebook/rocksdb/tree/master/java">rocksdbjni</a></td><td>yes</td><td><a href="https://mvnrepository.com/artifact/org.rocksdb/rocksdbjni">yes</a></td><td>5.0.1 (7.4.3+ recommended)</td></tr>
<tr><td>com.github.jnr</td><td><a href="https://github.com/jnr/jffi">jffi</a></td><td>yes</td><td><a href="https://mvnrepository.com/artifact/com.github.jnr/jffi">yes</a></td><td>1.2.13</td></tr>
<tr><td>org.apache.commons</td><td><a href="https://github.com/apache/commons-crypto">commons-crypto</a></td><td>yes</td><td><a href="https://search.maven.org/artifact/org.apache.commons/commons-crypto/1.1.0/jar">yes</a></td><td>1.1.0</td></tr>
<tr><td>io.netty</td><td><a href="https://github.com/netty/netty">netty-transport-native-epoll</a></td><td>yes</td><td><a href="https://mvnrepository.com/artifact/io.netty/netty-transport-native-epoll">yes</a></td><td>4.1.50</td></tr>
<tr><td>io.netty</td><td><a href="https://github.com/netty/netty-tcnative">netty-tcnative</a></td><td>yes</td><td><a href="https://mvnrepository.com/artifact/io.netty/netty-tcnative">yes</a></td><td>2.0.31</td></tr>
<tr><td>org.fusesource.jansi</td><td><a href="https://github.com/fusesource/jansi-native">jansi-native</a></td><td>yes</td><td>no</td><td></td></tr>
<tr><td>org.fusesource.leveldbjni</td><td><a href="https://github.com/fusesource/leveldbjni">leveldbjni-all</a></td><td>no</td><td>no</td><td></td></tr>
<tr><td>org.fusesource.sigar</td><td><a href="https://github.com/hyperic/sigar">sigar</a></td><td>yes (refer https://github.com/hyperic/sigar/pull/140)</td><td><a href="https://pkgs.org/download/libhyperic-sigar-java">debian</a></td><td>1.6.4</td></tr>
<tr><td>org.apache.hadoop</td><td><a href="https://github.com/twitter/hadoop-lzo">hadoop-lzo</a></td><td>yes</td><td>no</td><td></td></tr>
</tbody></table>
</div>
<hr />
<p>Updated on 2022-08-02</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="net-on-graviton"><a class="header" href="#net-on-graviton">.NET on Graviton</a></h1>
<p>.NET is an open-source platform for writing different types of applications. Software engineers can write .NET based applications in multiple languages such as C#, F#, and Visual Basic. .NET applications are compiled into Common Intermediate Language (CIL). When an application is executed, the Common Language Runtime (CLR) loads that application binary and uses a just-in-time (JIT) compiler to generate machine code for the architecture being executed on. For more information, please see <a href="https://dotnet.microsoft.com/learn/dotnet/what-is-dotnet">what is .NET</a>.</p>
<h2 id="net-versions"><a class="header" href="#net-versions">.NET Versions</a></h2>
<div class="table-wrapper"><table><thead><tr><th>Version</th><th>Linux Arm32</th><th>Linux Arm64</th><th>Notes</th></tr></thead><tbody>
<tr><td>.NET 9</td><td>Yes</td><td>Yes</td><td>v9.0.0 released November 12, 2024 with Arm64 Linux builds. See also <a href="https://learn.microsoft.com/en-us/dotnet/core/whats-new/dotnet-9/runtime#arm64-vectorization-in-net-libraries">Arm64 vectorization in .NET libraries</a>.</td></tr>
<tr><td>.NET 8</td><td>Yes</td><td>Yes</td><td>v8.0.0 released November 14, 2023 with Arm64 Linux builds. See also <a href="https://devblogs.microsoft.com/dotnet/this-arm64-performance-in-dotnet-8/">Arm64 Performance Improvements in .NET 8</a>. For details on .NET 8 and Graviton, check out this blog: <a href="https://aws.amazon.com/blogs/dotnet/powering-net-8-with-aws-graviton3-benchmarks/">Powering .NET 8 with AWS Graviton3: Benchmarks</a></td></tr>
<tr><td>.NET 7</td><td>Yes</td><td>Yes</td><td>v7.0.0 released November 8, 2022 with Arm64 Linux builds. For more details check out this video: <a href="https://www.youtube.com/watch?v=V4Lxs5TbaFk">Boosting .NET application performance with Arm64 and AWS Graviton 3</a> Note that .NET 7 is <a href="https://dotnet.microsoft.com/en-us/platform/support/policy/dotnet-core#lifecycle">out of support</a>.</td></tr>
<tr><td><a href="https://dotnet.microsoft.com/download/dotnet/6.0">.NET 6</a></td><td>Yes</td><td>Yes</td><td>V6.0.0 released November 8, 2021 with Arm64 Linux builds. For more details check out this blog: <a href="https://aws.amazon.com/blogs/developer/net-6-on-aws/">.NET 6 on AWS</a> and video: <a href="https://www.youtube.com/watch?v=iMlyZI9NhFw">AWS re:Invent 2021 - Accelerate .NET 6 performance with Arm64 on AWS Graviton2</a></td></tr>
<tr><td><a href="https://dotnet.microsoft.com/download/dotnet/5.0">.NET 5</a></td><td>Yes</td><td>Yes</td><td>Arm64-specific optimizations in the .NET libraries and the code produced by RyuJIT. <a href="https://devblogs.microsoft.com/dotnet/arm64-performance-in-net-5/">Arm64 Performance in .NET 5</a>. Note that .NET 5 is <a href="https://dotnet.microsoft.com/en-us/platform/support/policy/dotnet-core#lifecycle">out of support</a>.</td></tr>
<tr><td><a href="https://dotnet.microsoft.com/learn/dotnet/what-is-dotnet-framework">.NET Framework 4.x</a></td><td>No</td><td>No</td><td>The original implementation of the .NET Framework does not support Linux hosts, and Windows hosts are not suported on Graviton.</td></tr>
<tr><td><a href="https://dotnet.microsoft.com/download/dotnet/3.1">.NET Core 3.1</a></td><td>Yes</td><td>Yes</td><td>.NET Core 3.0 added support for <a href="https://docs.microsoft.com/en-us/dotnet/core/whats-new/dotnet-core-3-0#linux-improvements">Arm64 for Linux</a>. Note that .NET Core 3.1 is <a href="https://dotnet.microsoft.com/en-us/platform/support/policy/dotnet-core#lifecycle">out of support</a>.</td></tr>
<tr><td><a href="https://dotnet.microsoft.com/download/dotnet/2.1">.NET Core 2.1</a></td><td>Yes*</td><td>No</td><td>Initial support was for <a href="https://github.com/dotnet/announcements/issues/82">Arm32 was added to .NET Core 2.1</a>. *Operating system support is limited, please see the <a href="https://github.com/dotnet/core/blob/main/release-notes/2.1/2.1-supported-os.md">official documentation</a>. Note that .NET Core 2.1 is <a href="https://dotnet.microsoft.com/en-us/platform/support/policy/dotnet-core#lifecycle">out of support</a>.</td></tr>
</tbody></table>
</div>
<h2 id="net-5"><a class="header" href="#net-5">.NET 5</a></h2>
<p>With .NET 5 Microsoft has made specific Arm64 architecture optimizations. These optimizations were made in the .NET libraries as well as in the machine code output by the JIT process.</p>
<ul>
<li>AWS DevOps Blog <a href="https://aws.amazon.com/blogs/devops/build-and-deploy-net-web-applications-to-arm-powered-aws-graviton-2-amazon-ecs-clusters-using-aws-cdk/">Build and Deploy .NET web applications to ARM-powered AWS Graviton 2 Amazon ECS Clusters using AWS CDK</a></li>
<li>AWS Compute Blog <a href="https://aws.amazon.com/blogs/compute/powering-net-5-with-aws-graviton2-benchmark-results/">Powering .NET 5 with AWS Graviton2: Benchmarks</a></li>
<li>Microsoft .NET Blog <a href="https://devblogs.microsoft.com/dotnet/arm64-performance-in-net-5/">ARM64 Performance in .NET 5</a></li>
</ul>
<h2 id="building--publishing-for-linux-arm64"><a class="header" href="#building--publishing-for-linux-arm64">Building &amp; Publishing for Linux Arm64</a></h2>
<p>The .NET SDK supports choosing a <a href="https://docs.microsoft.com/en-us/dotnet/core/rid-catalog">Runtime Identifier (RID)</a> used to target platforms where the applications run. These RIDs are used by .NET dependencies (NuGet packages) to represent platform-specific resources in NuGet packages. The following values are examples of RIDs: linux-arm64, linux-x64, ubuntu.14.04-x64, win7-x64, or osx.10.12-x64. For the NuGet packages with native dependencies, the RID designates on which platforms the package can be restored.</p>
<p>You can build and publish on any host operating system. As an example, you can develop on Windows and build locally to target Arm64, or you can use a CI server like Jenkins on Linux. The commands are the same.</p>
<pre><code class="language-bash">dotnet build -r linux-arm64
dotnet publish -c Release -r linux-arm64
</code></pre>
<p>For more information about <a href="https://docs.microsoft.com/en-us/dotnet/core/deploying/deploy-with-cli">publishing .NET apps with the .NET CLI</a> please see the offical documents.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="nodejs-on-graviton"><a class="header" href="#nodejs-on-graviton">Node.js on Graviton</a></h1>
<p>Graviton is an excellent choice for running web applications with Node.js. There
are a few considerations to be aware of to get the best performance.</p>
<h2 id="use-multiprocessing"><a class="header" href="#use-multiprocessing">Use Multiprocessing</a></h2>
<p>Node.JS is fundamentally single threaded and so on an instance with more than
one vCPU (which is most of them!), the node process will leave the CPU
underutilized. There a few ways to improve this.</p>
<ol>
<li>Use a load balancer, such as Nginx, to balance incoming HTTP requests across
multiple processes.</li>
<li>Use a built in module, <code>cluster</code> to balance the load across several forks of
the node process.</li>
</ol>
<p>The details of how to do this is beyond the scope of this document, but can be
easily found with a few quick searches of the web.</p>
<h2 id="use-statically-linked-builds"><a class="header" href="#use-statically-linked-builds">Use Statically Linked Builds</a></h2>
<p>If you download compiled binaries of the Node from Nodejs.org, you will have
statically linked binaries. Some package managers distribute Node as a thin
<code>node</code> binary which is dynamically linked to <code>libnode.so</code> where most of the code
lives. This is fine and allows other applications to link with <code>libnode.so</code>, but
it adds a small amount of extra overhead in each function call since each one
must use an extra step of indirection to load the destination function address.
This hardly matters at all until your application reaches a threshold volume of
incoming requests and it can no longer service all requests coming in. In a
dynamically linked <code>node</code>, this threshold will be lower. This is true on on all
EC2 instance types; it is not unique to Graviton.</p>
<h2 id="applications-using-many-and-complex-regular-expressions"><a class="header" href="#applications-using-many-and-complex-regular-expressions">Applications Using Many and Complex Regular Expressions</a></h2>
<p>A shortcoming in the just in time compiler in V8 for aarch64 creates a long link
chain of veeneers when evaluating complex regular expressions. A new version of
V8 addresses this, but it has not yet been merged into NodeJS main. If your
application relies heavily on regular expression performance AND you find that
the performance is lower on Graviton, try adding <code>--regexp-interpret-all</code> to
the node arguments to force V8 to interpret rather than compile regular
expressions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="php-on-graviton"><a class="header" href="#php-on-graviton">PHP on Graviton</a></h1>
<p>PHP is a general-purpose scripting language geared towards web development.
PHP scripts are executed by an interpreter implemented as a plug-in module
in web servers, a separate daemon (php-fpm) or a CGI executable (php-cgi).</p>
<p>PHP 7.4 and later are tested to perform well on Graviton. It works out of
the box on Ubuntu 22.04 and AL2023, but requires extra steps on AL2.</p>
<h3 id="opcache-on-amazon-linux-2-al2"><a class="header" href="#opcache-on-amazon-linux-2-al2">OPcache on Amazon Linux 2 (AL2)</a></h3>
<p>OPcache improves PHP performance by storing precompiled script bytecode in shared memory, thereby removing
the need for PHP to load and parse scripts on each request. Installing it can significantly improve
execution time on most workloads. More information about OPcache available in the
<a href="https://www.php.net/manual/en/book.opcache.php">PHP Manual</a>.</p>
<p>OPcache is installed by default on Amazon Linux 2023 (AL2023) and later, but not yet available in Amazon Linux 2 (AL2).
See <a href="php-opcache-al2.html">PHP OPcache Installation on AL2</a> for manual build and install instructions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="php-opcache-installation-on-amazon-linux-2-al2"><a class="header" href="#php-opcache-installation-on-amazon-linux-2-al2">PHP OPcache Installation on Amazon Linux 2 (AL2)</a></h1>
<h3 id="install-php"><a class="header" href="#install-php">Install PHP</a></h3>
<p>First run <code>sudo amazon-linux-extras install -y php8.0</code> to install PHP 8 from AL2 extras, if not already installed.</p>
<h3 id="sanity-check"><a class="header" href="#sanity-check">Sanity Check</a></h3>
<p>Verify that OPcache is not already present after installation; stop here if so.</p>
<p>Run the following commands to see if "opcache.so" is present and enabled in php.ini.
<code>php --version</code> prints a "with Zend OPcache" line on successful load.</p>
<pre><code>$ file /usr/lib64/php/modules/opcache.so
/usr/lib64/php/modules/opcache.so: ELF 64-bit LSB shared object      &lt;-- already installed

$ php --version
PHP 8.0.30 (cli) (built: Aug 24 2023 20:32:36) ( NTS )
Copyright (c) The PHP Group
Zend Engine v4.0.30, Copyright (c) Zend Technologies
    with Zend OPcache v8.0.30, Copyright (c), by Zend Technologies   &lt;-- already enabled
</code></pre>
<h3 id="install-dependencies"><a class="header" href="#install-dependencies">Install Dependencies</a></h3>
<p>Install PHP dependencies required to build OPcache. This is ideally done by running <code>sudo yum-builddep php</code>,
which fails in some configurations due to packaging conflict requiring both <code>libzip010-compat-devel</code> and <code>libzip-devel</code>.
Run the following as a workaround:</p>
<pre><code>sudo yum install apr apr-devel apr-util apr-util-bdb apr-util-devel aspell aspell-devel autoconf automake bzip2-devel cpp cyrus-sasl cyrus-sasl-devel elfutils-devel elfutils-libelf-devel enchant enchant-devel expat-devel freetype-devel gcc gcc-c++ gdbm-devel generic-logos-httpd glib2-devel gmp-devel httpd httpd-devel httpd-filesystem httpd-tools libacl-devel libatomic libattr-devel libcurl-devel libdb-devel libedit-devel libgcrypt-devel libgpg-error-devel libICE libicu-devel libitm libjpeg-turbo-devel libmpc libpng-devel libsanitizer libSM libsodium libsodium-devel libtool libtool-ltdl libtool-ltdl-devel libwebp-devel libX11 libX11-common libX11-devel libXau libXau-devel libxcb libxcb-devel libXext libxml2-devel libXpm libXpm-devel libxslt libxslt-devel libXt libzip-devel lm_sensors-devel m4 mailcap mod_http2 mpfr ncurses-c++-libs ncurses-devel net-snmp net-snmp-agent-libs net-snmp-devel net-snmp-libs oniguruma oniguruma-devel openldap-devel pam-devel perl-devel perl-ExtUtils-Install perl-ExtUtils-MakeMaker perl-ExtUtils-Manifest perl-ExtUtils-ParseXS perl-Test-Harness popt-devel postgresql postgresql-devel pyparsing recode recode-devel rpm-devel sqlite-devel systemd-devel systemtap-sdt-devel t1lib t1lib-devel tcp_wrappers-devel tokyocabinet tokyocabinet-devel unixODBC unixODBC-devel xorg-x11-proto-devel xz-devel
</code></pre>
<h3 id="build-source-rpm"><a class="header" href="#build-source-rpm">Build Source RPM</a></h3>
<pre><code>cd ~
yumdownloader --source php
rpm -ivh ./php-8.0.30-1.amzn2.src.rpm
sudo yum-builddep php
cd ./rpmbuild/SPECS
rpmbuild -ba php.spec
</code></pre>
<h3 id="install-opcache"><a class="header" href="#install-opcache">Install OPcache</a></h3>
<pre><code>cd ~/rpmbuild/BUILD
sudo cp ./php-8.0.30/build-cgi/modules/opcache.so /usr/lib64/php/modules/opcache.so
sudo cp ./php-8.0.30/10-opcache.ini /etc/php.d/10-opcache.ini
</code></pre>
<p>Verify installation by running <code>php --version</code>. Output show now look similar to above examples.
Reboot your instance or restart php-fpm and your http server to use OPcache.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="python-on-graviton"><a class="header" href="#python-on-graviton">Python on Graviton</a></h1>
<p>Python is an interpreted, high-level, general-purpose programming language, with interpreters available for many operating systems and architectures, including arm64. <em><a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Read more on Wikipedia</a></em></p>
<h2 id="1-installing-python-packages"><a class="header" href="#1-installing-python-packages">1. Installing Python packages</a></h2>
<p>When <em>pip</em> (the standard package installer for Python) is used, it pulls the packages from <a href="https://pypi.org">Python Package Index</a> and other indexes. To ensure you
can install binary packages from <a href="https://pypi.org">Python Package Index</a>, make sure to update your pip installation to a new enough version (&gt;19.3).</p>
<pre><code># To ensure an up-to-date pip version
sudo python3 -m pip install --upgrade pip
</code></pre>
<p>AWS is actively working to make pre-compiled packages available for Graviton. You can see a current list of the over 200 popular python packages we track nightly
for AL2 and Ubuntu for Graviton support status at our <a href="https://geoffreyblake.github.io/arm64-python-wheel-tester/">Python wheel tester</a>.</p>
<p>In the case <em>pip</em> could not find a pre-compiled package, it automatically downloads, compiles, and builds the package from source code.
Normally it may take a few more minutes to install the package from source code than from pre-built.  For some large packages,
it may take up to 20 minutes. In some cases, compilation may fail due to missing dependencies.  Before trying to build a python package from source, try
<code>python3 -m pip install --prefer-binary &lt;package&gt;</code> to attempt to install a wheel that is not the latest version.  Sometimes automated package builders
will push a release without all the wheels due to failures during a build that will be corrected at a later date.  If this is not an option, follow
the following instructions to build a python package from source.</p>
<h3 id="11-prerequisites-for-installing-python-packages-from-source"><a class="header" href="#11-prerequisites-for-installing-python-packages-from-source">1.1 Prerequisites for installing Python packages from source</a></h3>
<p>For installing common Python packages from source code, we need to install the following development tools:</p>
<p>On <strong>AmazonLinux2 or RedHat</strong>:</p>
<pre><code>sudo yum install "@Development tools" python3-pip python3-devel blas-devel gcc-gfortran lapack-devel
python3 -m pip install --user --upgrade pip
</code></pre>
<p>On <strong>Debian/Ubuntu</strong>:</p>
<pre><code>sudo apt update
sudo apt-get install build-essential python3-pip python3-dev libblas-dev gfortran liblapack-dev
python3 -m pip install --user --upgrade pip
</code></pre>
<p>On all distributions, additional compile time dependencies might be needed depending on the Python modules you are trying to install.</p>
<h3 id="12-recommended-versions"><a class="header" href="#12-recommended-versions">1.2 Recommended versions</a></h3>
<p>When adopting Graviton, it is recommended to use recent software versions as much as possible, and Python is no exception.</p>
<p>Python 2.7 is EOL since January the 1st 2020, it is definitely recommended to upgrade to a Python 3.x version before moving to Graviton.</p>
<p>Python 3.9 will reach <a href="https://devguide.python.org/versions/">EOL in October, 2025</a>, so when starting to port an application to Graviton, it is recommended to target at least Python 3.10.</p>
<h3 id="13-python-on-al2-and-rhel-8"><a class="header" href="#13-python-on-al2-and-rhel-8">1.3 Python on AL2 and RHEL 8</a></h3>
<p>AL2 and RHEL 8 distribute older Pythons by default: 3.7 and 3.6 respectively.  Python 3.6 is EOL
<a href="https://endoflife.date/python">since December, 2021</a> and Python 3.7 will be EOL <a href="https://endoflife.date/python">on June 2023</a>.
Therefore, some package maintainers have already begun dropping support for
Python 3.6 and 3.7 by omitting prebuilt wheels published to <a href="https://pypi.org">pypi.org</a>.
For some packages, it is still possible to use the default Python by using the distribution
from the package manager. For example <code>numpy</code> no longer publishes Python 3.6 wheels,
but can be installed from the package manager: <code>yum install python3-numpy</code>.</p>
<p>Another option is to use Python 3.8 instead of the default Python pacakge. You can
install Python 3.8 and pip: <code>yum install python38-pip</code>. Then use pip to install
the latest versions of packages: <code>pip3 install numpy</code>.  On AL2, you will need to use <code>amazon-linux-extras enable python3.8</code> to expose Python 3.8 packages.</p>
<p>Some common Python packages that are distributed by the package manager are:</p>
<ol>
<li>python3-numpy</li>
<li>python3-markupsafe</li>
<li>python3-pillow</li>
</ol>
<p>To see a full list run: <code>yum search python3</code></p>
<h3 id="python-wheel-glibc-requirements"><a class="header" href="#python-wheel-glibc-requirements">Python wheel glibc requirements</a></h3>
<p>Some python wheel packages installed with <code>pip</code> require newer libc versions implicitly and will fail to import properly in some cases with a similar
error message as below:</p>
<pre><code>ImportError: /lib64/libm.so.6: version `GLIBC_2.27' not found
</code></pre>
<p>This can be a problem on distributions such as Amazon Linux 2 that ship with a relatively old glibc (v2.26 in case of Amazon Linux 2).
This happens because  <code>pip</code> does a simple string match on the wheel filename to determine if a wheel will be compatible with the system.
In these cases, it is recommended to first identify if a version of the package is available through the distro's package manager,
install an older version of the package if able, or finally upgrade to a distro that uses a newer glibc -- such as AL2023, Ubuntu 20.04, or Ubuntu 22.04.</p>
<h2 id="2-scientific-and-numerical-application-numpy-scipy-blas-etc"><a class="header" href="#2-scientific-and-numerical-application-numpy-scipy-blas-etc">2. Scientific and numerical application (NumPy, SciPy, BLAS, etc)</a></h2>
<p>Python relies on native code to achieve high performance.  For scientific and
numerical applications NumPy and SciPy provide an interface to high performance
computing libraries such as ATLAS, BLAS, BLIS, OpenBLAS, etc.  These libraries
contain code tuned for Graviton processors.</p>
<p>It is recommended to use the latest software versions as much as possible. If the latest
version migration is not feasible, please ensure that it is at least the minimum version
recommended below because multiple fixes related to data precision and correctness on
aarch64 went into OpenBLAS between v0.3.9 and v0.3.17 and the below SciPy and NumPy versions
upgraded OpenBLAS from v0.3.9 to OpenBLAS v0.3.17.</p>
<p>OpenBLAS:  &gt;= v0.3.17
SciPy: &gt;= v1.7.2
NumPy: &gt;= 1.21.1</p>
<p>Both <a href="https://pypi.org/project/scipy/1.5.3/#files">SciPy&gt;=1.5.3</a> and <a href="https://pypi.org/project/numpy/1.19.0/#files">NumPy&gt;=1.19.0</a>
vend binary wheel packages for Aarch64, but if you need better performance, then
compiling the best performance numerical library is an option.  To do so, follow the below instructions.</p>
<h3 id="21-install-openblas"><a class="header" href="#21-install-openblas">2.1 Install OpenBLAS</a></h3>
<p>OpenBLAS is an optimized BLAS (Basic Linear Algebra Subprograms) library based on GotoBLAS2 1.13 BSD version. The library provides optimized "gemv" and "gemm" routines for Graviton architecture. Binary distribuiton is available for both "pthread" and "openmp" runtime, with "openblas" being the pthread version and "openblas-openmp" the openmp version. Install the appropriate version based on the execution runtime.</p>
<h4 id="211-install-openblas-on-ubuntu-and-debian"><a class="header" href="#211-install-openblas-on-ubuntu-and-debian">2.1.1 Install OpenBLAS on Ubuntu and Debian</a></h4>
<pre><code># pthread version
sudo apt -y install libopenblas-dev

# openmp version
sudo apt -y install libopenblas-openmp-dev
</code></pre>
<h4 id="212-install-openblas-on-amazonlinux2023-al2023-and-redhat"><a class="header" href="#212-install-openblas-on-amazonlinux2023-al2023-and-redhat">2.1.2 Install OpenBLAS on AmazonLinux2023 (AL2023) and RedHat</a></h4>
<pre><code># pthread version
sudo yum -y install openblas

# openmp version
sudo yum -y install openblas-openmp
</code></pre>
<h4 id="213-install-openblas-with-conda"><a class="header" href="#213-install-openblas-with-conda">2.1.3 Install OpenBLAS with Conda</a></h4>
<p>Please refer to the Graviton Support in Conda section to setup conda environment.</p>
<pre><code># pthread version
conda install -y openblas

# openmp version
conda install -y openblas=*=*openmp*
</code></pre>
<h3 id="22-blis-may-be-a-faster-blas"><a class="header" href="#22-blis-may-be-a-faster-blas">2.2 BLIS may be a faster BLAS</a></h3>
<p>The default SciPy and NumPy binary installations with <code>pip3 install numpy scipy</code>
are configured to use OpenBLAS.  The default installations of SciPy and NumPy
are easy to setup and well tested.</p>
<p>Some workloads will benefit from using BLIS. Benchmarking SciPy and NumPy
workloads with BLIS might allow to identify additional performance improvement.</p>
<h3 id="23-install-numpy-and-scipy-with-blis-on-ubuntu-and-debian"><a class="header" href="#23-install-numpy-and-scipy-with-blis-on-ubuntu-and-debian">2.3 Install NumPy and SciPy with BLIS on Ubuntu and Debian</a></h3>
<p>On Ubuntu and Debian <code>apt install python3-numpy python3-scipy</code> will install NumPy
and SciPy with BLAS and LAPACK libraries. To install SciPy and NumPy with BLIS
and OpenBLAS on Ubuntu and Debian:</p>
<pre><code>sudo apt -y install python3-scipy python3-numpy libopenblas-dev libblis-dev
sudo update-alternatives --set libblas.so.3-aarch64-linux-gnu \
    /usr/lib/aarch64-linux-gnu/blis-openmp/libblas.so.3
</code></pre>
<p>To switch between available alternatives:</p>
<pre><code>sudo update-alternatives --config libblas.so.3-aarch64-linux-gnu
sudo update-alternatives --config liblapack.so.3-aarch64-linux-gnu
</code></pre>
<h3 id="24-install-numpy-and-scipy-with-blis-on-amazonlinux2-al2-and-redhat"><a class="header" href="#24-install-numpy-and-scipy-with-blis-on-amazonlinux2-al2-and-redhat">2.4 Install NumPy and SciPy with BLIS on AmazonLinux2 (AL2) and RedHat</a></h3>
<p>Prerequisites to build SciPy and NumPy with BLIS on arm64 AL2 and RedHat:</p>
<pre><code># Install AL2/RedHat prerequisites
sudo yum install "@Development tools" python3-pip python3-devel blas-devel gcc-gfortran

# Install BLIS
git clone https://github.com/flame/blis $HOME/blis
cd $HOME/blis;  ./configure --enable-threading=openmp --enable-cblas --prefix=/usr cortexa57
make -j4;  sudo make install

# Install OpenBLAS
git clone https://github.com/xianyi/OpenBLAS.git $HOME/OpenBLAS
cd $HOME/OpenBLAS
make -j4 BINARY=64 FC=gfortran USE_OPENMP=1 NUM_THREADS=64
sudo make PREFIX=/usr install
</code></pre>
<p>To build and install NumPy and SciPy with BLIS and OpenBLAS:</p>
<pre><code>git clone https://github.com/numpy/numpy/ $HOME/numpy
cd $HOME/numpy;  pip3 install .

git clone https://github.com/scipy/scipy/ $HOME/scipy
cd $HOME/scipy;  pip3 install .
</code></pre>
<p>When NumPy and SciPy detect the presence of the BLIS library at build time, they
will use BLIS in priority over the same functionality from BLAS and
OpenBLAS. OpenBLAS or LAPACK libraries need to be installed along BLIS to
provide LAPACK functionality.  To change the library dependencies, one can set
environment variables <code>NPY_BLAS_ORDER</code> and <code>NPY_LAPACK_ORDER</code> before building numpy
and scipy. The default is:
<code>NPY_BLAS_ORDER=mkl,blis,openblas,atlas,accelerate,blas</code> and
<code>NPY_LAPACK_ORDER=mkl,openblas,libflame,atlas,accelerate,lapack</code>.</p>
<h3 id="25-testing-numpy-and-scipy-installation"><a class="header" href="#25-testing-numpy-and-scipy-installation">2.5 Testing NumPy and SciPy installation</a></h3>
<p>To test that the installed NumPy and SciPy are built with BLIS and OpenBLAS, the
following commands will print native library dependencies:</p>
<pre><code>python3 -c "import numpy as np; np.__config__.show()"
python3 -c "import scipy as sp; sp.__config__.show()"
</code></pre>
<p>In the case of Ubuntu and Debian these commands will print <code>blas</code> and <code>lapack</code>
which are symbolic links managed by <code>update-alternatives</code>.</p>
<h3 id="26-improving-blis-and-openblas-performance-with-multi-threading"><a class="header" href="#26-improving-blis-and-openblas-performance-with-multi-threading">2.6 Improving BLIS and OpenBLAS performance with multi-threading</a></h3>
<p>When OpenBLAS is built with <code>USE_OPENMP=1</code> it will use OpenMP to parallelize the
computations.  The environment variable <code>OMP_NUM_THREADS</code> can be set to specify
the maximum number of threads.  If this variable is not set, the default is to
use a single thread.</p>
<p>To enable parallelism with BLIS, one needs to both configure with
<code>--enable-threading=openmp</code> and set the environment variable <code>BLIS_NUM_THREADS</code>
to the number of threads to use, the default is to use a single thread.</p>
<h3 id="27-graviton-support-in-conda--anaconda"><a class="header" href="#27-graviton-support-in-conda--anaconda">2.7 Graviton support in Conda / Anaconda</a></h3>
<p>Anaconda is a distribution of the Python and R programming languages for scientific computing, that aims to simplify package management and deployment.</p>
<p>Anaconda has announced <a href="https://www.anaconda.com/blog/anaconda-aws-graviton2">support for AWS Graviton on May 14, 2021</a>.</p>
<p>Instructions to install the full Anaconda package installer can be found at https://docs.anaconda.com/anaconda/install/graviton2/ .</p>
<p>Anaconda also offers a lightweight version called <a href="https://docs.conda.io/en/latest/miniconda.html">Miniconda</a> which is a small, bootstrap version of Anaconda that includes only conda, Python, the packages they depend on, and a small number of other useful packages, including pip, zlib and a few others.</p>
<p>Here is an example on how to use it to install <a href="https://numpy.org/">numpy</a> and <a href="https://pandas.pydata.org/">pandas</a> for Python 3.9.</p>
<p>The first step is to install conda:</p>
<pre><code>$ wget https://repo.anaconda.com/miniconda/Miniconda3-py39_4.10.3-Linux-aarch64.sh
$ chmod a+x Miniconda3-py39_4.10.3-Linux-aarch64.sh
$ ./Miniconda3-py39_4.10.3-Linux-aarch64.sh
</code></pre>
<p>Once installed, you can either use the <code>conda</code> command directly to install packages, or write an environment definition file and create the corresponding environment.</p>
<p>Here's an example to install <a href="https://numpy.org/">numpy</a> and <a href="https://pandas.pydata.org/">pandas</a> (<code>graviton-example.yml</code>):</p>
<pre><code>name: graviton-example
dependencies:
  - numpy
  - pandas
</code></pre>
<p>The next step is to instantiate the environment from that definition:</p>
<pre><code>$ conda env create -f graviton-example.yml
</code></pre>
<h2 id="3-machine-learning-python-packages"><a class="header" href="#3-machine-learning-python-packages">3. Machine Learning Python packages</a></h2>
<h3 id="31-pytorch"><a class="header" href="#31-pytorch">3.1 PyTorch</a></h3>
<pre><code>pip install numpy
pip install torch torchvision
</code></pre>
<p>Please refer to the <a href="machinelearning/pytorch.html">Graviton PyTorch user guide</a> for optimizing PyTorch inference performance on Graviton.</p>
<h3 id="32-tensorflow"><a class="header" href="#32-tensorflow">3.2 TensorFlow</a></h3>
<pre><code>pip install tensorflow
</code></pre>
<p>Please refer to the <a href="machinelearning/tensorflow.html">Graviton TensorFlow user guide</a> for the recommended configuration and best practices.</p>
<h3 id="33-dgl"><a class="header" href="#33-dgl">3.3 DGL</a></h3>
<p>Make sure Pytorch is installed,  if not, follow <a href="machinelearning/pytorch.html">Pytorch installation steps</a></p>
<p>On <strong>Ubuntu</strong>:</p>
<p>Follow the <a href="https://github.com/dmlc/dgl/blob/master/docs/source/install/index.rst#install-from-source">install from source</a> instructions.</p>
<h3 id="34-sentencepiece"><a class="header" href="#34-sentencepiece">3.4 Sentencepiece</a></h3>
<p><a href="https://pypi.org/project/sentencepiece/0.1.94/#history">Sentencepiece&gt;=1.94 now has pre-compiled binary wheels available for Graviton</a>.</p>
<h3 id="35-morfeusz"><a class="header" href="#35-morfeusz">3.5	Morfeusz</a></h3>
<p>On <strong>Ubuntu</strong>:</p>
<pre><code># download the source
wget http://download.sgjp.pl/morfeusz/20200913/morfeusz-src-20200913.tar.gz
tar -xf morfeusz-src-20200913.tar.gz
cd Morfeusz/
sudo apt install cmake zip build-essential autotools-dev \
    python3-stdeb python3-pip python3-all-dev python3-pyparsing devscripts \
    libcppunit-dev acl  default-jdk swig python3-all-dev python3-stdeb
export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF8
mkdir build
cd build
cmake ..
sudo make install
sudo ldconfig -v
sudo PYTHONPATH=/usr/local/lib/python make install-builder
</code></pre>
<p>If you run into issue with the last command (<em>make install-builder</em>), please try:</p>
<pre><code>sudo PYTHONPATH=`which python3` make install-builder
</code></pre>
<h2 id="4-other-python-packages"><a class="header" href="#4-other-python-packages">4. Other Python packages</a></h2>
<h3 id="confluent_kafka"><a class="header" href="#confluent_kafka">confluent_kafka</a></h3>
<p>First, install librdkafka and its development libraries by following the
instructions on <a href="software/librdkafka.html">this page</a>. As part of this process,
you will install <code>gcc</code>, <code>pip</code> for Python3, and Python development headers.</p>
<p>Once complete, you can then install the <code>confluent_kafka</code> module directly from
source:</p>
<pre><code>python3 -m pip install --user --no-binary confluent-kafka confluent-kafka
</code></pre>
<h3 id="open3d"><a class="header" href="#open3d">open3d</a></h3>
<p>Open3d required glibc version 2.27 or higher. Amazon Linux 2 includes glibc 2.26, which is not sufficient. In order to
use open3d, please use Amazon Linux 2023 or later, Ubuntu Bionic (18.04) or later, or another supported distribution.
See <a href="http://www.open3d.org/docs/release/getting_started.html">open3d documentation</a> for more information.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="rust-on-graviton"><a class="header" href="#rust-on-graviton">Rust on Graviton</a></h1>
<p>Rust is supported on Linux/arm64 systems as a tier1 platform along side x86.</p>
<h3 id="large-system-extensions-lse-1"><a class="header" href="#large-system-extensions-lse-1">Large-System Extensions (LSE)</a></h3>
<p>All available Graviton processors (excluding the first and not recommended Graviton1)
have support for the Armv8.2 instruction set.  Armv8.2 specification includes the
large-system extensions (LSE) introduced in Armv8.1. LSE provides low-cost atomic
operations:</p>
<p>LSE improves system throughput for CPU-to-CPU communication, locks, and mutexes.
The improvement can be up to an order of magnitude when using LSE instead of
load/store exclusives. LSE can be enabled in Rust and we've seen cases on
larger machines where performance is improved by over 3x by setting the <code>RUSTFLAGS</code>
environment variable and rebuilding your project.</p>
<pre><code>export RUSTFLAGS="-Ctarget-feature=+lse"
cargo build --release
</code></pre>
<p>If you're running only on Graviton2 or newer hardware you can also enable other
instructions by setting the cpu target such as the example below:</p>
<pre><code>export RUSTFLAGS="-Ctarget-cpu=neoverse-n1"
cargo build --release
</code></pre>
<p>When Rust is configured to use LLVM 12 or newer, target feature
<code>+outline-atomics</code> is available.  Outline-atomics produces a binary containing
two versions of the atomic operation following the hardware capabilities.  When
the code executes on a newer hardware such as Graviton2, the processor will
execute LSE instructions; when the code executes on older hardware without LSE
instructions, the processor will execute Armv8.0 atomics instructions.</p>
<p>Rust 1.57 (release on December 2, 2021) enables by default outline-atomics
target feature when compiling for arm64-linux with LLVM 12 or newer.  When using
older Rust releases, outline-atomics target feature can be enabled with</p>
<pre><code>export RUSTFLAGS="-Ctarget-feature=+outline-atomics"
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="r-on-graviton"><a class="header" href="#r-on-graviton">R on Graviton</a></h1>
<p><strong>Introduction</strong></p>
<p>R is a free software environment for statistical computing and graphics. It compiles and runs on a wide variety of platforms (including arm64).  <em><a href="%5B(https://www.r-project.org)%5D">Read more on the R Project page</a>(https://www.r-project.org)</em>.
This page is meant to discuss differences between running R on Graviton versus other platforms, not to give instructions for R in general.</p>
<h2 id="1-installing-r"><a class="header" href="#1-installing-r">1. Installing R</a></h2>
<p>Because of its use cases, performance is a consideration for R.  For that reason, using Amazon Linux is recommended because it is regularly updated to include Graviton related optimizations.</p>
<p>All instructions here are tested using the Amazon Linux distribution (specifically the 2023 version). Other than the package manager (yum/apt), they should work on other distributions as well.</p>
<p>As on most platforms, the easiest way to install R is using the built in package manager.</p>
<p><code>sudo yum install R</code></p>
<p>This will install R.  However, as is also the case on most platforms, the package manager doesn't always have the latest version.  If you need a more current version, you would need to install manually from the source.</p>
<h2 id="2-installing-r-packages"><a class="header" href="#2-installing-r-packages">2. Installing R packages</a></h2>
<p>CRAN (the default R package repository) hosts most packages as source code.  This means that installing a package using the built in package manager (install.packages) will automatically download the source and compile it for your platform. This works well on the Graviton platform because it creates the binaries you need and also lets you take advantage of processor optimizations that may be compiled in.</p>
<p>Packages may not install because of missing libraries.  In most cases, install.packages will show you the missing packages that provide those libraries.  If too many things scrolled by on the screen, run</p>
<p><code>&gt;warnings()</code></p>
<p>from the R command prompt to review.</p>
<p>There are some packages that need to be installed a little differently on Graviton because their installation includes binary distribution.</p>
<p><strong>For example:</strong></p>
<p><code>&gt;install.packages(c("devtools"), dependencies=TRUE) </code></p>
<p>will tell you that you need to first install libcurl and openssl.  For Amazon Linux, use the package names listed on the <code>\* rpm:</code> line.</p>
<p>In this case:</p>
<pre><code>sudo yum install openssl-devel
sudo yum install libcurl-devel
</code></pre>
<p>However, one of the required packages, gert, will tell you it needs <code>libgit2-devel</code>.  You don't see this in the installation on x86 because the gert install package includes a script that downloads a static linked binary if it doesn't find the needed library.</p>
<p><code>libgit2-devel</code> is not currently available through yum, so you need to install manually.</p>
<p>In order to do that, you may need two additional packages, <code>cmake</code> and <code>git</code>.  <strong>You also need to use the install prefix of /usr instead of /usr/local</strong></p>
<p>From the linux command line:</p>
<pre><code>sudo yum install cmake
sudo yum install git
git clone https://github.com/libgit2/libgit2
cd libgit2
mkdir build &amp;&amp; cd build
sudo cmake .. -DCMAKE_INSTALL_PREFIX=/usr
sudo cmake --build . --target install
cd ..
rm -rf libgit2
</code></pre>
<p>After that, you can return to R and run</p>
<p><code>&gt;install.packages(c("devtools"), dependencies=TRUE) </code></p>
<p>and it should complete.</p>
<h2 id="3-compiled-code-use"><a class="header" href="#3-compiled-code-use">3. Compiled code use</a></h2>
<p>Any R package or program that uses compiled code will probably need to have that code recompiled.  Refer to <em><a href="%5B(https://cran.r-project.org/web/packages/box/vignettes/compiled-code.html)%5D">Using compiled code</a>(https://cran.r-project.org/web/packages/box/vignettes/compiled-code.html)</em> on the R Project site to see examples of what compiled code use may look like.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="spark-on-graviton"><a class="header" href="#spark-on-graviton">Spark on Graviton</a></h1>
<p>Apache Spark is a data processing framework widely used to extract information from large pools of data.
One main problem that affects performance is the straggler, where a long-running task slows down the entire cluster. Stragglers in Spark are usually caused by non-uniform distribution of work or data being skewed non uniformly across nodes, resulting in a single task taking up more work. Our goal should be to keep all the CPUs busy and not have a small set of cores executing long running tasks. Setting up a correct configuration depends on the dataset size, number of instances, core count/instance, and computational complexity.</p>
<p>Below are some general guidelines that can be referenced by users trying to improve overall application performance across Spark clusters. Since there is no one standard config set that works well in all cases, we advise users to benchmark real applications after following the below guidelines.</p>
<ol>
<li>
<p>Shuffle Partitions: This configuration option is important to mitigate performance issues due to stragglers. Recommendations are</p>
<ul>
<li>To have a partition size to be less than 200 MB to gain optimized performance</li>
<li>The number of partitions should be multiples of the number of cores available (1xcores, 2xcores .. etc)</li>
</ul>
<p>Below are the benchmark results showing the total execution time by varying shuffle partitions value in Spark. Benchmarking is performed on Spark cluster with 128 vCPUs spread across 8 Graviton3 instances, executing queries on 1 TB TPC-DS dataset.
We have seen 80% improvement in performance when using optimized value vs non-optimized value.</p>
</li>
</ol>
<p>|shuffle_partitions	|Execution time (mins)	|%Diff	|
|---	|---	|---	|
|10	|175	|Baseline	|
|16	|117	|-33%	|
|30	|72	|-59%	|
|64	|50	|-71%	|
|128	|48	|-73%	|
|256	|39	|-78%	|
|512	|37	|-79%	|
|1024	|35	|-80%	|
|2000	|35	|-80%	|</p>
<p><em>Lower numbers are better, and negative % diff means faster. Benchmarked on Spark 3.3.0 with Java 17 using spark-sql-perf framework from Databricks</em></p>
<ol start="2">
<li>When using Amazon EMR to setup Spark cluster, it is recommended to use EMR defaults for configuration options. For any other specific cases that need specific tuning, the general optimization guide can be referenced from https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-spark-performance.html.</li>
<li>Adaptive Query Execution(AQE) is an optimization technique in Spark SQL that makes use of runtime statistics to choose the most efficient query execution plan, which is enabled by default since Apache Spark 3.2.0. For users, using older Spark versions, we recommend turning it on and seeing if it improves performance. (https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution)</li>
<li>We have seen 40% improvement in performance when using Spark 3 with Java 17 when compared to Spark 2 with Java 8. So we recommend using latest Spark 3 with Java 17.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="aws-lambda-on-graviton2"><a class="header" href="#aws-lambda-on-graviton2">AWS Lambda on Graviton2</a></h1>
<p><a href="https://aws.amazon.com/lambda/">AWS Lambda</a> now allows you to configure new and existing functions to run on Arm-based AWS Graviton2 processors in addition to x86-based functions. Using this processor architecture option allows you to get up to 34% better price performance. Duration charges, billed with <a href="https://aws.amazon.com/blogs/aws/new-for-aws-lambda-1ms-billing-granularity-adds-cost-savings/">millisecond granularity</a>, are 20 percent lower when compared to current x86 pricing. This also applies to duration charges when using <a href="https://aws.amazon.com/blogs/aws/new-provisioned-concurrency-for-lambda-functions/">Provisioned Concurrency</a>. Compute <a href="https://aws.amazon.com/blogs/aws/savings-plan-update-save-up-to-17-on-your-lambda-workloads/">Savings Plans</a> supports Lambda functions powered by Graviton2.</p>
<p>The blog post <a href="https://aws.amazon.com/blogs/compute/migrating-aws-lambda-functions-to-arm-based-aws-graviton2-processors/">Migrating AWS Lambda functions to Arm-based AWS Graviton2 processors</a> shows some considerations when moving from x86 to arm64 as the migration process is code and workload dependent.</p>
<p>This page highlights some of the migration considerations and also provides some simple to deploy demos you can use to explore how to build and migrate to Lambda functions using Arm/Graviton2.</p>
<p>The architecture change does not affect the way your functions are invoked or how they communicate their responses back. Integrations with APIs, services, applications, or tools are not affected by the new architecture and continue to work as before.
The following runtimes, which use <a href="https://aws.amazon.com/amazon-linux-2/">Amazon Linux 2</a>, are supported on Arm:</p>
<ul>
<li>Node.js 12 and above</li>
<li>Python 3.8 and above</li>
<li>Java 8 (java8.al2) and above</li>
<li>.NET Core 3.1 and above</li>
<li>Ruby 2.7 and above</li>
<li><a href="https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html">Custom runtime</a> (provided.al2) and above</li>
</ul>
<h2 id="migrating-x86-lambda-functions-to-arm64"><a class="header" href="#migrating-x86-lambda-functions-to-arm64">Migrating x86 Lambda functions to arm64</a></h2>
<h3 id="functions-without-architecture-specific-dependencies-or-binaries"><a class="header" href="#functions-without-architecture-specific-dependencies-or-binaries">Functions without architecture-specific dependencies or binaries</a></h3>
<p>Many Lambda functions may only need a configuration change to take advantage of the price/performance of Graviton2. Other functions may require repackaging the Lambda function using Arm-specific dependencies, or rebuilding the function binary or container image.</p>
<p>If your functions don’t use architecture-specific dependencies or binaries, you can switch from one architecture to the other with a single configuration change. Many functions using interpreted languages such as Node.js and Python, or functions compiled to Java bytecode, can switch without any changes. Ensure you check binaries in dependencies, Lambda layers, and Lambda extensions.</p>
<h3 id="building-function-code-for-graviton2"><a class="header" href="#building-function-code-for-graviton2">Building function code for Graviton2</a></h3>
<p>For compiled languages like Rust and Go, you can use the <code>provided.al2</code> or <code>provided.al2023</code>  <a href="https://docs.aws.amazon.com/lambda/latest/dg/runtimes-custom.html">custom runtimes</a>, which supports Arm. You provide a binary that communicates with the <a href="https://docs.aws.amazon.com/lambda/latest/dg/runtimes-api.html">Lambda Runtime API</a>.</p>
<p>When compiling for Go, set <code>GOARCH</code> to <code>arm64</code>.</p>
<pre><code>GOOS=linux GOARCH=arm64 go build
</code></pre>
<p>When compiling for Rust, set the <code>target</code>.</p>
<pre><code>cargo build --release -- target-cpu=neoverse-n1
</code></pre>
<p>The default installation of Python <code>pip</code> on some Linux distributions is out of date (&lt;19.3). To install binary wheel packages released for Graviton, upgrade the pip installation using:</p>
<pre><code>sudo python3 -m pip install --upgrade pip
</code></pre>
<p>Functions packaged as container images must be built for the architecture (x86 or arm64) they are going to use. There are arm64 architecture versions of the <a href="https://docs.aws.amazon.com/lambda/latest/dg/runtimes-images.html#runtimes-images-lp">AWS provided base images for Lambda</a>. To specify a container image for arm64, use the arm64 specific image tag, for example, for Node.js 20:</p>
<pre><code>public.ecr.aws/lambda/nodejs:20-arm64
public.ecr.aws/lambda/nodejs:latest-arm64
public.ecr.aws/lambda/nodejs:20.2024.01.05.14-arm64
</code></pre>
<p>Arm64 images are also available from <a href="https://hub.docker.com/u/amazon">Docker Hub</a>.
You can also use arbitrary Linux base images in addition to the AWS provided Amazon Linux 2 images. Images that support arm64 include <a href="https://alpinelinux.org/">Alpine</a> Linux 3.12.7 or later, <a href="https://www.debian.org/">Debian</a> 10 and 11, Ubuntu 18.04 and 20.04. For more information and details of other supported Linux versions, see <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/os.md#operating-systems-available-for-graviton-based-instances">Operating systems available for Graviton based instances</a>.</p>
<p>The <a href="https://github.com/alexcasalboni/aws-lambda-power-tuning">AWS Lambda Power Tuning</a> open-source project runs your functions using different settings to suggest a configuration to minimize costs and maximize performance. The tool allows you to compare two results on the same chart and incorporate arm64-based pricing. This is useful to compare two versions of the same function, one using x86 and the other arm64.</p>
<h2 id="demo-building-lambda-functions-for-graviton2"><a class="header" href="#demo-building-lambda-functions-for-graviton2">DEMO: Building Lambda functions for Graviton2</a></h2>
<p>Demo requirements:</p>
<ul>
<li><a href="https://aws.amazon.com/serverless/sam/">AWS Serverless Application Model (AWS SAM)</a></li>
<li><a href="https://docs.docker.com/get-docker/">Docker</a></li>
</ul>
<p>Clone the repository and change into the demo directory</p>
<pre><code>git clone https://github.com/aws/aws-graviton-getting-started
cd aws-graviton-getting-started/aws-lambda/GravitonLambdaNumber
</code></pre>
<h3 id="migrating-a-lambda-function-from-x86-to-arm64"><a class="header" href="#migrating-a-lambda-function-from-x86-to-arm64">Migrating a Lambda function from x86 to arm64</a></h3>
<p>This demo shows how to migrate an existing Lambda function from x86 to arm64 using an x86 based workstation.
The Node.js function code in <a href="aws-lambda//src/app.js"><code>/src/app.js</code></a> uses the <a href="https://www.npmjs.com/package/axios">axios</a> client to connect to a third part service, <a href="http://numbersapi.com/">http://numbersapi.com/</a>, to find interesting facts about numbers. As the axios library is not a binary file, it can seamlessly work on Graviton2 without compilation.</p>
<p>The existing application consists of an API endpoint which invokes the Lambda function, retrieves the number fact, and returns the response.</p>
<p>Build the existing x86 function version using AWS SAM.</p>
<pre><code>sam build
</code></pre>
<p><img src="aws-lambda//aws-lambda/img/sambuild.png" alt="sam build" /></p>
<p>Deploy the function to your AWS account:</p>
<pre><code>sam deploy --stack-name GravitonLambdaNumber -g
</code></pre>
<p>Accept the initial defaults, and ensure you enter Y for <code>LambdaNumberFunction may not have authorization defined, Is this okay? [y/N]: y</code></p>
<p><img src="aws-lambda//aws-lambda/img/samdeploy-g.png" alt="sam deploy -g" /></p>
<p>AWS SAM deploys the infrastructure and outputs an APIBasePath</p>
<p><img src="aws-lambda//aws-lambda/img/ApiBasePath.png" alt="ApiBasePath" /></p>
<p>Use <code>curl</code> and make a POST request to the APIBasePath URL with a number as a date.</p>
<pre><code>curl -X POST https://6ioqy4z9ee.execute-api.us-east-1.amazonaws.com -H 'Content-Type: application/json' -d '{ "number": "345", "type": "date" }'
</code></pre>
<p>The Lambda function should respond with the <code>x64</code> processor architecture and a fact about the date.</p>
<p><img src="aws-lambda//aws-lambda/img/curlx86.png" alt="curl x86" /></p>
<p>Amend the processor architecture within the AWS SAM <a href="aws-lambda/./GravitonLambdaNumber/template.yml">template.yml</a> file.
replace</p>
<pre><code>Architectures:
- x86_64
</code></pre>
<p>with</p>
<pre><code>Architectures:
- arm64
</code></pre>
<p>Build the function using an arm64 architecture and deploy the change to the cloud.</p>
<pre><code>sam build
sam deploy
</code></pre>
<p>Use the same <code>curl</code> command, making a POST request to the APIBasePath URL with a number as a date.</p>
<pre><code>curl -X POST https://6ioqy4z9ee.execute-api.us-east-1.amazonaws.com -H 'Content-Type: application/json' -d '{ "number": "345", "type": "date" }'
</code></pre>
<p>The Lambda function should respond with the <code>arm64</code> processor architecture and a fact about the date.</p>
<p><img src="aws-lambda//aws-lambda/img/curlarm64.png" alt="curl arm64" /></p>
<p>The function has seamlessly migrated from x86 to arm64.</p>
<h3 id="building-a-lambda-function-with-binary-dependencies"><a class="header" href="#building-a-lambda-function-with-binary-dependencies">Building a Lambda function with binary dependencies</a></h3>
<p>This function has no binary dependencies. If you do have a function that required compilation for arm64, AWS SAM can use an arm64 build container image to create the artifacts for arm64.
This functionality works both ways. You can build arm64 specific dependencies on an x86 workstation and also build x86 specific dependencies on an arm64 workstation.</p>
<p>Specify <code>--use-container</code> to use the build container.</p>
<pre><code>sam build --use-container
</code></pre>
<p><img src="aws-lambda//aws-lambda/img/sambuildcontainer.png" alt="sam build --use-container" /></p>
<h3 id="local-testing"><a class="header" href="#local-testing">Local testing</a></h3>
<p>You can test the arm64 function locally using either AWS SAM or Docker natively.</p>
<p>When using AWS SAM, you can use <a href="aws-lambda/%5Btemplate.yml%5D(https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/sam-cli-command-reference-sam-local-invoke.html)"><code>sam local invoke</code></a> to test your function locally, passing in a sample <code>event.json</code></p>
<pre><code>sam local invoke LambdaNumberFunction -e ./test/event.json
</code></pre>
<p><img src="aws-lambda//aws-lambda/img/samlocalinvoke.png" alt="sam local invoke" /></p>
<h3 id="building-arm64-lambda-functions-as-container-images"><a class="header" href="#building-arm64-lambda-functions-as-container-images">Building arm64 Lambda functions as container images</a></h3>
<p>You can build arm64 functions as container images. You  can use <a href="https://aws.amazon.com/blogs/compute/using-container-image-support-for-aws-lambda-with-aws-sam/">AWS SAM natively</a> to build container images.</p>
<p>You can also use Docker native commands instead of AWS SAM.
To build a container image of this function using Docker, use the <a href="aws-lambda/./GravitonLambdaNumber/src/Dockerfile">Dockerfile</a> and specify the <code>nodejs:20-arm64</code> base image.</p>
<pre><code>FROM public.ecr.aws/lambda/nodejs:20-arm64
COPY app.js package*.json $LAMBDA_TASK_ROOT
RUN npm install
CMD [ "app.lambdaHandler" ]
</code></pre>
<p>Build the container image.</p>
<pre><code>docker build -t dockernumber-arm ./src 
</code></pre>
<p><img src="aws-lambda//aws-lambda/img/dockerbuild.png" alt="docker build" /></p>
<p>Inspect the container image to confirm the <code>Architecture</code>.</p>
<pre><code>docker inspect dockernumber-arm | grep Architecture 
</code></pre>
<p><img src="aws-lambda//aws-lambda/img/dockerinspect.png" alt="docker inspect" /></p>
<p>You can locally test the function using <code>docker run</code>
In another terminal window run the Lambda function docker container image.</p>
<pre><code>docker run -p 9000:8080 dockernumber-arm:latest
</code></pre>
<p><img src="aws-lambda//aws-lambda/img/dockerrun.png" alt="docker run" /></p>
<p>Use <code>curl</code> to invoke the Lambda function using the local docker endpoint, passing in a sample <code>event.json</code>.</p>
<pre><code>curl -XPOST "http://localhost:9000/2015-03-31/functions/function/invocations" -d @./test/event.json
</code></pre>
<p><img src="aws-lambda//aws-lambda/img/dockerrunresponse.png" alt="docker run response" /></p>
<p>You can view the local logs in the <code>docker run</code> terminal window.</p>
<p>To create a Lambda function from the local image, first create an [Amazon Elastic Container Registry (ECR)]](https://aws.amazon.com/ecr/) repository and login to ECR.
Substitute the AWS account number <code>123456789012</code> and AWS Region values with your details</p>
<pre><code>aws ecr create-repository --repository-name dockernumber-arm --image-scanning-configuration scanOnPush=true
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-east-1.amazonaws.com
</code></pre>
<p>Tag and push the docker image to ECR.</p>
<pre><code>docker tag dockernumber-arm:latest 123456789012.dkr.ecr.us-east-1.amazonaws.com/dockernumber-arm:latest
docker push 123456789012.dkr.ecr.us-east-1.amazonaws.com/dockernumber-arm:latest
</code></pre>
<p>You can then create a Lambda function from the container image using the AWS Management Console, AWS CLI, or other tools.
<img src="aws-lambda//aws-lambda/img/createfunctionfromimage.png" alt="Create function from image" /></p>
<h2 id="comparing-x86-and-arm64-performance-and-cost"><a class="header" href="#comparing-x86-and-arm64-performance-and-cost">Comparing x86 and arm64 performance and cost.</a></h2>
<p>You can use the <a href="https://github.com/alexcasalboni/aws-lambda-power-tuning">AWS Lambda Power Tuning</a> open-source project to suggest a configuration to minimize costs and maximize performance. The tool allows you to compare two results on the same chart and incorporate arm64-based pricing. This is useful to compare two versions of the same function, one using x86 and the other arm64.</p>
<p>A demo application computes prime numbers. The AWS SAM <a href="aws-lambda/template.yml">template.yml</a> file contains two Lambda functions, one for x86 and one for arm64. Both functions use the same Python source code in <a href="aws-lambda/./src/app.py"><code>./src/app.py</code></a> to compute the prime numbers.</p>
<p>Ensure the repository is cloned from the previous demo and change into the directory.</p>
<pre><code>cd ../PythonPrime
</code></pre>
<p>Build the application within the AWS SAM build container images and deploy to the cloud.
Accept the default <code>sam deploy</code> prompts.</p>
<pre><code>sam build --use-container
sam deploy --stack-name PythonPrime -g
</code></pre>
<p>Note the Output values of the two Lambda functions:
<img src="aws-lambda//aws-lambda/img/primefunctions.png" alt="Prime Functions" /></p>
<h3 id="deploy-the-aws-lambda-power-tuning-state-machine"><a class="header" href="#deploy-the-aws-lambda-power-tuning-state-machine">Deploy the AWS Lambda Power Tuning State Machine</a></h3>
<p>Navigate to <a href="https://serverlessrepo.aws.amazon.com/applications/arn:aws:serverlessrepo:us-east-1:451282441545:applications~aws-lambda-power-tuning">https://serverlessrepo.aws.amazon.com/applications/arn:aws:serverlessrepo:us-east-1:451282441545:applications~aws-lambda-power-tuning</a> and choose <strong>Deploy</strong>.
Select <em>I acknowledge that this app creates custom IAM roles</em> and choose <strong>Deploy</strong>.</p>
<p>Once deployed, under <em>Resources</em>, choose the <em>powerTuningStateMachine</em>.</p>
<p>Choose <strong>Start execution</strong>
For the state machine input, specify the ARN of the x86 Lambda function from the AWS SAM Outputs, and set the memory <code>powerValues</code> to check. Run 10 invocations for each memory configuration, in parallel, specifying the Lambda function payload.
The following is an example input.</p>
<pre><code>{
	"lambdaARN": "arn:aws:lambda:us-east-1:123456789012:function:gravitonpythonprime-PythonPrimeX86Function-3Gge9WXmHObZ",
	"powerValues": [
		128,
		256,
		512,
		1024,
		1536,
		2048,
		3008,
		10240
	],
	"num": 10,
	"parallelInvocation": true,
	"payload": {
		"queryStringParameters": {
			"max": "1000000"
		}
	}
}
</code></pre>
<p>Select <em>Open in a new browser tab</em> and choose <strong>Start execution</strong>.
The Lambda Power Tuning state machine runs for each configured memory value.
<img src="aws-lambda//aws-lambda/img/powertuningstatemachine.png" alt="PowerTuningStateMachine" /></p>
<p>Once complete, the <em>Execution event history</em> final step, <em>ExecutionSucceeded</em> contains a visualization URL.</p>
<pre><code>{
  "output": {
    "power": 512,
    "cost": 0.000050971200000000004,
    "duration": 6067.418333333332,
    "stateMachine": {
      "executionCost": 0.00035,
      "lambdaCost": 0.007068295500000001,
      "visualization": "https://lambda-power-tuning.show/#gAAAAQACAAQABgAIwAsAKA==;5njERiCGREZZm71F9rxARW12AkU66d5EDqzeRLFs30Q=;a4NdODSTXTjpyVU42k9ZOLixXDipans4V224ONx8nTk="
    }
  },
  "outputDetails": {
    "truncated": false
  }
}
</code></pre>
<p>Browse to the lambda-power-tuning URL to view the average <em>Invocation time (ms)</em> and <em>Invocation Cost (USD)</em> for each memory value for the x86 function.</p>
<p><img src="aws-lambda//aws-lambda/img/powertuningx86results.png" alt="PowerTuning x86 results" /></p>
<p>Navigate back to the Step Functions console and run another state machine, specifying the ARN of the arm64 Lambda function from the AWS SAM Outputs.
Once complete, copy the visualization URL from the <em>Execution event history</em> final step, <em>ExecutionSucceeded</em>.</p>
<p>In the <em>lambda-power-tuning</em> browser tab showing the x86 results, choose <strong>Compare</strong>.
Enter <strong>x86</strong> as name for function 1
Enter <strong>arm64</strong> as the name for function 2
Paste in the URL from the arm64 function and choose <strong>Compare</strong>.</p>
<p><img src="aws-lambda//aws-lambda/img/powertuningcompare.png" alt="PowerTuning Compare" /></p>
<p>View the comparison between the <em>x86</em> and the <em>arm64</em> function.</p>
<p><img src="aws-lambda//aws-lambda/img/powertuningcomparison.png" alt="PowerTuning Comparison" /></p>
<p>At 2048 MB, the arm64 function is 29% faster and 43% cheaper than the identical Lambda function running on x86!
Power Tuning gives you a data driven approach to select the optimal memory configuration for your Lambda functions. This allows you to also compare x86 and arm64 and may allow you to reduce the memory configuration of your arm64 Lambda functions, further reducing costs.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="operating-systems-available-for-graviton-based-instances"><a class="header" href="#operating-systems-available-for-graviton-based-instances">Operating Systems available for Graviton based instances</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Version</th><th><a href="optimizing.html#locksynchronization-intensive-workload">LSE Support</a></th><th>Kernel page size</th><th>AMI</th><th>Metal support</th><th>Comment</th></tr></thead><tbody>
<tr><td>Amazon Linux 2023</td><td>All versions</td><td>Yes</td><td>4KB</td><td><a href="amis_cf_sm.html">AMIs</a></td><td>Yes</td><td>Pointer Authentication enabled on Graviton3</td></tr>
<tr><td>Amazon Linux 2</td><td>2.26-35 or later</td><td>Yes</td><td>4KB</td><td><a href="amis_cf_sm.html">AMIs</a></td><td>Yes</td><td>End of Life (EOL) scheduled 2025-06-30</td></tr>
<tr><td>Ubuntu Pro</td><td>22.04 LTS</td><td>Yes</td><td>4KB</td><td><a href="https://aws.amazon.com/marketplace/pp/prodview-uy7jg4dds3qjw">MarketPlace</a></td><td>Yes</td><td></td></tr>
<tr><td>Ubuntu</td><td>24.04 LTS</td><td>Yes</td><td>4KB</td><td><a href="https://cloud-images.ubuntu.com/locator/ec2/">noble</a></td><td>Yes</td><td></td></tr>
<tr><td>Ubuntu</td><td>22.04 LTS</td><td>Yes</td><td>4KB</td><td><a href="https://cloud-images.ubuntu.com/locator/ec2/">jammy</a></td><td>Yes</td><td></td></tr>
<tr><td>Ubuntu</td><td>20.04 LTS</td><td>Yes</td><td>4KB</td><td><a href="https://cloud-images.ubuntu.com/locator/ec2/">focal</a></td><td>Yes</td><td></td></tr>
<tr><td>Ubuntu</td><td>18.04 LTS</td><td>Yes (*)</td><td>4KB</td><td><a href="https://cloud-images.ubuntu.com/locator/ec2/">bionic</a></td><td>Yes</td><td>(*) needs <code>apt install libc6-lse</code>. Free support ended 2023/05/31.</td></tr>
<tr><td>SuSE</td><td>15 SP2 or later</td><td>Planned</td><td>4KB</td><td><a href="https://aws.amazon.com/marketplace/pp/B07SPTXBDX">MarketPlace</a></td><td>Yes</td><td></td></tr>
<tr><td>Redhat Enterprise Linux</td><td>8.2 or later</td><td>Yes</td><td>64KB</td><td><a href="https://aws.amazon.com/marketplace/pp/B07T2NH46P">MarketPlace</a></td><td>Yes</td><td></td></tr>
<tr><td><del>Redhat Enterprise Linux</del></td><td><del>7.x</del></td><td><del>No</del></td><td><del>64KB</del></td><td><del><a href="https://aws.amazon.com/marketplace/pp/B07KTFV2S8">MarketPlace</a></del></td><td></td><td>Supported on A1 instances but not on Graviton2 and later based ones</td></tr>
<tr><td>AlmaLinux</td><td>8.4 or later</td><td>Yes</td><td>64KB</td><td><a href="https://wiki.almalinux.org/cloud/AWS.html">AMIs</a></td><td>Yes</td><td></td></tr>
<tr><td>Alpine Linux</td><td>3.12.7 or later</td><td>Yes (*)</td><td>4KB</td><td><a href="https://www.alpinelinux.org/cloud/">AMIs</a></td><td></td><td>(*) LSE enablement checked in version 3.14</td></tr>
<tr><td>CentOS</td><td>8.2.2004 or later</td><td>No</td><td>64KB</td><td><a href="https://wiki.centos.org/Cloud/AWS#Images">AMIs</a></td><td>Yes</td><td></td></tr>
<tr><td>CentOS Stream</td><td>8</td><td>No (*)</td><td>64KB (*)</td><td><a href="https://www.centos.org/centos-stream/">Downloads</a></td><td></td><td>(*) details to be confirmed once AMI's are available</td></tr>
<tr><td><del>CentOS</del></td><td><del>7.x</del></td><td><del>No</del></td><td><del>64KB</del></td><td><del><a href="https://wiki.centos.org/Cloud/AWS#Images">AMIs</a></del></td><td></td><td>Supported on A1 instances but not on Graviton2 and later based ones</td></tr>
<tr><td>Debian</td><td>12</td><td>Yes</td><td>4KB</td><td><a href="https://wiki.debian.org/Cloud/AmazonEC2Image/Bookworm">Community</a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-63gms6fbfaota">MarketPlace</a></td><td>Yes</td><td></td></tr>
<tr><td>Debian</td><td>11</td><td>Yes</td><td>4KB</td><td><a href="https://wiki.debian.org/Cloud/AmazonEC2Image/Bullseye">Community</a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-jwzxq55gno4p4">MarketPlace</a></td><td>Yes</td><td></td></tr>
<tr><td>Debian</td><td>10</td><td><a href="https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=956418">Planned for Debian 11</a></td><td>4KB</td><td><a href="https://wiki.debian.org/Cloud/AmazonEC2Image/Buster">Community</a> or <a href="https://aws.amazon.com/marketplace/pp/B085HGTX5J">MarketPlace</a></td><td>Yes, as of Debian 10.7 (2020-12-07)</td><td></td></tr>
<tr><td>FreeBSD</td><td>13.5</td><td>Yes</td><td>4KB</td><td><a href="https://www.freebsd.org/releases/13.5R/announce">Community</a> or <a href="https://aws.amazon.com/marketplace/pp/B09291VW11">MarketPlace</a></td><td>No</td><td>Device hotplug and API shutdown don't work</td></tr>
<tr><td>FreeBSD</td><td>14.2</td><td>Yes</td><td>4KB</td><td><a href="https://www.freebsd.org/releases/14.2R/announce/">Community</a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-axdyrrhr6pboq">MarketPlace</a> (UFS) or <a href="https://aws.amazon.com/marketplace/pp/prodview-7xtubzy4v4oo4">MarketPlace</a> (ZFS)</td><td>Yes</td><td>Device hotplug doesn't work</td></tr>
<tr><td>FreeBSD</td><td>14.3</td><td>Yes</td><td>4KB</td><td><a href="https://www.freebsd.org/releases/14.3R/announce/">Community</a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-axdyrrhr6pboq">MarketPlace</a> (UFS) or <a href="https://aws.amazon.com/marketplace/pp/prodview-7xtubzy4v4oo4">MarketPlace</a> (ZFS)</td><td>Yes</td><td>Maintained by <a href="mailto:cperciva@FreeBSD.org">Colin Percival</a></td></tr>
<tr><td>Flatcar Linux</td><td>3033.2.0 or later</td><td>Yes</td><td>4KB</td><td><a href="https://www.flatcar.org/docs/latest/installing/cloud/aws-ec2/">AMIs</a> or <a href="https://aws.amazon.com/marketplace/pp/prodview-zmao5idgwafbi">marketplace</a></td><td>Yes</td><td></td></tr>
<tr><td>Rocky Linux</td><td>8.4 or later</td><td>Yes (*)</td><td>64KB (*)</td><td><a href="https://rockylinux.org/download">ISOs</a></td><td></td><td><a href="https://docs.rockylinux.org/release_notes/8-changelog/">Release Notes</a><br>(*) details to be confirmed once AMI's are available</td></tr>
</tbody></table>
</div><div class="table-wrapper"><table><thead><tr><th>OS Name</th><th>Minimum recommended Linux kernel version for Graviton</th></tr></thead><tbody>
<tr><td>Amazon Linux 2023</td><td>All supported kernels</td></tr>
<tr><td>Amazon Linux 2</td><td>&gt;= 4.14.273-207.502.amzn2, &gt;= 5.4.186-102.354.amzn2, or &gt;= 5.10.106-102.504.amzn2</td></tr>
<tr><td>Ubuntu 24.04</td><td>All supported kernels</td></tr>
<tr><td>Ubuntu 22.04</td><td>All supported kernels</td></tr>
<tr><td>Ubuntu 20.04</td><td>&gt;= 5.4.0-1047-aws, &gt;= 5.8.0-1034-aws, &gt;= 5.11.0-1009-aws</td></tr>
<tr><td>Ubuntu 18.04</td><td>&gt;= 4.15.0-1101-aws, &gt;= 5.4.0-1047-aws</td></tr>
<tr><td>Redhat Entreprise Linux 8</td><td>&gt;= 4.18.0-305.10</td></tr>
</tbody></table>
</div>
<h1 id="operating-systems-which-do-not-support-graviton-based-instances"><a class="header" href="#operating-systems-which-do-not-support-graviton-based-instances">Operating systems which do not support Graviton based instances</a></h1>
<div class="table-wrapper"><table><thead><tr><th>Name</th><th>Version</th></tr></thead><tbody>
<tr><td>Microsoft Windows</td><td>All versions</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="container-based-workloads-on-graviton"><a class="header" href="#container-based-workloads-on-graviton">Container-based workloads on Graviton</a></h1>
<p>AWS Graviton processors are ideal for container-based workloads.</p>
<h3 id="preparing-for-graviton"><a class="header" href="#preparing-for-graviton">Preparing for Graviton</a></h3>
<p>The first step for leveraging the benefits of Graviton-based instances as container hosts is to ensure all production software dependencies support the arm64 architecture, as one cannot run images built for an x86_64 host on an arm64 host, and vice versa.</p>
<p>Most of the container ecosystem supports both architectures, and often does so transparently through <a href="https://www.docker.com/blog/multi-platform-docker-builds/">multiple-architecture (multi-arch)</a> images, where the correct image for the host architecture is deployed automatically.</p>
<p>The major container image repositories, including <a href="https://hub.docker.com">Dockerhub</a>, <a href="https://www.quay.io">Quay</a>, and <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/what-is-ecr.html">Amazon Elastic Container Registry (ECR)</a> all support <a href="https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/">multi-arch</a> images.</p>
<h4 id="creating-multi-arch-container-images"><a class="header" href="#creating-multi-arch-container-images">Creating Multi-arch container images</a></h4>
<p>While most images already support multi-arch (i.e. arm64 and x86_64/amd64), we describe couple of ways for developers to to create a multi-arch image if needed.</p>
<ol>
<li><a href="https://github.com/docker/buildx#getting-started">Docker Buildx</a></li>
<li>Using a CI/CD Build Pipeline such as <a href="https://github.com/aws-samples/aws-multiarch-container-build-pipeline">Amazon CodePipeline</a> to coordinate native build and manifest generation.</li>
</ol>
<h3 id="deploying-to-graviton"><a class="header" href="#deploying-to-graviton">Deploying to Graviton</a></h3>
<p>Most container orchestration platforms support both arm64 and x86_64 hosts.</p>
<p>Both <a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html#amazon-linux-2-(arm64)">Amazon Elastic Container Service (ECS)</a> and <a href="https://aws.amazon.com/blogs/containers/eks-on-graviton-generally-available/">Amazon Elastic Kubernetes Service (EKS)</a> support Graviton-powered instances.</p>
<p>We have compiled a list of popular software within the container ecosystem that explicitly supports arm64:</p>
<h4 id="ecosystem-support"><a class="header" href="#ecosystem-support">Ecosystem Support</a></h4>
<div class="table-wrapper"><table><thead><tr><th style="text-align: left">Name</th><th style="text-align: left">URL</th><th style="text-align: left">Comment</th></tr></thead><tbody>
<tr><td style="text-align: left">Istio</td><td style="text-align: left">https://github.com/istio/istio/releases/</td><td style="text-align: left">Container images for arm64 available at Docker Hub starting with 1.15.0 release</td></tr>
<tr><td style="text-align: left">Envoy</td><td style="text-align: left">https://www.envoyproxy.io/docs/envoy/v1.18.3/start/docker</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">TensorFlow with TensorFlow Serving</td><td style="text-align: left">763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-inference-graviton:2.13.0-cpu-py310-ubuntu20.04-ec2</td><td style="text-align: left">refer to <a href="machinelearning/tensorflow.html">Graviton TensorFlow user guide</a> for how to use.</td></tr>
<tr><td style="text-align: left">PyTorch with TorchServe</td><td style="text-align: left">763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-graviton:2.1.0-cpu-py310-ubuntu20.04-ec2</td><td style="text-align: left">refer to <a href="machinelearning/pytorch.html">Graviton PyTorch user guide</a> for how to use</td></tr>
<tr><td style="text-align: left">Traefik</td><td style="text-align: left">https://github.com/containous/traefik/releases</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Flannel</td><td style="text-align: left">https://github.com/coreos/flannel/releases</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Helm</td><td style="text-align: left">https://github.com/helm/helm/releases/tag/v2.16.9</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Jaeger</td><td style="text-align: left">https://github.com/jaegertracing/jaeger/pull/2176</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Fluent-bit</td><td style="text-align: left">https://github.com/fluent/fluent-bit/releases/</td><td style="text-align: left">compile from source</td></tr>
<tr><td style="text-align: left">core-dns</td><td style="text-align: left">https://github.com/coredns/coredns/releases/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">external-dns</td><td style="text-align: left">https://github.com/kubernetes-sigs/external-dns/blob/master/docs/faq.md#which-architectures-are-supported</td><td style="text-align: left">support from 0.7.5+</td></tr>
<tr><td style="text-align: left">Prometheus</td><td style="text-align: left">https://prometheus.io/download/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">containerd</td><td style="text-align: left">https://github.com/containerd/containerd/issues/3664</td><td style="text-align: left">nightly builds provided for arm64</td></tr>
<tr><td style="text-align: left">kube-state-metrics</td><td style="text-align: left">https://github.com/kubernetes/kube-state-metrics/issues/1037</td><td style="text-align: left">use k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.0.0-beta for arm64</td></tr>
<tr><td style="text-align: left">cluster-autoscaler</td><td style="text-align: left">https://github.com/kubernetes/autoscaler/pull/3714</td><td style="text-align: left">arm64 support as of v1.20.0</td></tr>
<tr><td style="text-align: left">gRPC</td><td style="text-align: left">https://github.com/protocolbuffers/protobuf/releases/</td><td style="text-align: left">protoc/protobuf support</td></tr>
<tr><td style="text-align: left">Nats</td><td style="text-align: left">https://github.com/nats-io/nats-server/releases/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">CNI</td><td style="text-align: left">https://github.com/containernetworking/plugins/releases/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Cri-o</td><td style="text-align: left">https://github.com/cri-o/cri-o/blob/master/README.md#installing-crio</td><td style="text-align: left">tested on Ubuntu 18.04 and 20.04</td></tr>
<tr><td style="text-align: left">Trivy</td><td style="text-align: left">https://github.com/aquasecurity/trivy/releases/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Argo</td><td style="text-align: left">https://github.com/argoproj/argo-cd/releases</td><td style="text-align: left">arm64 images published as of 2.3.0</td></tr>
<tr><td style="text-align: left">Cilium</td><td style="text-align: left">https://docs.cilium.io/en/stable/contributing/development/images/</td><td style="text-align: left">Multi arch supported from v 1.10.0</td></tr>
<tr><td style="text-align: left">Calico</td><td style="text-align: left">https://hub.docker.com/r/calico/node/tags?page=1&amp;ordering=last_updated</td><td style="text-align: left">Multi arch supported on master</td></tr>
<tr><td style="text-align: left">Tanka</td><td style="text-align: left">https://github.com/grafana/tanka/releases</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Consul</td><td style="text-align: left">https://www.consul.io/downloads</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Nomad</td><td style="text-align: left">https://www.nomadproject.io/downloads</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Packer</td><td style="text-align: left">https://www.packer.io/downloads</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Vault</td><td style="text-align: left">https://www.vaultproject.io/downloads</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Terraform</td><td style="text-align: left">https://github.com/hashicorp/terraform/issues/14474</td><td style="text-align: left">arm64 support as of v0.14.0</td></tr>
<tr><td style="text-align: left">Flux</td><td style="text-align: left">https://github.com/fluxcd/flux/releases/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Pulumi</td><td style="text-align: left">https://github.com/pulumi/pulumi/issues/4868</td><td style="text-align: left">arm64 support as of v2.23.0</td></tr>
<tr><td style="text-align: left">New Relic</td><td style="text-align: left">https://download.newrelic.com/infrastructure_agent/binaries/linux/arm64/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Datadog - EC2</td><td style="text-align: left">https://www.datadoghq.com/blog/datadog-arm-agent/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Datadog - Docker</td><td style="text-align: left">https://hub.docker.com/r/datadog/agent-arm64</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Dynatrace</td><td style="text-align: left">https://www.dynatrace.com/news/blog/get-out-of-the-box-visibility-into-your-arm-platform-early-adopter/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Grafana</td><td style="text-align: left">https://grafana.com/grafana/download?platform=arm</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Loki</td><td style="text-align: left">https://github.com/grafana/loki/releases</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">kube-bench</td><td style="text-align: left">https://github.com/aquasecurity/kube-bench/releases/tag/v0.3.1</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">metrics-server</td><td style="text-align: left">https://github.com/kubernetes-sigs/metrics-server/releases/tag/v0.3.7</td><td style="text-align: left">docker image is multi-arch from v.0.3.7</td></tr>
<tr><td style="text-align: left">AWS Copilot</td><td style="text-align: left">https://github.com/aws/copilot-cli/releases/tag/v0.3.0</td><td style="text-align: left">arm64 support as of v0.3.0</td></tr>
<tr><td style="text-align: left">AWS ecs-cli</td><td style="text-align: left">https://github.com/aws/amazon-ecs-cli/pull/1110</td><td style="text-align: left">v1.20.0 binaries in us-west-2 s3</td></tr>
<tr><td style="text-align: left">Amazon EC2 Instance Selector</td><td style="text-align: left">https://github.com/aws/amazon-ec2-instance-selector/releases/</td><td style="text-align: left">also supports the -a cpu_architecture flag for discovering arm64-based instances in a particular region</td></tr>
<tr><td style="text-align: left">AWS Node Termination Handler</td><td style="text-align: left">https://github.com/aws/aws-node-termination-handler/releases/</td><td style="text-align: left">arm64 support under kubernetes (via helm)</td></tr>
<tr><td style="text-align: left">AWS IAM Authenticator</td><td style="text-align: left">https://docs.aws.amazon.com/eks/latest/userguide/install-aws-iam-authenticator.html</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">AWS ALB Ingress Controller</td><td style="text-align: left">https://github.com/kubernetes-sigs/aws-alb-ingress-controller/releases/tag/v1.1.9</td><td style="text-align: left">multi-arch image as of v1.1.9</td></tr>
<tr><td style="text-align: left">AWS EFS CSI Driver</td><td style="text-align: left">https://github.com/kubernetes-sigs/aws-efs-csi-driver/pull/241</td><td style="text-align: left">support merged 8/27/2020</td></tr>
<tr><td style="text-align: left">AWS EBS CSI Driver</td><td style="text-align: left">https://github.com/kubernetes-sigs/aws-ebs-csi-driver/pull/527</td><td style="text-align: left">support merged 8/26/2020</td></tr>
<tr><td style="text-align: left">Amazon Inspector Agent</td><td style="text-align: left">https://docs.aws.amazon.com/inspector/latest/userguide/inspector_installing-uninstalling-agents.html#install-linux</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Amazon CloudWatch Agent</td><td style="text-align: left">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Install-CloudWatch-Agent.html</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">AWS Systems Manager SSM Agent</td><td style="text-align: left">https://docs.aws.amazon.com/systems-manager/latest/userguide/sysman-manual-agent-install.html</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">AWS CLI</td><td style="text-align: left">https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html#ARM</td><td style="text-align: left">v1 and v2 both supported</td></tr>
<tr><td style="text-align: left">FireLens for Amazon ECS</td><td style="text-align: left">https://github.com/aws/aws-for-fluent-bit/issues/44</td><td style="text-align: left">arm64 support as of v2.9.0</td></tr>
<tr><td style="text-align: left">Flatcar Container Linux</td><td style="text-align: left">https://www.flatcar.org</td><td style="text-align: left">arm64 support in Stable channel as of 3033.2.0</td></tr>
<tr><td style="text-align: left">Seqera Containers</td><td style="text-align: left">https://www.seqera.io/containers/</td><td style="text-align: left"></td></tr>
<tr><td style="text-align: left">Kublr</td><td style="text-align: left">https://kublr.com</td><td style="text-align: left"></td></tr>
</tbody></table>
</div>
<p><strong>If your software isn't listed above, it doesn't mean it won't work!</strong></p>
<p>Many products work on arm64 but don't explicitly distribute arm64 binaries or build multi-arch images <em>(yet)</em>. AWS, Arm, and many developers in the community are working with maintainers and contributing expertise and code to enable full binary or multi-arch support.</p>
<h3 id="kubernetes"><a class="header" href="#kubernetes">Kubernetes</a></h3>
<p>Kubernetes (and EKS) supports arm64, and thus Graviton instances. If all of your containerized workloads support arm64, then you can run your cluster with Graviton nodes exclusively. However, if you have some workloads that can only run on amd64 (x86) instances, or if you just want to be able to run both amd64 (x86) and arm64 nodes in the same cluster, then there are a couple of ways to accomplish that:</p>
<h4 id="multiarch-images"><a class="header" href="#multiarch-images">Multiarch Images</a></h4>
<p>If you are able to use multiarch images (see above) for all containers in your cluster, then you can simply run a mix of amd64 and arm64 nodes without any further action. The multiarch image manifest will ensure that the correct image layers are pulled for a given node's architecture.</p>
<h4 id="built-in-labels"><a class="header" href="#built-in-labels">Built-in labels</a></h4>
<p>You can schedule pods on nodes according to the <code>kubernetes.io/arch</code> <a href="https://kubernetes.io/docs/reference/labels-annotations-taints/#kubernetes-io-arch">label</a>. This label is automatically added to nodes by Kubernetes and allows you to schedule pods accordingly with a <a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector">node selector</a> like this:</p>
<pre><code>nodeSelector:
  kubernetes.io/arch: amd64
</code></pre>
<h4 id="using-taints"><a class="header" href="#using-taints">Using taints</a></h4>
<p>Taints are especially helpful if adding Graviton nodes to an existing cluster with mostly amd64-only (x86-only) containers. While using the built-in <code>kubernetes.io/arch</code> label requires you to explicitly use a node selector to place amd64-only containers on the right instances, tainting Graviton instances prevents Kubernetes from scheduling incompatible containers on them without requiring you to change any existing configuration. For example, you can do this with a managed node group using eksctl by adding <code>--kubelet-extra-args '--register-with-taints=arm=true:NoSchedule'</code> to the kubelet startup arguments as documented <a href="https://eksctl.io/usage/eks-managed-nodes/">here</a>. (Note that if you only taint arm64 instances and don't specify any node selectors, then you will need to ensure that the images you build for Graviton instances are multiarch images that can also run on x86 instance types. Alternatively, you can build arm64-only images and ensure that they are only scheduled onto arm64 images using node selectors.)</p>
<h4 id="cluster-autoscaler-considerations"><a class="header" href="#cluster-autoscaler-considerations">Cluster Autoscaler considerations</a></h4>
<p>If using the Kubernetes <a href="https://github.com/kubernetes/autoscaler">Cluster Autoscaler</a> in a cluster with both x86-based and Graviton instance types, note that you should tag each Auto Scaling group with <code>k8s.io/cluster-autoscaler/node-template/label/*</code> or <code>k8s.io/cluster-autoscaler/node-template/taint/*</code> tags as documented <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html">here</a> to ensure that the Autoscaler can tell which pods can be placed in which ASG. (Note that this does not actually apply any labels or taints to nodes, but serves only to give scheduling hints to the Cluster Autoscaler.)</p>
<hr />
<h3 id="further-reading"><a class="header" href="#further-reading">Further reading</a></h3>
<ul>
<li><a href="https://github.com/aws-ia/ecs-blueprints/blob/main/terraform/fargate-examples/graviton/README.md">ECS blueprints on AWS Graviton</a></li>
<li><a href="https://www.docker.com/blog/speed-up-building-with-docker-buildx-and-graviton2-ec2/">Speed up Docker BuildX by building natively on EC2 Graviton</a></li>
<li><a href="https://tech.smartling.com/building-multi-architecture-docker-images-on-arm-64-c3e6f8d78e1c">Building multi-arch docker images with buildx</a></li>
<li><a href="https://community.arm.com/developer/tools-software/tools/b/tools-software-ides-blog/posts/unifying-arm-software-development-with-docker">Unifying Arm software development with Docker</a></li>
<li><a href="https://duske.me/posts/modern-multiarch-builds-with-docker/">Modern multi-arch builds with docker</a></li>
<li><a href="https://spot.io/blog/eks-simplified-on-ec2-graviton2-instances/">Leveraging Spot and Graviton2 with EKS</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="headless-website-testing-with-chrome-and-puppeteer-on-graviton"><a class="header" href="#headless-website-testing-with-chrome-and-puppeteer-on-graviton">Headless website testing with Chrome and Puppeteer on Graviton.</a></h1>
<p>Chrome is a popular reference browser for web site unit testing on EC2 x86 instances. In the samer manner it can be used on EC2 Graviton instances.
It is open source in its Chromium incarnation, supports 'headless' mode and can simulate user actions via Javascript.
The Chrome web site <a href="https://developer.chrome.com/blog/headless-chrome/#using-programmatically-node">mentions Puppeteer</a> and it is used here to show a specific
example of headless website testing on Graviton.
“<a href="https://pptr.dev/">Puppeteer</a> is a Node.js library which provides a high-level API to control Chrome/Chromium over the DevTools Protocol.
Puppeteer runs in headless mode by default, but can be configured to run in full ("headful") Chrome/Chromium."
It can serve as a replacement for the previously very popular Phantomjs.
“PhantomJS (phantomjs.org) is a headless WebKit scriptable with JavaScript. The latest stable release is version 2.1.
Important: PhantomJS development is suspended until further notice (see #15344 for more details).“
The APIs are different, so code targetting PhantomJS has to be rewritten when moving to Puppeteer (see Appendix).
Puppeteer is open source and has 466 contributors and 364k users and thus is likely to be supported for some time.</p>
<h3 id="get-a-recent-version-of-nodejs"><a class="header" href="#get-a-recent-version-of-nodejs">Get a recent version of NodeJS.</a></h3>
<p>Ubuntu-22:</p>
<pre><code>sudo apt-get update
sudo apt-get install -y ca-certificates curl gnupg
sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://deb.nodesource.com/gpgkey/nodesource-repo.gpg.key | sudo gpg --dearmor -o /etc/apt/keyrings/nodesource.gpg
NODE_MAJOR=20
echo "deb [signed-by=/etc/apt/keyrings/nodesource.gpg] https://deb.nodesource.com/node_$NODE_MAJOR.x nodistro main" | sudo tee /etc/apt/sources.list.d/nodesource.list
sudo apt-get update
sudo apt-get install nodejs -y
</code></pre>
<p>AL2023:</p>
<pre><code>sudo yum install https://rpm.nodesource.com/pub_20.x/nodistro/repo/nodesource-release-nodistro-1.noarch.rpm -y
sudo yum install nodejs -y --setopt=nodesource-nodejs.module_hotfixes=1
</code></pre>
<p>[https://github.com/nodesource/distributions/tree/master]</p>
<h3 id="install-puppeteer"><a class="header" href="#install-puppeteer">Install Puppeteer.</a></h3>
<pre><code>npm i puppeteer@21.1.0
</code></pre>
<p>Puppeteer packages an x86 version of Chrome which needs to be replaced with an aarch64 version.</p>
<h3 id="install-aarch64-version-of-chrome"><a class="header" href="#install-aarch64-version-of-chrome">Install aarch64 version of Chrome.</a></h3>
<p>Ubuntu-22:</p>
<pre><code>sudo apt update
sudo apt install chromium-browser chromium-codecs-ffmpeg
</code></pre>
<p>AL2023:</p>
<pre><code>QTVER=5.15.9-2
CHROMEVER=116.0.5845.96

sudo dnf -y install \
    https://kojipkgs.fedoraproject.org//packages/minizip/2.8.9/2.el8/aarch64/minizip-2.8.9-2.el8.aarch64.rpm \
    https://download-ib01.fedoraproject.org/pub/epel/9/Everything/aarch64/Packages/n/nss-mdns-0.15.1-3.1.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/gstreamer1-1.18.4-4.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/libcanberra-0.30-26.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/libcanberra-gtk3-0.30-26.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/sound-theme-freedesktop-0.8-17.el9.noarch.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/qt5-qtbase-$QTVER.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/qt5-qtbase-common-$QTVER.el9.noarch.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/qt5-qtbase-gui-$QTVER.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/glx-utils-8.4.0-12.20210504git0f9e7d9.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/pipewire-libs-0.3.47-2.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/fdk-aac-free-2.0.0-8.el9.aarch64.rpm \
    http://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/libldac-2.0.2.3-10.el9.aarch64.rpm \
    https://kojipkgs.fedoraproject.org//packages/chromium/$CHROMEVER/1.el8/aarch64/chromium-$CHROMEVER-1.el8.aarch64.rpm \
    https://kojipkgs.fedoraproject.org//packages/chromium/$CHROMEVER/1.el8/aarch64/chromium-common-$CHROMEVER-1.el8.aarch64.rpm \
    https://kojipkgs.fedoraproject.org//packages/chromium/$CHROMEVER/1.el8/aarch64/chromium-headless-$CHROMEVER-1.el8.aarch64.rpm \
    https://kojipkgs.fedoraproject.org//packages/chromium/$CHROMEVER/1.el8/aarch64/chromedriver-$CHROMEVER-1.el8.aarch64.rpm
</code></pre>
<p>As QT version above will be updated, and the old one become unavailable at some point, the version variable will need to be changed accordingly.
If this is the case, check: https://mirror.stream.centos.org/9-stream/AppStream/aarch64/os/Packages/ to see which version is available.</p>
<pre><code>rm .cache/puppeteer/chrome/linux-116.0.5845.96/chrome-linux64/chrome
ln -s /usr/bin/chromium-browser .cache/puppeteer/chrome/linux-116.0.5845.96/chrome-linux64/chrome
</code></pre>
<p>This brute force install works because:</p>
<pre><code>/snap/bin/chromium --version
Chromium 116.0.5845.96 snap
</code></pre>
<p>Puppeteer needs a particular version of chromium, in this case (Puppeteer-21.1.0), it uses Chromium 116.0.5845.96.
If a different version of Puppeteer is needed, the directory <code>~/.cache/puppeteer/chrome</code> indicates which version of Chromium it is targeting.
This version must be assigned to the <code>CHROMEVER</code> variable above.
If the required version of Chromium is not available at <code>https://kojipkgs.fedoraproject.org/packages/chromium/</code>
then proceed to <code>https://chromium.googlesource.com/chromium/src/+/main/docs/linux/build_instructions.md</code>.</p>
<h3 id="code-examples"><a class="header" href="#code-examples">Code Examples</a></h3>
<p>Example code for Puppeteer can be found at https://github.com/puppeteer/examples</p>
<h3 id="virtual-framebuffer-xserver"><a class="header" href="#virtual-framebuffer-xserver">Virtual Framebuffer Xserver</a></h3>
<p>Some code examples, such as puppeteer/examples/oopif.js, may need a 'headful' chrome and thus an Xserver.
A virtual framebuffer Xserver can be used for that.</p>
<p>Ubuntu-22.04: <code>sudo apt install xvfb</code>
AL2023: <code>sudo yum install Xvfb</code></p>
<pre><code>Xvfb &amp;
export DISPLAY=:0
</code></pre>
<p>When Chrome is now invoked in headful mode, it has an Xserver to render to.</p>
<p>This can be tested with:</p>
<pre><code>node puppeteer/examples/oopif.js
</code></pre>
<p>The oopif.js example invokes chrome in headful mode.</p>
<h2 id="appendix"><a class="header" href="#appendix">Appendix.</a></h2>
<h3 id="code-example-to-show-the-difference-in-api-between-phantomjs-and-puppeteer"><a class="header" href="#code-example-to-show-the-difference-in-api-between-phantomjs-and-puppeteer">Code example to show the difference in API between PhantomJS and Puppeteer.</a></h3>
<p>Puppeteer Screenshot:</p>
<pre><code>'use strict';

const puppeteer = require('puppeteer');

(async () =&gt; {
  const browser = await puppeteer.launch();
  const page = await browser.newPage();
  await page.goto('https://www.google.com', {waitUntil: 'load', timeout: 1000});
  await page.screenshot({path: 'google.png'});
  await browser.close();
})();
</code></pre>
<p>The same with PhantomJS:</p>
<pre><code>var page = require('webpage').create();
page.open('http://www.google.com', function() {
    setTimeout(function() {
        page.render('google.png');
        phantom.exit();
    }, 200);
});
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="building-librdkafka-for-aws-graviton-including-python-module"><a class="header" href="#building-librdkafka-for-aws-graviton-including-python-module">Building librdkafka for AWS Graviton (including Python module)</a></h1>
<p><a href="https://github.com/confluentinc/librdkafka">librdkafka</a> is a C library
implementation of the Apache Kafka protocol, providing Producer, Consumer and
Admin clients. It was designed with message delivery reliability and high
performance in mind.</p>
<h2 id="table-of-contents"><a class="header" href="#table-of-contents">Table of contents</a></h2>
<!-- no toc -->
<ul>
<li><a href="software/librdkafka.html#amazon-linux-2">Amazon Linux 2</a></li>
<li><a href="software/librdkafka.html#red-hat-enterprise-linux-8-and-compatible-el-8-distributions-eg-rocky-linux">Red Hat Enterprise Linux 8 (and compatible EL 8 distributions e.g. Rocky Linux)</a></li>
<li><a href="software/librdkafka.html#ubuntu-2004-focal-and-2204-jammy">Ubuntu 20.04 (Focal) and 22.04 (Jammy)</a></li>
</ul>
<h2 id="amazon-linux-2"><a class="header" href="#amazon-linux-2">Amazon Linux 2</a></h2>
<p>First, install the necessary dependencies, add the current user to the <code>mock</code> group, and log out.</p>
<pre><code class="language-sh">sudo amazon-linux-extras install -y mock2
sudo yum -y install git
sudo usermod -G mock `id -un`
logout
</code></pre>
<p>Next, log back into the instance, and run:</p>
<pre><code class="language-sh">export LIBRDKAFKA_VERSION=2.3.0   #The minimum version required is 2.3.0
git clone -b v${LIBRDKAFKA_VERSION} https://github.com/confluentinc/librdkafka 
cd librdkafka/packaging/rpm
MOCK_CONFIG=/etc/mock/amazonlinux-2-aarch64.cfg make
# Packages will be placed in ./pkgs-${LIBRDKAFKA_VERSION}-1-/etc/mock/amazonlinux-2-aarch64.cfg/
cd ./pkgs-${LIBRDKAFKA_VERSION}-1-/etc/mock/amazonlinux-2-aarch64.cfg/
sudo yum -y install *.aarch64.rpm
</code></pre>
<p>Once you have installed the RPM packages, you can build and install the Python module:</p>
<pre><code class="language-sh">sudo yum -y install gcc python3-devel
python3 -m pip install --user --no-binary confluent-kafka confluent-kafka
</code></pre>
<h2 id="red-hat-enterprise-linux-8-and-compatible-el-8-distributions-eg-rocky-linux"><a class="header" href="#red-hat-enterprise-linux-8-and-compatible-el-8-distributions-eg-rocky-linux">Red Hat Enterprise Linux 8 (and compatible EL 8 distributions e.g. Rocky Linux)</a></h2>
<p>First, install the necessary dependencies, add the current user to the <code>mock</code> group, and log out.</p>
<pre><code>sudo dnf config-manager --set-enabled powertools
sudo dnf install -y git make epel-release
sudo dnf install -y mock
sudo usermod -G mock `id -un`
logout
</code></pre>
<p>Next, log back into the instance, and run:</p>
<pre><code class="language-sh">export LIBRDKAFKA_VERSION=2.0.2 # Or whichever version you need. We tested with 2.0.2.
git clone -b v${LIBRDKAFKA_VERSION} https://github.com/confluentinc/librdkafka 
cd librdkafka/packaging/rpm
make
# Packages will be placed in ./pkgs-${LIBRDKAFKA_VERSION}-1-/etc/mock/amazonlinux-2-aarch64.cfg/
cd ./pkgs-2.0.2-1-default
sudo dnf -y install *.aarch64.rpm
</code></pre>
<p>Once you have installed the RPM packages, you can build and install the Python module:</p>
<pre><code class="language-sh">sudo dnf -y install gcc python3-devel
python3 -m pip install --user --no-binary confluent-kafka confluent-kafka
</code></pre>
<h2 id="ubuntu-2004-focal-and-2204-jammy"><a class="header" href="#ubuntu-2004-focal-and-2204-jammy">Ubuntu 20.04 (Focal) and 22.04 (Jammy)</a></h2>
<pre><code class="language-sh">export LIBRDKAFKA_VERSION=2.0.2 # Or whichever version you need. We tested with 2.0.2.
export EMAIL=builder@example.com
sudo apt-get update
sudo apt-get install -y git-buildpackage debhelper zlib1g-dev libssl-dev libsasl2-dev liblz4-dev
git clone https://github.com/confluentinc/librdkafka 
cd librdkafka
git checkout -b debian v${LIBRDKAFKA_VERSION}
dch --newversion ${LIBRDKAFKA_VERSION}-1 "Release version ${LIBRDKAFKA_VERSION}" --urgency low
dch --release --distribution unstable ""
git commit -am "Tag Debian release ${LIBRDKAFKA_VERSION}"
mkdir ../build-area
git archive --format=tgz --output=../build-area/librdkafka_${LIBRDKAFKA_VERSION}.orig.tar.gz HEAD
gbp buildpackage -us -uc --git-verbose --git-builder="debuild --set-envvar=VERSION=${LIBRDKAFKA_VERSION} --set-envvar=SKIP_TESTS=y -i -I" --git-ignore-new
</code></pre>
<p>This will yield a set of Debian packages in the build area. To install them:</p>
<pre><code class="language-sh">sudo dpkg -i ../build-area/*_arm64.deb
</code></pre>
<p>Once you have installed the packages, you can build and install the Python module:</p>
<pre><code class="language-sh">python3 -m pip install --user --no-binary confluent-kafka confluent-kafka
</code></pre>
<h2 id="example-dockerfile-for-python-module"><a class="header" href="#example-dockerfile-for-python-module">Example Dockerfile for Python module</a></h2>
<p>The following <code>Dockerfile</code> can be used to build a container image based on
Debian Bullseye containing the Python module. It produces a minimized image via
a multi-stage build.</p>
<pre><code>FROM public.ecr.aws/docker/library/python:3.10.10-slim-bullseye AS build

ARG LIBRDKAFKA_VERSION=2.0.2
ENV EMAIL=nobody@build.example.com

WORKDIR /build
RUN apt-get update &amp;&amp; \
    apt-get install -y git-buildpackage debhelper zlib1g-dev libssl-dev libsasl2-dev liblz4-dev python3-dev &amp;&amp; \
    git clone https://github.com/confluentinc/librdkafka &amp;&amp; \
    cd librdkafka &amp;&amp; \
    git checkout -b debian v${LIBRDKAFKA_VERSION} &amp;&amp; \
    dch --newversion ${LIBRDKAFKA_VERSION}-1 "Release version ${LIBRDKAFKA_VERSION}" --urgency low &amp;&amp; \
    dch --release --distribution unstable "" &amp;&amp; \
    git commit -am "Tag Debian release ${LIBRDKAFKA_VERSION}" &amp;&amp; \
    mkdir ../build-area &amp;&amp; \
    git archive --format=tgz --output=../build-area/librdkafka_${LIBRDKAFKA_VERSION}.orig.tar.gz HEAD &amp;&amp; \
    gbp buildpackage -us -uc --git-verbose --git-builder="debuild --set-envvar=VERSION=${LIBRDKAFKA_VERSION} --set-envvar=SKIP_TESTS=y -i -I" --git-ignore-new &amp;&amp; \
    apt-get -y install ../build-area/*.deb &amp;&amp; \
    python3 -m pip install --no-binary confluent-kafka confluent-kafka


FROM public.ecr.aws/docker/library/python:3.10.10-slim-bullseye
ARG LIBRDKAFKA_VERSION=2.0.2
COPY --from=build /build/build-area/*.deb /tmp/
RUN apt-get update &amp;&amp; apt-get -y install /tmp/*.deb &amp;&amp; apt-get clean &amp;&amp; rm -rf /var/cache/apt
COPY --from=build /usr/local/lib/python3.10/site-packages/confluent_kafka-${LIBRDKAFKA_VERSION}-py3.10.egg-info \
                  /usr/local/lib/python3.10/site-packages/confluent_kafka-${LIBRDKAFKA_VERSION}-py3.10.egg-info
COPY --from=build /usr/local/lib/python3.10/site-packages/confluent_kafka/ \
                  /usr/local/lib/python3.10/site-packages/confluent_kafka/
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="amazon-machine-images-amis"><a class="header" href="#amazon-machine-images-amis">Amazon Machine Images (AMIs)</a></h1>
<p>This document covers how to find AMIs compatible with AWS Graviton, and how to look up and use the AMIs in AWS System Manager and AWS CloudFormation.</p>
<h2 id="using-the-console"><a class="header" href="#using-the-console">Using the console</a></h2>
<p>AMIs can both be found in the console as explained in the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/finding-an-ami.html#finding-an-ami-console">AWS documentation</a>
when launching instances interactively.</p>
<h2 id="using-apis---aws-systems-manager-parameter-store"><a class="header" href="#using-apis---aws-systems-manager-parameter-store">Using APIs - AWS Systems Manager Parameter Store</a></h2>
<p>When integrating the AMI lookup process in a script or in a piece of code, it is more convenient to leverage <a href="https://aws.amazon.com/systems-manager/">AWS Systems Manager</a> Parameter Store.</p>
<p>There's a good article about it on the AWS Compute blog: <a href="https://aws.amazon.com/blogs/compute/query-for-the-latest-amazon-linux-ami-ids-using-aws-systems-manager-parameter-store/">Query for the latest Amazon Linux AMI IDs using AWS Systems Manager Parameter Store</a>.</p>
<div class="table-wrapper"><table><thead><tr><th>OS release</th><th>Parameter name</th></tr></thead><tbody>
<tr><td>Amazon Linux 2023</td><td><code>/aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-arm64</code></td></tr>
<tr><td>Amazon Linux 2023 minimal</td><td><code>/aws/service/ami-amazon-linux-latest/al2023-ami-minimal-kernel-default-arm64</code></td></tr>
<tr><td>Amazon Linux 2</td><td><code>/aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-arm64-gp2</code></td></tr>
<tr><td>Ubuntu 24.04</td><td><code>/aws/service/canonical/ubuntu/server/24.04/stable/current/arm64/hvm/ebs-gp3/ami-id</code></td></tr>
<tr><td>Ubuntu 22.04</td><td><code>/aws/service/canonical/ubuntu/server/22.04/stable/current/arm64/hvm/ebs-gp2/ami-id</code></td></tr>
<tr><td>Ubuntu 20.04</td><td><code>/aws/service/canonical/ubuntu/server/20.04/stable/current/arm64/hvm/ebs-gp2/ami-id</code></td></tr>
<tr><td>Debian 12</td><td><code>/aws/service/debian/release/12/latest/arm64</code></td></tr>
<tr><td>Debian 11</td><td><code>/aws/service/debian/release/11/latest/arm64</code></td></tr>
<tr><td>SLES 15 SP6</td><td><code>/aws/service/suse/sles/15-sp6/arm64/latest</code></td></tr>
</tbody></table>
</div>
<p>Here is an example to get the AMI ID of the latest <strong>Amazon Linux 2023</strong> version in the us-east-1 region:</p>
<pre><code class="language-sh">$ aws ssm get-parameter --name /aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-arm64 --region us-east-1 --query Parameter.Value --output text
</code></pre>
<p>Here is an example to get the AMI ID of the latest <strong>Amazon Linux 2</strong> version in the us-east-1 region:</p>
<pre><code class="language-sh">$ aws ssm get-parameter --name /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-arm64-gp2 --region us-east-1 --query Parameter.Value --output text
</code></pre>
<p>AWS CloudFormation also supports AWS Systems Manager Parameter Store for obtaining AMI IDs without
hard-coding them.</p>
<p>Here is an example demonstrating how to refer to the latest <strong>Amazon Linux 2023 AMI</strong> for Graviton/arm64 in a CloudFormation template:</p>
<pre><code class="language-yaml">Parameters:
  LatestAmiId:
    Type: AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;
    Default: /aws/service/ami-amazon-linux-latest/al2023-ami-kernel-default-arm64

Resources:
 GravitonInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref LatestAmiId
      InstanceType: c7g.medium
</code></pre>
<p>Here is an example demonstrating how to refer to the latest <strong>Amazon Linux 2</strong> AMI for Graviton/arm64 in a CloudFormation template:</p>
<pre><code class="language-yaml">Parameters:
  LatestAmiId:
    Type: AWS::SSM::Parameter::Value&lt;AWS::EC2::Image::Id&gt;
    Default: /aws/service/ami-amazon-linux-latest/amzn2-ami-hvm-arm64-gp2

Resources:
 GravitonInstance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: !Ref LatestAmiId
      InstanceType: c7g.medium
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><p>In addition to the AWS Graviton-based Amazon EC2 instances, Graviton is also available via a growing list of AWS managed services as follows:</p>
<p>Note: You can always find the latest Graviton announcements via these <a href="https://aws.amazon.com/new/?whats-new-content-all.sort-by=item.additionalFields.postDateTime&amp;whats-new-content-all.sort-order=desc&amp;whats-new-content-all.q=Graviton&amp;whats-new-content-all.q_operator=AND#What.27s_New_Feed">What's New posts</a>.</p>
<div class="table-wrapper"><table><thead><tr><th style="text-align: center">Service</th><th style="text-align: center">Status</th><th>Resources</th></tr></thead><tbody>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/app-mesh/">AWS App Mesh</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/11/aws-app-mesh-arm64-envoy-images/">AWS App Mesh now supports ARM64-based Envoy Images</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/rds/aurora/">Amazon Aurora</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2023/05/amazon-aurora-mysql-postgresql-graviton3-based-r7g-instance-family/">Amazon Aurora MySQL and PostgreSQL support for Graviton3 based R7g instance family</a>, <a href="https://aws.amazon.com/about-aws/whats-new/2021/03/achieve-up-to-35-percent-better-price-performance-with-amazon-aurora-using-new-graviton2-instances/">Achieve up to 35% better price/performance with Amazon Aurora using new Graviton2 instances</a><br>Related blog: <a href="https://aws.amazon.com/blogs/database/key-considerations-in-moving-to-graviton2-for-amazon-rds-and-amazon-aurora-databases/">Key considerations in moving to Graviton2 for Amazon RDS and Amazon Aurora databases</a><br>For supported instance types and database engine versions see <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Concepts.DBInstanceClass.html">Aurora DB Instances</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/ec2/autoscaling/">Amazon EC2 Auto Scaling</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2020/11/amazon-ec2-auto-scaling-announces-support-for-multiple-launch-templates-for-auto-scaling-groups/">Amazon EC2 Auto Scaling announces support for multiple launch templates for Auto Scaling groups</a><br>Associated blog: <a href="https://aws.amazon.com/blogs/compute/supporting-aws-graviton2-and-x86-instance-types-in-the-same-auto-scaling-group/">Supporting AWS Graviton2 and x86 instance types in the same Auto Scaling group</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/batch/">AWS Batch</a></td><td style="text-align: center">GA</td><td>Blog: <a href="https://aws.amazon.com/blogs/devops/target-cross-platform-go-builds-with-aws-codebuild-batch-builds/">Target cross-platform Go builds with AWS CodeBuild Batch builds</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/codebuild/">AWS CodeBuild</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/02/aws-codebuild-supports-arm-based-workloads-using-aws-graviton2/">AWS CodeBuild supports Arm-based workloads using AWS Graviton2</a></td></tr>
<tr><td style="text-align: center"><a href="https://codecatalyst.aws/">Amazon CodeCatalyst</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2023/04/general-availability-amazon-codecatalyst/">Announcing the general availability of Amazon CodeCatalyst</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/documentdb/">Amazon DocumentDB</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/11/better-performance-amazon-documentdb-mongodb-graviton2-instances/">Achieve up to 30% better performance with Amazon DocumentDB (with MongoDB compatibility) using new Graviton2 instances</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/ecr/">Amazon ECR</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2020/05/ecr-now-supports-manifest-lists-for-multi-architecture-images/">ECR now supports Manifest Lists for multi-architecture images</a><br>Associated blog: <a href="https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/">Introducing multi-architecture container images for Amazon ECR</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/ecs/">Amazon ECS</a></td><td style="text-align: center">GA</td><td><a href="https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs-optimized_AMI.html">Amazon ECS-optimized AMIs</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/eks/">Amazon EKS</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2020/08/amazon-eks-support-for-arm-based-instances-powered-by-aws-graviton-now-generally-available/">Amazon EKS support for Arm-based instances powered by AWS Graviton is now generally available</a><br>Launch Blog: <a href="https://aws.amazon.com/blogs/containers/eks-on-graviton-generally-available/">Amazon EKS on AWS Graviton2 generally available: considerations on multi-architecture apps</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/elasticbeanstalk/">AWS Elastic Beanstalk</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/11/elastic-beanstalk-aws-graviton-ec2/">Elastic Beanstalk supports AWS Graviton-based Amazon EC2 instance types</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/elasticache/">Amazon ElastiCache</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2023/08/amazon-elasticache-m7g-r7g-graviton-3-nodes/">Amazon ElastiCache now supports M7g and R7g Graviton3-based nodes</a>, <a href="https://aws.amazon.com/about-aws/whats-new/2020/10/amazon-elasticache-now-supports-m6g-and-r6g-graviton2-based-instances/">Amazon ElastiCache now supports M6g and R6g Graviton2-based instances</a><br> What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/11/amazon-elasticache-supports-t4g-graviton2-based-instances/">Amazon ElastiCache now supports T4g Graviton2-based instances</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/emr/">Amazon EMR</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2023/03/amazon-emr-amazon-ec2-c7g-graviton3-instances/">Amazon EMR now supports Amazon EC2 C7g (Graviton3) instances</a>, <a href="https://aws.amazon.com/about-aws/whats-new/2020/12/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance/">Amazon EMR now provides up to 30% lower cost and up to 15% improved performance for Spark workloads on Graviton2-based instances</a><br>Launch Blog: <a href="https://aws.amazon.com/blogs/big-data/amazon-emr-now-provides-up-to-30-lower-cost-and-up-to-15-improved-performance-for-spark-workloads-on-graviton2-based-instances/">Amazon EMR now provides up to 30% lower cost and up to 15% improved performance for Spark workloads on Graviton2-based instances</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/emr/serverless/">Amazon EMR Serverless</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2022/11/aws-graviton2-emr-serverless-35-percent-price-performance-spark-hive-workloads/">Announcing AWS Graviton2 support for Amazon EMR Serverless - Get up to 35% better price-performance for your serverless Spark and Hive workload</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/fargate/">AWS Fargate</a></td><td style="text-align: center">GA</td><td>Launch Blog: <a href="https://aws.amazon.com/blogs/aws/announcing-aws-graviton2-support-for-aws-fargate-get-up-to-40-better-price-performance-for-your-serverless-containers/">Announcing AWS Graviton2 Support for AWS Fargate – Get up to 40% Better Price-Performance for Your Serverless Containers</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/gamelift/">Amazon Gamelift</a></td><td style="text-align: center">GA</td><td>Launch Blog: <a href="https://aws.amazon.com/blogs/gametech/now-available-new-asia-pacific-osaka-region-and-graviton2-support-for-amazon-gamelift/">Now available: New Asia Pacific (Osaka) region and Graviton2 support for Amazon GameLift</a><br>Addition of Graviton3: <a href="https://aws.amazon.com/about-aws/whats-new/2023/08/amazon-gamelift-instances-aws-graviton-3-processors/">Announcing Amazon GameLift support for instances powered by AWS Graviton3 processors</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/lambda/">AWS Lambda</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/09/better-price-performance-aws-lambda-functions-aws-graviton2-processor/">Achieve up to 34% better price/performance with AWS Lambda Functions powered by AWS Graviton2 processor</a><br>Launch Blog: <a href="https://aws.amazon.com/blogs/aws/aws-lambda-functions-powered-by-aws-graviton2-processor-run-your-functions-on-arm-and-get-up-to-34-better-price-performance/">AWS Lambda Functions Powered by AWS Graviton2 Processor – Run Your Functions on Arm and Get Up to 34% Better Price Performance</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/memorydb/">Amazon MemoryDB</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/08/amazon-memorydb-redis/">Announcing Amazon MemoryDB for Redis</a><br>Launch Blog: <a href="https://aws.amazon.com/blogs/aws/introducing-amazon-memorydb-for-redis-a-redis-compatible-durable-in-memory-database-service/">Introducing Amazon MemoryDB for Redis – A Redis-Compatible, Durable, In-Memory Database Service</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/msk/">Amazon Managed Streaming for Apache Kafka (MSK)</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-msk-graviton3-m7g-instances-provisioned-clusters/">Amazon MSK now supports Graviton3-based M7g instances for new provisioned clusters</a><br>In-place upgrades: <a href="https://aws.amazon.com/about-aws/whats-new/2024/06/amazon-msk-upgrades-m5-t3-instance-graviton3-m7g/">Amazon MSK supports in-place upgrades from M5, T3 instance types to Graviton3 based M7G</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/neptune/">Amazon Neptune</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/11/aws-graviton2-based-instances-amazon-neptune/">Announcing AWS Graviton2-based instances for Amazon Neptune</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/opensearch-service/">Amazon OpenSearch Service</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2021/05/amazon-elasticsearch-service-offers-aws-graviton2-m6g-c6g-r6g-r6gd-instances/">Amazon Elasticsearch Service now offers AWS Graviton2 (M6g, C6g, R6g, and R6gd) instances</a><br>Related blog: <a href="https://aws.amazon.com/blogs/big-data/increase-amazon-elasticsearch-service-performance-by-upgrading-to-graviton2/">Increase Amazon Elasticsearch Service performance by upgrading to Graviton2</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/rds/">Amazon RDS</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2023/04/amazon-rds-m7g-r7g-database-instances/">Amazon RDS now supports M7g and R7g database instances</a>, <a href="https://aws.amazon.com/about-aws/whats-new/2020/10/achieve-up-to-52-percent-better-price-performance-with-amazon-rds-using-new-graviton2-instances/">Achieve up to 52% better price/performance with Amazon RDS using new Graviton2 instances</a><br>Launch Blog: <a href="https://aws.amazon.com/blogs/aws/new-amazon-rds-on-graviton2-processors/">New – Amazon RDS on Graviton2 Processors</a><br>Related blog: <a href="https://aws.amazon.com/blogs/database/key-considerations-in-moving-to-graviton2-for-amazon-rds-and-amazon-aurora-databases/">Key considerations in moving to Graviton2 for Amazon RDS and Amazon Aurora databases</a><br>For supported instance types and database engine versions see <a href="https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.DBInstanceClass.html">RDS DB Instances</a></td></tr>
<tr><td style="text-align: center"><a href="https://aws.amazon.com/pm/sagemaker/">Amazon SageMaker</a></td><td style="text-align: center">GA</td><td>What's New: <a href="https://aws.amazon.com/about-aws/whats-new/2022/10/amazon-sagemaker-adds-new-graviton-based-instances-model-deployment/">Amazon SageMaker adds eight new Graviton-based instances for model deployment</a> <br> Related blog: <a href="https://aws.amazon.com/blogs/machine-learning/run-machine-learning-inference-workloads-on-aws-graviton-based-instances-with-amazon-sagemaker/">Run machine learning inference workloads on AWS Graviton-based instances with Amazon SageMaker</a>, <a href="https://aws.amazon.com/blogs/machine-learning/reduce-amazon-sagemaker-inference-cost-with-aws-graviton/">Reduce Amazon SageMaker inference cost with AWS Graviton</a></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><p>We have compiled a list of Independent Software Vendor (ISV) products publicly listing arm64/Graviton support.
If your favorite ISV product isn't listed below, it doesn't mean it's not supported.</p>
<p>We welcome your feedback at ec2-arm-dev-feedback@amazon.com or by opening an issue in this GitHub repo. We also welcome your Pull Requests updating this list.</p>
<p>Additionally:</p>
<ul>
<li>If you are a developer looking for ISV product support for arm64, please also reach out to your ISV.</li>
<li>If you are an ISV and would like to partner with us, please also reach out to your AWS Partner Network (<a href="https://aws.amazon.com/partners/">APN</a>) contact.</li>
</ul>
<div class="table-wrapper"><table><thead><tr><th>Category</th><th>ISV</th><th>Product</th><th>Resources</th></tr></thead><tbody>
<tr><td>Analytics</td><td><a href="https://partners.amazonaws.com/partners/001E000001OMadFIAT/Confluent,%20Inc.">Confluent</a></td><td><a href="https://www.confluent.io/product/confluent-platform/">Confluent Platform</a></td><td>Announcement: https://www.confluent.io/blog/introducing-confluent-platform-7-6/#arm64-support<br>Documentation: https://docs.confluent.io/platform/current/installation/system-requirements.html</td></tr>
<tr><td>Analytics</td><td><a href="https://partners.amazonaws.com/partners/001E0000016WxP5IAK/Databricks">Databricks</a></td><td><a href="https://databricks.com/product/data-lakehouse">Databricks Lakehouse</a></td><td>Announcement: https://databricks.com/blog/2022/04/18/announcing-databricks-support-for-aws-graviton2-with-up-to-3x-better-price-performance.html<br>Documentation: https://docs.databricks.com/clusters/graviton.html</td></tr>
<tr><td>Analytics</td><td><a href="https://partners.amazonaws.com/partners/001E000000UfalSIAR">InterSystems</a></td><td><a href="https://www.intersystems.com/products/intersystems-iris/">IRIS (Community Edition)</a></td><td>Release: https://aws.amazon.com/marketplace/pp/B08629M8S4<br>Announcement: https://community.intersystems.com/post/intersystems-iris-now-available-aws-graviton2-based-amazon-ec2-instances</td></tr>
<tr><td>Analytics</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl139IAB/Splunk">Splunk</a></td><td><a href="https://www.splunk.com/en_us/software/splunk-cloud-platform.html">Splunk Cloud Platform</a></td><td>Announcement: https://www.splunk.com/en_us/blog/platform/splunk-embarks-on-aws-graviton-journey-with-amazon-ec2-im4gn-and-is4gen-instances.html</td></tr>
<tr><td>Analytics</td><td><a href="https://partners.amazonaws.com/partners/0010L00001rFEKcQAO">Starburst</a></td><td><a href="https://www.starburst.io/platform/starburst-enterprise/">Starburst Enterprise</a></td><td>Release: https://aws.amazon.com/marketplace/pp/prodview-2bpppdqlesn6w<br>Announcement: https://docs.starburst.io/361-e/release/release-360-e.html</td></tr>
<tr><td>Android</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl0yjIAB">Canonical</a></td><td><a href="https://anbox-cloud.io/">Anbox Cloud</a></td><td>Release: https://aws.amazon.com/marketplace/pp/prodview-aqmdt52vqs5qk<br>Announcement: https://anbox-cloud.io/docs/tut/installing-appliance, https://ubuntu.com/blog/canonical-achieves-aws-graviton-ready-designation</td></tr>
<tr><td>Android</td><td><a href="https://www.genymobile.com/">Genymobile</a></td><td><a href="https://www.genymotion.com/">Genymotion Cloud</a></td><td>Release: https://aws.amazon.com/marketplace/pp/B08KHVZWMJ<br>Announcement: https://www.genymotion.com/blog/just-launched-arm-native-android-in-the-cloud-environment-on-aws/</td></tr>
<tr><td>Containers</td><td><a href="https://partners.amazonaws.com/partners/001E000001BxMyKIAV">HashiCorp</a></td><td><a href="https://www.hashicorp.com/products/consul">Consul</a></td><td>Release: https://www.consul.io/downloads<br>Announcement: https://github.com/hashicorp/consul/blob/v0.8.4/CHANGELOG.md</td></tr>
<tr><td>Containers</td><td><a href="https://partners.amazonaws.com/partners/001E0000016ohrNIAQ">Docker</a></td><td><a href="https://www.docker.com/products/build-cloud/">Docker Build Cloud</a></td><td>Release: https://www.docker.com/products/build-cloud/<br>Announcement: https://www.docker.com/blog/introducing-docker-build-cloud/</td></tr>
<tr><td>Containers</td><td><a href="https://kinvolk.io">Kinvolk</a></td><td><a href="https://kinvolk.io/flatcar-container-linux/pro/">Flatcar Container Linux</a></td><td>Release: https://kinvolk.io/flatcar-container-linux/releases/<br>Announcement: https://kinvolk.io/docs/flatcar-container-linux/latest/installing/cloud/aws-ec2/</td></tr>
<tr><td>Containers</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl0xiIAB">Red Hat</a></td><td><a href="https://www.redhat.com/en/technologies/cloud-computing/openshift/container-platform">OpenShift Container Platform</a></td><td>Release: https://access.redhat.com/documentation/en-us/openshift_container_platform/4.10/html-single/release_notes/index#ocp-4-10-OCP-on-ARM<br>Announcement: https://cloud.redhat.com/blog/introducing-red-hat-openshift-4.10</td></tr>
<tr><td>Containers</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl0xiIAB">Red Hat</a></td><td><a href="https://aws.amazon.com/rosa/">ROSA HCP</a></td><td>Release: https://docs.redhat.com/en/documentation/red_hat_openshift_service_on_aws/4/html-single/introduction_to_rosa/index#rosa-sdpolicy-aws-instance-types-graviton_rosa-hcp-instance-types</td></tr>
<tr><td>Data Science &amp; ML</td><td><a href="https://partners.amazonaws.com/partners/0010L00001qFzbyQAC">Anaconda</a></td><td><a href="https://www.anaconda.com/products/commercial-edition">Anaconda</a></td><td>Release: https://docs.anaconda.com/anaconda/install/graviton2/, https://repo.anaconda.com/pkgs/main/linux-aarch64/<br>Announcement: https://www.anaconda.com/blog/anaconda-aws-graviton2</td></tr>
<tr><td>Database</td><td><a href="https://aerospike.com/">Aerospike</a></td><td><a href="https://aerospike.com/products/database/">Aerospike Database</a></td><td>Release: <a href="https://aerospike.com/news/press-release/announcing-aerospike-on-graviton3/">Aerospike Showcases New Levels of Database Performance, Price, and Sustainability on Graviton3 at AWS re:Invent</a></td></tr>
<tr><td>Database</td><td><a href="https://keydb.dev/">EQ Alpha</a></td><td><a href="https://keydb.dev/keydb-pro.html">KeyDB Professional</a></td><td>Release: https://keydb.dev/downloads.html<br>Announcement: https://docs.keydb.dev/blog/2020/03/02/blog-post/</td></tr>
<tr><td>Database</td><td><a href="https://partners.amazonaws.com/partners/001E000001A1vNOIAZ">Elastic</a></td><td><a href="https://www.elastic.co/elasticsearch/">Elasticsearch</a></td><td>Release: https://www.elastic.co/downloads/elasticsearch<br>Announcement: https://www.elastic.co/blog/elasticsearch-on-arm</td></tr>
<tr><td>Database</td><td><a href="https://partners.amazonaws.com/partners/001E000001d9pndIAA">Heimdall Data</a></td><td><a href="https://www.heimdalldata.com/overview/">Heimdall Proxy Enterprise Edition</a></td><td>Release: https://aws.amazon.com/marketplace/pp/B08CL15V3N<br>Announcement: https://aws.amazon.com/marketplace/pp/B08CL15V3N</td></tr>
<tr><td>Database</td><td><a href="https://partners.amazonaws.com/partners/001E000000U0VKNIA3">MongoDB</a></td><td><a href="https://www.mongodb.com/">Server (Community &amp; Enterprise)</a></td><td>Release: https://www.mongodb.com/try/download<br>Announcement: https://docs.mongodb.com/manual/installation/</td></tr>
<tr><td>Database</td><td><a href="https://www.oracle.com/">Oracle</a></td><td><a href="https://www.oracle.com/">Oracle Instant Client</a></td><td>Release: https://www.oracle.com/database/technologies/instant-client/linux-arm-aarch64-downloads.html<br>Announcement: https://blogs.oracle.com/opal/post/oracle-instant-client-for-linux-arm-aarch64</td></tr>
<tr><td>Database</td><td><a href="https://arangodb.com/">ArangoDB</a></td><td><a href="https://arangodb.com/arangograph-selfmanaged/">ArangoDB Enterprise</a></td><td>Release: https://arangodb.com/download-arangodb-enterprise/<br>Announcement: https://arangodb.com/download-arangodb-enterprise/install-enterprise/docker-3-12/</td></tr>
<tr><td>Database</td><td>Couchbase</td><td>Enterprise Edition Server on Amazon Linux 2 - Graviton</td><td><a href="https://aws.amazon.com/marketplace/pp/prodview-fozt4ivft2tsa">AWS Marketplace</a></td></tr>
<tr><td>Database</td><td>Couchbase</td><td>Enterprise Server on Amazon Linux 2 - Graviton (Gold Support)</td><td><a href="https://aws.amazon.com/marketplace/pp/prodview-fozt4ivft2tsa">AWS Marketplace</a></td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000001EsVSNIA3">Buildkite</a></td><td><a href="https://buildkite.com/docs/agent">Agent</a></td><td>Release: https://github.com/buildkite/agent/releases<br>Announcement: https://buildkite.com/blog/buildkites-transition-to-arm</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000001EsVSNIA3">Buildkite</a></td><td><a href="https://buildkite.com/docs/tutorials/elastic-ci-stack-aws">Elastic CI Stack for AWS</a></td><td>Release: https://buildkite.com/changelog/115-aws-elastic-stack-support-for-arm-instances-rocket<br>Announcement: https://buildkite.com/blog/buildkites-transition-to-arm</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000000wLx02IAC">CircleCI</a></td><td><a href="https://circleci.com/">CircleCI</a></td><td>Release: https://circleci.com/docs/2.0/arm-resources/<br>Announcement: https://www.businesswire.com/news/home/20210330005259/en/CircleCI-Extends-Arm-Support-to-its-CICD-Cloud-Offering</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000000wLx02IAC">CircleCI</a></td><td><a href="https://circleci.com/build-environments/runner/">Self-hosted Runner</a></td><td>Release: https://circleci.com/docs/2.0/runner-installation/#supported-platforms<br>Announcement: https://circleci.com/blog/self-hosted-runners-add-arm-support-to-circleci/</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000000UfaS2IAJ">CloudLinux</a></td><td><a href="https://www.kernelcare.com/">KernelCare</a></td><td>Release: https://aws.amazon.com/marketplace/pp/Cloud-Linux-KernelCare/B085ZLFK7B<br>Announcement: https://aws.amazon.com/blogs/apn/live-patching-linux-without-downtime-on-aws-graviton2-instances/</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000000sVvXiIAK">GitHub</a></td><td><a href="https://docs.github.com/en/actions/hosting-your-own-runners">GitHub Actions Self-hosted Runner</a></td><td>Release: https://github.com/actions/runner/releases<br>Announcement: https://github.blog/changelog/2019-12-03-github-actions-self-hosted-runners-on-arm-architectures/</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E0000018YWFfIAO">GitLab</a></td><td><a href="https://docs.gitlab.com/runner/">GitLab Runner</a></td><td>Release: https://gitlab-runner-downloads.s3.amazonaws.com/latest/index.html<br>Announcement: https://about.gitlab.com/blog/2020/05/15/gitlab-arm-aws-graviton2-solution/</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/0010h00001d4srXAAQ">Granulate</a></td><td><a href="https://granulate.io/">gAgent</a></td><td>Release: https://docs.granulate.io/gagent/the-gagent<br>Announcement: https://granulate.io/granulate-aws-graviton-computing-optimization/</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/0010L00001v2cHWQAY">Harness (Drone.io)</a></td><td><a href="https://docs.drone.io/runner/overview/">Drone (Runner &amp; Server)</a></td><td>Release: https://docs.drone.io/runner/docker/installation/linux/<br>Announcement: https://blog.drone.io/drone-announces-official-support-for-arm/</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000001BxMyKIAV">HashiCorp</a></td><td><a href="https://www.hashicorp.com/products/terraform">Terraform</a></td><td>Release: https://www.terraform.io/downloads.html<br>Announcement: https://www.hashicorp.com/blog/announcing-hashicorp-terraform-0-14-beta</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000001BxMyKIAV">HashiCorp</a></td><td><a href="https://www.hashicorp.com/products/vault">Vault</a></td><td>Release: https://www.vaultproject.io/downloads<br>Announcement: https://github.com/hashicorp/vault/milestone/17?closed=1</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rp5GkIAJ">Progress (Chef)</a></td><td><a href="https://www.chef.io/products/chef-infra">Chef Infra Client</a></td><td>Release: https://downloads.chef.io/products/infra-client<br>Announcement: https://www.chef.io/press-release/chef-infra-16-empowers-experts-and-simplifies-adoption-for-new-users</td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl12xIAB">Puppet</a></td><td><a href="https://puppet.com/products/puppet-enterprise/">Puppet Enterprise: Agent</a></td><td>Release: https://puppet.com/docs/pe/latest/supported_operating_systems.html<br>Announcement: https://puppet.com/blog/puppet-enterprise-adds-arm-high-sierra-support-ssl-curl-updates/</td></tr>
<tr><td>DevOps</td><td><a href="https://goteleport.com/">Teleport (Gravitational)</a></td><td><a href="https://goteleport.com/teleport/">Teleport Unified Access Plane</a></td><td>Release: https://goteleport.com/teleport/download/<br>Announcement: https://goteleport.com/teleport/docs/changelog/#500</td></tr>
<tr><td>DevOps</td><td><a href="https://www.tetrate.io">Tetrate</a></td><td><a href="https://www.tetrate.io/tetrate-istio-subscription/">Tetrate Istio Subscription</a></td><td><a href="https://aws.amazon.com/marketplace/pp/prodview-r2kdhgz2mmocm">AWS Marketplace Subscription</a><br /><a href="https://www.tetrate.io/blog/tetrate-expands-aws-partnership-to-bring-enterprise-grade-istio-for-eks-and-eks-distro/">Announcement</a></td></tr>
<tr><td>DevOps</td><td><a href="https://partners.amazonaws.com/partners/001E0000019F9EUIA0">Travis CI</a></td><td><a href="https://travis-ci.com/">Travis CI (SaaS)</a></td><td>Release: "arch: arm64-graviton2" https://docs.travis-ci.com/user/multi-cpu-architectures/<br>Announcement: https://blog.travis-ci.com/2020-09-11-arm-on-aws</td></tr>
<tr><td>Development</td><td><a href="https://www.azul.com/">Azul</a></td><td><a href="https://www.azul.com/products/zulu-enterprise/">Zulu Enterprise OpenJDK</a></td><td>Release: https://www.azul.com/downloads/zulu/<br>Announcement: https://www.azul.com/products/zulu-system-specifications/</td></tr>
<tr><td>Developer Platform</td><td>Giant Swarm</td><td>Cloud Native Developer Platform SaaS</td><td><a href="https://aws.amazon.com/marketplace/pp/prodview-z7ul33v35nzam">AWS Marketplace</a></td></tr>
<tr><td>Developer Platform</td><td>Proxima Cloud</td><td>CRM: Next-Generation Pharma CRM</td><td><a href="https://proximacloud.com/">Proxima Cloud</a></td></tr>
<tr><td>Developer Platform</td><td>DevOps and Cloud Implementation by Armakuni</td><td>DevOps and Cloud Implementation</td><td><a href="https://www.armakuni.com/">Armakuni</a></td></tr>
<tr><td>Hardened Images</td><td>CIS</td><td>Hardened Image Level 1 on Amazon Linux 2 (ARM)</td><td><a href="https://www.cisecurity.org/cis-hardened-images">CIS</a></td></tr>
<tr><td>Hardened Images</td><td>CIS</td><td>Hardened Image Level 1 on Ubuntu Linux Server 22.04 LTS (ARM)</td><td><a href="https://www.cisecurity.org/cis-hardened-images">CIS</a></td></tr>
<tr><td>Hardened Images</td><td>CIS</td><td>Hardened Image Level 1 on Amazon Linux 2023 (ARM)</td><td><a href="https://www.cisecurity.org/cis-hardened-images">CIS</a></td></tr>
<tr><td>HPC</td><td><a href="https://partners.amazonaws.com/partners/001E0000015pgFUIAY/ANSYS,%20Inc..">Ansys</a></td><td><a href="https://www.ansys.com/products/fluids/ansys-fluent">Fluent</a><br><a href="https://www.ansys.com/products/structures/ansys-ls-dyna">LS-Dyna</a><br><a href="https://www.ansys.com/products/semiconductors/ansys-redhawk-sc">Redhawk-SC</a></td><td>Announcement: <a href="https://www.ansys.com/content/dam/amp/2023/june/quick-request/arm-64-processor-support-announcement-june-2023.pdf">https://www.ansys.com/content/dam/amp/2023/june/quick-request/arm-64-processor-support-announcement-june-2023.pdf)</a><br>Blog: <a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/">https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/</a></td></tr>
<tr><td>HPC</td><td><a href="https://partners.amazonaws.com/partners/001E000001JVfzMIAT">CFD Direct</a></td><td><a href="https://cfd.direct/cloud/aws/">CFD Direct From the Cloud</a></td><td>Release: https://aws.amazon.com/marketplace/pp/B08CHKT98H<br>Announcement: https://cfd.direct/cloud/cfddfc-arm-aws-c6g/</td></tr>
<tr><td>Mobile / Emulation</td><td>Genymotion</td><td>Cloud - Android 13.0 (arm64)</td><td><a href="https://www.genymotion.com/cloud/">Genymotion</a></td></tr>
<tr><td>Mobile / Emulation</td><td>Genymotion</td><td>Cloud - Android 14.0 (arm64)</td><td><a href="https://www.genymotion.com/cloud/">Genymotion</a></td></tr>
<tr><td>Networking</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl12PIAR">F5 (NGINX)</a></td><td><a href="https://www.nginx.com/products/nginx/">NGINX Plus</a></td><td>Release: https://aws.amazon.com/marketplace/pp/Nginx-Software-Inc-NGINX-Plus-Enterprise-Amazon-Li/B07RQN9ZQP<br>Announcement: https://www.prnewswire.com/news-releases/nginx-plus-now-available-for-latest-generation-of-arm-based-64-bit-servers-665653129.html</td></tr>
<tr><td>Networking</td><td><a href="https://partners.amazonaws.com/partners/0010L00001siBxVQAU/Kong%20Inc.">Kong (API GW)</a></td><td><a href="https://konghq.com/products/kong-gateway">Kong Gateway</a></td><td>Release: https://download.konghq.com/gateway-3.x-amazonlinux-2022/Packages/k/<br>Announcement: https://konghq.com/blog/engineering/validated-aws-graviton3-al2023</td></tr>
<tr><td>Networking</td><td>Heimdall</td><td>Proxy Enterprise Edition (ARM)</td><td><a href="https://www.heimdalldata.com/">Heimdall Data</a></td></tr>
<tr><td>Networking</td><td>Heimdall</td><td>Proxy Enterprise PLUS Edition (ARM)</td><td><a href="https://www.heimdalldata.com/">Heimdall Data</a></td></tr>
<tr><td>Networking</td><td>Netgate</td><td>pfSense Plus Firewall/VPN/Router (ARM64/Graviton)</td><td><a href="https://www.netgate.com/solutions/pfsense/">Netgate</a></td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl0vyIAB/AppDynamics">AppDynamics</a></td><td><a href="https://www.appdynamics.com/product/application-performance-monitoring/">AppDynamics APM Platform</a></td><td>Documentation: https://docs.appdynamics.com/appd/22.x/22.3/en/infrastructure-visibility/machine-agent/machine-agent-requirements-and-supported-environments</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/0010h00001dpaa1AAA/Cribl,%20Inc.">Cribl</a></td><td><a href="https://cribl.io/logstream/">Cribl LogStream</a></td><td>Release: https://cdn.cribl.io/dl/3.0.1/cribl-3.0.1-719ad848-linux-arm64.tgz<br>Announcement: https://cribl.io/blog/comparing-intel-amd-and-graviton2/</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rp57sIAB">Datadog</a></td><td><a href="https://docs.datadoghq.com/agent/">Datadog Agent</a></td><td>Release: https://hub.docker.com/r/datadog/agent<br>Announcement: https://www.datadoghq.com/blog/datadog-arm-agent/</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/001E000000texmiIAA">Dynatrace</a></td><td><a href="https://www.dynatrace.com/support/help/setup-and-configuration/dynatrace-oneagent/">OneAgent</a></td><td>Release: https://www.dynatrace.com/support/help/setup-and-configuration/dynatrace-oneagent/oneagent-technology-support/oneagent-platform-and-capability-support-matrix/<br>Announcement: https://www.dynatrace.com/news/blog/get-out-of-the-box-visibility-into-your-arm-platform-early-adopter/</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/0010h00001cBmQtAAK">Grafana Labs</a></td><td><a href="https://grafana.com/products/enterprise/">Grafana Enterprise</a></td><td>Release: https://grafana.com/grafana/download?edition=enterprise&amp;platform=arm<br>Announcement: https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v5-2/#native-builds-for-arm</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/0010L00001rElnvQAC">Honeycomb</a></td><td><a href="https://www.honeycomb.io/overview/">Honeycomb (SaaS, including Honeytail agent)</a></td><td>Release: https://www.honeycomb.io/pricing/<br>Announcement: Documentation of arm64 support for Honeytail agent:  https://docs.honeycomb.io/getting-data-in/integrations/honeytail/</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl12lIAB">New Relic</a></td><td><a href="https://newrelic.com/products/application-monitoring">APM agents</a></td><td>Release: https://docs.newrelic.com/docs/new-relic-one/install-configure/install-new-relic/#apm-install<br>Announcement: https://docs.newrelic.com/docs/apm/agents/manage-apm-agents/configuration/support-arm-graviton-x86-64</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl12lIAB">New Relic</a></td><td><a href="https://newrelic.com/products/infrastructure">Infrastructure Agent</a></td><td>Release: https://download.newrelic.com/infrastructure_agent/binaries/linux/arm64/<br>Announcement: https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl139IAB">Splunk</a></td><td><a href="https://www.splunk.com/en_us/software/splunk-enterprise.html">Enterprise: Universal Forwarder</a></td><td>Release: https://www.splunk.com/en_us/download/universal-forwarder.html<br>Announcement: https://docs.splunk.com/Documentation/Splunk/8.1.1/ReleaseNotes/MeetSplunk<br> Related Blog: https://aws.amazon.com/blogs/apn/monitoring-your-linux-workloads-on-aws-graviton-with-splunk-cloud/</td></tr>
<tr><td>Observability</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl0y6IAB">Sumo Logic</a></td><td><a href="https://help.sumologic.com/Beta/Sumo_Logic_Distribution_for_OpenTelemetry">Collector (OpenTelemetry)</a></td><td>Release: Beta: https://github.com/SumoLogic/sumologic-otel-collector/blob/main/docs/Installation.md#linux-on-arm64<br>Announcement: (https://support.sumologic.com/hc/en-us/articles/360034233293-Is-ARM64-architecture-supported-by-Sumo-Logic-collector-installer-)</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000001LiLQqIAN">Aqua Security</a></td><td><a href="https://www.aquasec.com/aqua-cloud-native-security-platform/">Aqua Cloud Native Security Platform</a></td><td>Release: https://www.aquasec.com/demo/<br>Announcement: https://blog.aquasec.com/aqua-support-aws-graviton</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000001LiLQqIAN">Aqua Security</a></td><td><a href="https://www.aquasec.com/products/open-source-projects/">Open Source (Trivy, kube-bench)</a></td><td>Release: https://gallery.ecr.aws/?architectures=ARM+64&amp;verified=verified&amp;searchTerm=aqua<br>Announcement: https://github.com/aquasecurity/trivy/releases/, https://github.com/aquasecurity/kube-bench/releases</td></tr>
<tr><td>Security</td><td><a href="https://aws.amazon.com/marketplace/seller-profile?id=277af30e-567f-4076-a7c2-8bb6ec439138">Bloombase</a></td><td><a href="https://www.bloombase.com/products/storesafe">Bloombase StoreSafe</a></td><td>Release: https://supportal.bloombase.com<br>Announcement: https://www.youtube.com/watch?v=WsOoR0zTgho</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/0010L00001iUuBxQAK">CIS</a></td><td><a href="https://www.cisecurity.org/cis-hardened-images/amazon/">CIS Hardened Images</a></td><td>Release: https://aws.amazon.com/marketplace/seller-profile?id=dfa1e6a8-0b7b-4d35-a59c-ce272caee4fc&amp;ref=dtl_B078SH1GP1<br>Announcement: https://www.cisecurity.org/blog/cis-hardened-vms-on-aws-graviton2-enhancing-ec2-security/</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000001VAPbPIAX">CrowdStrike</a></td><td><a href="https://www.crowdstrike.com/endpoint-security-products/">Falcon</a></td><td>Release: CrowdStrike FAQ listing Graviton/Linux support: https://www.crowdstrike.com/en-us/products/faq/#accordionfaq-en-us-item-2/<br>Announcement: https://www.crowdstrike.com/blog/join-crowdstrike-at-aws-reinvent-2019/</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000001V9UhGIAV">Lacework</a></td><td><a href="https://www.lacework.com/workload-security/">Workload Security Agent</a></td><td>Release: https://github.com/lacework/lacework-agent-releases/releases<br>Announcement: https://www.lacework.com/lacework-expands-security-visibility-and-automation-across-amazon-web-services/, https://support.lacework.com/hc/en-us/articles/360059603473-November-2020-Agent-Release, https://support.lacework.com/hc/en-us/articles/360005230014-Supported-Operating-Systems</td></tr>
<tr><td>Security</td><td><a href="https://www.oneidentity.com/">One Identity</a></td><td><a href="https://www.oneidentity.com/products/one-identity-safeguard-authentication-services/">Safeguard Authentication Services</a></td><td>Release: https://support.oneidentity.com/technical-documents/safeguard-authentication-services/5.0.1/installation-guide/3#TOPIC-1559368<br>Announcement: "AARCH64" https://support.oneidentity.com/technical-documents/safeguard-authentication-services/5.0.1/release-notes#TOPIC-1565570</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl0xgIAB">Qualys</a></td><td><a href="https://www.qualys.com/cloud-agent/">Cloud Agent</a></td><td>Release: https://www.qualys.com/docs/qualys-cloud-agent-linux-install-guide.pdf<br>Announcement: https://notifications.qualys.com/product/2020/05/26/qualys-adds-cloud-agent-linux-support-for-aws-arm-based-ec2-instances</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000000lZ7eMIAS">Rapid7</a></td><td><a href="https://www.rapid7.com/products/insightvm/">InsightVM</a></td><td>Release: https://aws.amazon.com/blogs/apn/how-to-deploy-a-rapid7-insightvm-scan-engine-for-aws-graviton2-based-ec2-instances/<br>Announcement: https://aws.amazon.com/blogs/apn/how-to-deploy-a-rapid7-insightvm-scan-engine-for-aws-graviton2-based-ec2-instances/</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/0010L00001tA9EiQAK">SentinelOne</a></td><td><a href="https://www.sentinelone.com/platform/">Singularity XDR</a></td><td>Release: https://www.sentinelone.com/platform/singularity-cloud/<br>Announcement: https://www.sentinelone.com/blog/edr-for-cloud-workloads-running-on-aws-graviton/</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/0010L00001kXlYOQA0">Snyk</a></td><td><a href="https://snyk.io/product/container-vulnerability-management/">Snyk Container</a></td><td>Release: https://support.snyk.io/hc/en-us/articles/360003812538-Install-the-Snyk-CLI<br>Announcement: https://snyk.io/blog/scanning-arm-container-images-with-snyk/</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000000e1mVKIAY">Tenable</a></td><td><a href="https://www.tenable.com/products/nessus">Nessus (Scanner, Agent)</a></td><td>Release: https://www.tenable.com/downloads/nessus<br>Announcement: Graviton2 listed since Nessus 8.1.1: https://docs.tenable.com/generalrequirements/Content/NessusScannerSoftwareRequirements.htm</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000000jBMF3IAO/">Threat Stack</a></td><td><a href="https://www.threatstack.com/cloud-security-platform">Linux Agent</a></td><td>Release: https://www.threatstack.com/resources/arm-support-solution-brief<br>Announcement: https://www.threatstack.com/press-releases/arm-support-release</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000000Rl0yAIAR">Trend Micro</a></td><td><a href="https://www.trendmicro.com/en_us/business/products/hybrid-cloud/deep-security.html">Deep Security: Agent</a></td><td>Release: https://help.deepsecurity.trendmicro.com/software-long-term-support.html<br>Announcement: https://help.deepsecurity.trendmicro.com/20_0/on-premise/agent-compatibility.html, https://cloudone.trendmicro.com/docs/workload-security/supported-features-by-platform/#amazon-linux</td></tr>
<tr><td>Security</td><td><a href="https://partners.amazonaws.com/partners/001E000001hNx6dIAC/VMware,%20Inc">VMware</a></td><td><a href="https://www.vmware.com/products/carbon-black-cloud-endpoint.html">VMware Carbon Black (Linux Sensor)</a></td><td>Release: https://docs.vmware.com/en/VMware-Carbon-Black-Cloud/services/cbc-oer-linux-sensor.pdf  <br>Announcement: https://docs.vmware.com/en/VMware-Carbon-Black-Cloud/2.14.0/rn/vmware-carbon-black-cloud-linux-sensor-2140-release-notes/index.html</td></tr>
<tr><td>Security</td><td><a href="https://wazuh.com/">Wazuh</a></td><td><a href="https://wazuh.com/product/">Wazuh (Agent &amp; Manager)</a></td><td>Release: https://documentation.wazuh.com/4.0/installation-guide/packages-list.html<br>Announcement: https://github.com/wazuh/wazuh/blob/4.0/CHANGELOG.md</td></tr>
<tr><td>Security</td><td><a href="https://www.microsoft.com/">Microsoft</a></td><td><a href="https://www.microsoft.com/en-us/security/business/endpoint-security/microsoft-defender-endpoint">Defender for Endpoint</a></td><td>Announcement: https://techcommunity.microsoft.com/blog/microsoftdefenderatpblog/defender-for-endpoint-extends-support-to-arm-based-linux-servers/4364066</td></tr>
<tr><td>Security</td><td>Cribl</td><td>Cribl Stream (ARM64)</td><td><a href="https://aws.amazon.com/marketplace/pp/prodview-76rwg3pud2vze">AWS Marketplace</a></td></tr>
<tr><td>Security</td><td>Armis</td><td>Device Security Platform</td><td><a href="https://aws.amazon.com/marketplace/pp/prodview-zhbfuevcbnjfw">AWS Marketplace</a></td></tr>
<tr><td>Security</td><td>Armis</td><td>Centrix FEDRAMP Authorized</td><td><a href="https://www.armis.com/blog/armis-achieves-final-fedramp-authorization/">FedRAMP Announcement</a></td></tr>
<tr><td>Security</td><td>Armis</td><td>Centrix ViPR Pro</td><td><a href="https://www.cdw.com/product/armis-centrix-for-vipr-pro-license-50000-99999-assets/8100105">CDW Product Page</a></td></tr>
<tr><td>Security</td><td>Armis</td><td>Asset Management- 15000 to 24999 Assets- 1-8 Network Ports</td><td><a href="https://www.armis.com/">Armis</a></td></tr>
<tr><td>Security</td><td>Armis</td><td>with Professional Services Subscription</td><td><a href="https://www.armis.com/">Armis</a></td></tr>
<tr><td>Security</td><td>Check Point</td><td>Harmony SaaS</td><td><a href="https://www.checkpoint.com/products/harmony/">Check Point</a></td></tr>
<tr><td>Security</td><td>Check Point</td><td>Harmony Advanced &amp; DB Warden</td><td><a href="https://www.checkpoint.com/products/harmony/">Check Point</a></td></tr>
<tr><td>Security</td><td>Check Point</td><td>Harmony Endpoint</td><td><a href="https://www.checkpoint.com/products/harmony-endpoint/">Check Point</a></td></tr>
<tr><td>Security</td><td>Check Point</td><td>Harmony Email &amp; Collaboration (Avanan)</td><td><a href="https://www.checkpoint.com/products/harmony-email-collaboration/">Check Point</a></td></tr>
<tr><td>Security</td><td>Fortinet</td><td>FortiGate Next-Generation Firewall (ARM64/Graviton)</td><td><a href="https://www.fortinet.com/products/next-generation-firewall">Fortinet</a></td></tr>
<tr><td>Security</td><td>WafCharm</td><td>WafCharm for AWS Marketplace (Subscription)</td><td><a href="https://aws.amazon.com/marketplace/pp/prodview-4w2x4h4d7xq4i">AWS Marketplace</a></td></tr>
<tr><td>Security</td><td>ARMO</td><td>ARMO Platform</td><td><a href="https://www.armosec.io/platform/">ARMO Platform</a></td></tr>
<tr><td>Security</td><td>ArmorCode</td><td>ASPM, DevSecOps Orchestration</td><td><a href="https://www.armorcode.com/devsecops">ArmorCode</a></td></tr>
<tr><td>Compliance &amp; Security</td><td>stackArmor</td><td>ThreatAlert® ATO Accelerator for FedRAMP, FISMA, and CMMC</td><td><a href="https://www.stackarmor.com/">stackArmor</a></td></tr>
<tr><td>Storage</td><td><a href="https://min.io/">MinIO</a></td><td><a href="https://min.io/">MinIO</a></td><td>Release: https://min.io/download#/linux<br>Announcement: https://github.com/minio/minio/releases/tag/RELEASE.2020-06-12T00-06-19Z</td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="getting-started-with-hpc-on-graviton-instances"><a class="header" href="#getting-started-with-hpc-on-graviton-instances">Getting started with HPC on Graviton instances</a></h1>
<ul>
<li><a href="HPC/index.html#getting-started-with-hpc-on-graviton-instances">Getting started with HPC on Graviton instances</a>
<ul>
<li><a href="HPC/index.html#introduction">Introduction</a></li>
<li><a href="HPC/index.html#summary-of-the-recommended-configuration">Summary of the recommended configuration</a></li>
<li><a href="HPC/index.html#instructions-for-setting-up-the-hpc-cluster-for-best-performance">Instructions for setting up the HPC cluster for best performance</a>
<ul>
<li><a href="HPC/index.html#compilers">Compilers</a></li>
<li><a href="HPC/index.html#computation-libraries">Computation libraries</a></li>
<li><a href="HPC/index.html#efa-support">EFA support</a></li>
<li><a href="HPC/index.html#open-mpi">Open MPI</a></li>
</ul>
</li>
<li><a href="HPC/index.html#running-hpc-applications">Running HPC applications</a>
<ul>
<li><a href="HPC/index.html#hpc-packages">HPC packages</a></li>
<li><a href="HPC/index.html#wrf">WRF</a>
<ul>
<li><a href="HPC/index.html#build-wrf-45-with-acfl-on-graviton">Build WRF 4.5 with ACFL on Graviton</a></li>
<li><a href="HPC/index.html#setup-the-runtime-configuration-download-and-run-the-benchmark">Setup the runtime configuration, download and run the benchmark</a></li>
<li><a href="HPC/index.html#build-wps-45-with-acfl-on-graviton">Build WPS 4.5 with ACFL on Graviton</a></li>
</ul>
</li>
<li><a href="HPC/index.html#openfoam">OpenFOAM</a>
<ul>
<li><a href="HPC/index.html#install-and-build-openfoam-v2112-on-graviton-instances-with-acfl">Install and Build OpenFOAM v2112 on Graviton instances with ACfL</a></li>
<li><a href="HPC/index.html#setup-the-runtime-configuration-and-run-the-benchmark">Setup the runtime configuration and run the benchmark</a></li>
<li><a href="HPC/index.html#sample-output">Sample output</a></li>
</ul>
</li>
<li><a href="HPC/index.html#gromacs">Gromacs</a>
<ul>
<li><a href="HPC/index.html#build-gromacs-20224">Build Gromacs 2022.4</a></li>
<li><a href="HPC/index.html#run-the-benchmark">Run the benchmark</a></li>
<li><a href="HPC/index.html#sample-output-1">Sample output</a></li>
</ul>
</li>
<li><a href="HPC/index.html#code-saturne">Code Saturne</a>
<ul>
<li><a href="HPC/index.html#build-code-saturne-802">Build Code Saturne 8.0.2</a></li>
<li><a href="HPC/index.html#run-bench_f128_02-benchmark">Run the benchmark</a></li>
<li><a href="HPC/index.html#code-saturne-benchmark-sample-output">Sample output</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="HPC/setup-an-ec2-hpc-instance.html#compile-instructions-on-an-ec2">Compile instructions on an Ec2</a></li>
<li><a href="HPC/index.html#mpi-application-profiling">MPI application profiling</a>
<ul>
<li><a href="HPC/index.html#tau-performance-system">Tau Performance System</a></li>
<li><a href="HPC/index.html#appendix">Appendix</a>
<ul>
<li><a href="HPC/index.html#list-of-hpc-compilers-for-graviton">List of HPC compilers for Graviton</a></li>
<li><a href="HPC/index.html#common-hpc-applications-on-graviton">Common HPC Applications on Graviton</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="introduction"><a class="header" href="#introduction">Introduction</a></h2>
<p><a href="https://aws.amazon.com/blogs/aws/new-amazon-ec2-instance-types-in-the-works-c7gn-r7iz-and-hpc7g">C7gn/Hpc7g</a> instances are the latest additions to Graviton based EC2 instances, optimized for network and compute intensive High-Performance Computing (HPC) applications. This document is aimed to help HPC users get the optimal performance on Graviton instances. It covers the recommended compilers, libraries, and runtime configurations for building and running HPC applications. Along with the recommended software configuration, the document also provides example scripts to get started with 3 widely used open-source HPC applications: Weather Research and Forecasting (WRF), Open Source Field Operation And Manipulation (OpenFOAM) and Gromacs.</p>
<h2 id="summary-of-the-recommended-configuration"><a class="header" href="#summary-of-the-recommended-configuration">Summary of the recommended configuration</a></h2>
<p>Instance type: C7gn and Hpc7g (Graviton3E processor, max 200 Gbps network bandwidth, 2 GB RAM/vCPU)</p>
<p>Cluster manager: AWS ParallelCluster</p>
<ul>
<li>Base AMI: aws-parallelcluster-3.5.1-ubuntu-2004-lts-hvm-arm64</li>
<li>Operating System: Ubuntu 20.04 (The latest version supported by Parallel Cluster)</li>
<li>Linux Kernel: 5.15 &amp; later (for users intended to use custom AMIs)</li>
</ul>
<p>ENA driver: version 2.8.3 &amp; later (Enhanced networking)</p>
<p>EFA driver: version 1.23.0 &amp; later (<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html#efa-start-enable">docs.aws.amazon.coml#efa-start-enable</a>)</p>
<p>Compiler: Arm Compiler for Linux (ACfL) v23.04 &amp; later (<a href="HPC/index.html#list-of-hpc-compilers-for-graviton">see below for other compiler options</a>)</p>
<p>ArmPL: v23.04 &amp; later (included in the ACfL compiler)</p>
<p>MPI: Open MPI v4.1.4 &amp; later (the latest official release)</p>
<p>Storage: <a href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/getting-started.html">FSx for Lustre</a> for shared file system.  HPC instance types have limited EBS bandwidth, and using FSx for Lustre avoids a bottleneck at the headnode.</p>
<h2 id="instructions-for-setting-up-the-hpc-cluster-for-best-performance"><a class="header" href="#instructions-for-setting-up-the-hpc-cluster-for-best-performance">Instructions for setting up the HPC cluster for best performance</a></h2>
<p>We recommend using <a href="https://docs.aws.amazon.com/parallelcluster/latest/ug/what-is-aws-parallelcluster.html">AWS ParallelCluster</a> (previously known as <a href="http://cfncluster.readthedocs.io">CfnCluster</a>) to deploy and manage HPC clusters  on AWS EC2. AWS ParallelCluster 3.5.1 is a tool that can automatically set up the required compute resources, job scheduler, and shared filesystem commonly needed to run HPC applications. This section covers step-by-step instructions on how to set up or upgrade the tools and software packages to the recommended versions on a new ParallelCluster. Please refer to the individual sub-sections if you need to update certain software package on an existing cluster. For a new cluster setup, you can use <a href="HPC/scripts-setup/hpc7g-ubuntu2004-useast1.yaml">this template</a> and replace the subnet, S3 bucket name for <a href="HPC/scripts-setup/install-gcc-11.sh">custom action script</a>, and ssh key information from your account to create a Ubuntu 20.04 cluster. The command to create a new cluster is</p>
<pre><code>pcluster create-cluster --cluster-name test-cluster --cluster-configuration hpc7g-ubuntu2004-useast1.yaml
</code></pre>
<p>The cluster creation process takes about 10 minutes. You can find headNode information under the EC2 console page once the creation process is finished (see the image below). In the case that you have multiple headNodes under the account, you can go to instance summary and check <code>Instance profile arn</code> attribute to find out which one has a prefix matching the cluster-name you created.</p>
<p><img src="HPC/images/headNode-info-ec2console.png" alt="" /></p>
<p>Alternatively, you can also use <code>pcluster describe-cluster --cluster-name test-cluster</code> to find the instanceId of the headNode and <code>aws ec2 describe-instances --instance-ids &lt;instanceId&gt;</code> to find the public Ip.</p>
<pre><code>{
  "creationTime": "2023-04-19T12:56:19.079Z",
  "headNode": {
    "launchTime": "2023-05-09T14:17:39.000Z",
    "instanceId": "i-01489594da7c76f77",
    "publicIpAddress": "3.227.12.112",
    "instanceType": "c7g.4xlarge",
    "state": "running",
    "privateIpAddress": "10.0.1.55"
  },
  "version": "3.5.1",
  ...
}
</code></pre>
<p>You can log in to the headNode in the same way as a regular EC2 instance. Run the <a href="HPC/scripts-setup/install-tools-headnode-ubuntu2004.sh">setup script</a> with command <code>./scripts-setup/install-tools-headnode-ubuntu2004.sh</code> to install the required tools and packages (ACfL and Open MPI) on the shared storage, <code>/shared</code>.</p>
<h3 id="compilers-1"><a class="header" href="#compilers-1">Compilers</a></h3>
<p>Many HPC applications depend on compiler optimizations for better performance. We recommend using <a href="https://developer.arm.com/Tools%20and%20Software/Arm%20Compiler%20for%20Linux">Arm Compiler for Linux (ACfL)</a> because it is tailored for HPC codes and comes with Arm Performance Libraries (ArmPL), which includes optimized BLAS, LAPACK, FFT and math libraries. Follow the below instructions to install and use ACfL 23.04 (latest version as of Apr 2023) or run the installation script with command <code>./scripts-setup/0-install-acfl.sh</code>.</p>
<pre><code># Install environment modules
sudo apt install environment-modules
source /etc/profile.d/modules.sh

# Find the download link to Arm compiler for your OS on https://developer.arm.com/downloads/-/arm-compiler-for-linux
mkdir -p /shared/tools &amp;&amp; cd /shared/tools
wget -O arm-compiler-for-linux_23.04_Ubuntu-20.04_aarch64.tar &lt;link to the tar ball&gt;
tar xf arm-compiler-for-linux_23.04_Ubuntu-20.04_aarch64.tar

sudo ./arm-compiler-for-linux_23.04_Ubuntu-20.04/arm-compiler-for-linux_23.04_Ubuntu-20.04.sh \
-i /shared/arm -a --force

# load the module to use Arm Compiler for Linux (ACfL)
module use /shared/arm/modulefiles
module load acfl
</code></pre>
<p>You will see the following message if ACfL installation is successful.</p>
<pre><code>Unpacking...
Installing...The installed packages contain modulefiles under /shared/arm/modulefiles
You can add these to your environment by running:
                $ module use /shared/arm/modulefiles
Alternatively:  $ export MODULEPATH=$MODULEPATH:/shared/arm/modulefiles
</code></pre>
<p>Please refer to <a href="HPC/index.html#list-of-hpc-compilers-for-graviton">Appendix</a> for a partial list of other HPC compilers with Graviton support.</p>
<h3 id="computation-libraries"><a class="header" href="#computation-libraries">Computation libraries</a></h3>
<p>Using highly optimized linear algebra and FFT libraries can significantly speed-up the computation for certain HPC applications. We recommend <a href="https://developer.arm.com/documentation/102574/0100">Arm Performance Libraries (ArmPL)</a> because it offers a vectorized math library (libamath), BLAS, LAPACK, and FFT libraries with better performance compared to other implementations like OpenBLAS or FFTW. ArmPL can be used with the <code>-armpl</code> flag for ACfL; ArmPL can also be use with other compilers, for example GCC, by adding compilation options:  <code>-I${ARMPL_INCLUDES} -L${ARMPL_LIBRARIES} -larmpl</code>.</p>
<p>ACfL includes the ArmPL packages as well. If you wish to just install the ArmPL, follow the below steps or use script with command <code>./scripts-setup/1-install-armpl.sh</code>.</p>
<pre><code># Find the download link to ArmPL (Ubuntu 20.04, GCC-12) on https://developer.arm.com/downloads/-/arm-performance-libraries
mkdir -p /shared/tools &amp;&amp; cd /shared/tools
wget -O arm-performance-libraries_23.04_Ubuntu-20.04_gcc-10.2.tar &lt;link to ArmPL.tar&gt;
tar xf arm-performance-libraries_23.04_Ubuntu-20.04_gcc-10.2.tar
cd arm-performance-libraries_23.04_Ubuntu-20.04/
./arm-performance-libraries_23.04_Ubuntu-20.04.sh -i /shared/arm -a --force
</code></pre>
<p>You will see the following message if the installation is successful.</p>
<pre><code>Unpacking...
Installing...The installed packages contain modulefiles under /shared/arm/modulefiles
You can add these to your environment by running:
                $ module use /shared/arm/modulefiles
Alternatively:  $ export MODULEPATH=$MODULEPATH:/shared/arm/modulefiles
</code></pre>
<h3 id="efa-support"><a class="header" href="#efa-support">EFA support</a></h3>
<p>C7gn/Hpc7g instances come with an EFA (Elastic Fabric Adapter) interface for low latency node to node communication that offers a peak bandwidth of 200Gbps. Getting the correct EFA driver is crucial for the performance of network intensive HPC applications.  AWS parallel cluster 3.5.1 comes with the latest EFA driver, that supports the EFA interface on C7gn and Hpc7g. If you prefer to stay with an existing cluster generated by earlier versions of AWS ParallelCluster, please follow the steps below to check the EFA driver version and <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa-start.html#efa-start-enable">upgrade the driver</a> if necessary.</p>
<pre><code># ssh into a compute instance after it is configured
fi_info -p efa

# Output on instances without the proper EFA driver
fi_getinfo: -61

# Output on instances with the proper EFA driver
provider: efa
    fabric: EFA-fe80::94:3dff:fe89:1b70
    domain: efa_0-rdm
    version: 2.0
    type: FI_EP_RDM
    protocol: FI_PROTO_EFA
</code></pre>
<h3 id="open-mpi"><a class="header" href="#open-mpi">Open MPI</a></h3>
<p>For applications that use the Message Passing Interface (MPI) to communicate, we recommend using Open MPI v4.1.4 or later for Graviton Instances. AWS Parallel cluster 3.5.1 provides the Open MPI libraries built with default GCC. For best performance, it is recommended to re-compile them with ACfL 23.04 or GCC-11 and later version. The following snippet provides instructions on how to build Open MPI 4.1.4 with ACfL 23.04 or use the script with command <code>./scripts-setup/2a-install-openmpi-with-acfl.sh</code>.</p>
<pre><code># compile Open MPI with ACfL
export INSTALLDIR=/shared
export OPENMPI_VERSION=4.1.4
module use /shared/arm/modulefiles
module load acfl
export CC=armclang
export CXX=armclang++
export FC=armflang
export CFLAGS="-mcpu=neoverse-512tvb"

# assuming the efa driver is installed at the default directory /opt/amazon/efa
cd /shared/tools
wget -N https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.4.tar.gz
tar -xzvf openmpi-4.1.4.tar.gz
cd openmpi-4.1.4
mkdir build-acfl
cd build-acfl
../configure --prefix=${INSTALLDIR}/openmpi-${OPENMPI_VERSION}-acfl --enable-mpirun-prefix-by-default --without-verbs --disable-man-pages --enable-builtin-atomics --with-libfabric=/opt/amazon/efa  --with-libfabric-libdir=/opt/amazon/efa/lib
make -j$(nproc) &amp;&amp; make install
</code></pre>
<p>To check if the Open MPI build with ACfL,</p>
<pre><code>export PATH=/shared/openmpi-4.1.4-acfl/bin:$PATH
export LD_LIBRARY_PATH=/shared/openmpi-4.1.4-acfl/lib:$LD_LIBRARY_PATH
mpicc --version
</code></pre>
<p>You will get the following message if the build is successful</p>
<pre><code>Arm C/C++/Fortran Compiler version 23.04 (build number 21) (based on LLVM 16.0.0)
Target: aarch64-unknown-linux-gnu
Thread model: posix
InstalledDir: /shared/arm/arm-linux-compiler-23.04_Ubuntu-20.04/bin
</code></pre>
<h3 id="storage"><a class="header" href="#storage">Storage</a></h3>
<p>Some HPC applications require significant amounts of file I/O, however HPC instance types (Graviton instances included) don't have local storage, and have limited EBS bandwidth and IOPS.  Relying on EBS on each node can cause surprise slow-downs when the instance runs out of EBS burst credits.  This is one reason we don't recommend using an Hpc7g (or any HPC instance type) for headnodes, since the headnode performs additional I/O as the scheduler, and often serves a home directory to the compute nodes.  For these reasons the following recommendations are made:</p>
<ul>
<li>Use FSx for Lustre to serve data and configuration files to compute nodes.  FSx for Lustre file systems can be configured in a variety of sizes and throughputs to meet your specific needs.  See the SharedStorage section in the example <a href="HPC/scripts-setup/hpc7g-ubuntu2004-useast1.yaml">cluster configuration</a>.</li>
<li>Headnodes should be compute-optimized instances (such as C7gn or C7g), and sized with both compute needs and EBS/networking needs in mind.</li>
</ul>
<h2 id="running-hpc-applications"><a class="header" href="#running-hpc-applications">Running HPC applications</a></h2>
<p>Once the HPC cluster is setup following the above steps, you can run the following sample HPC applications on Graviton and check their performance. If there are any challenges in running these sample applications on Graviton instances, please raise an issue on <a href="https://github.com/aws/aws-graviton-getting-started">aws-graviton-getting-started</a> github page.</p>
<h3 id="hpc-packages"><a class="header" href="#hpc-packages">HPC packages</a></h3>
<div class="table-wrapper"><table><thead><tr><th>Package</th><th>Version</th><th>Build options</th><th>Run time configurations</th></tr></thead><tbody>
<tr><td>WRF (Weather Research &amp; Forecasting)</td><td>v4.5+</td><td>ACfL</td><td>8 CPUs per rank</td></tr>
<tr><td>OpenFOAM (Computational Fluid Dynamics simulation)</td><td>v2112+</td><td>ACfL</td><td>1 CPU per rank</td></tr>
<tr><td>Gromacs (Molecular Dynamics simulation)</td><td>v2022.4+</td><td>ACfL with SVE_SIMD option</td><td>1 CPU per rank</td></tr>
</tbody></table>
</div>
<h3 id="wrf"><a class="header" href="#wrf">WRF</a></h3>
<p>The WRF model is one of the most used numerical weather prediction (NWP) systems. WRF is used extensively for research and real-time forecasting. Large amount of computation resources are required for each simulation, especially for high resolution simulations. We recommend using <a href="https://github.com/wrf-model/WRF/releases#wrf-version-4.5">WRF 4.5</a>.</p>
<p>The WRF Pre-Processing System (WPS) preapres a domain (region of the Earth) for input to WRF. We recommend using <a href="https://github.com/wrf-model/WPS/releases/tag/v4.5">WPS 4.5</a>.</p>
<h4 id="build-wrf-45-with-acfl-on-graviton"><a class="header" href="#build-wrf-45-with-acfl-on-graviton">Build WRF 4.5 with ACFL on Graviton</a></h4>
<p>Use <a href="HPC/scripts-wrf/install-wrf-tools-acfl.sh">this script</a> with command <code>./scripts-wrf/install-wrf-tools-acfl.sh</code> to install the required tools: zlib, hdf5, pnetcdf, netcdf-c, and netcdf-fortran. Or use <a href="HPC/scripts-wrf">these scripts</a> in the numeric order to install the tools sequentially. You will get <a href="HPC/scripts-wrf/pnetcdf-success-message.txt">this message</a> if pnetcdf installation is successful; <a href="HPC/scripts-wrf/netcdf-c-success-message.txt">this message</a> if netcdf-c installation is successful; <a href="HPC/scripts-wrf/netcdf-fortran-success-message.txt">this message</a> if netcdf-fortran installation is successful.
Use <a href="HPC/scripts-wrf/compile-wrf-v45-acfl.sh">this script</a> with command <code>./scripts-wrf/compile-wrf-v45-acfl.sh</code> to configure and compile WRF.</p>
<pre><code># get WRF source v45
git clone https://github.com/wrf-model/WRF.git
cd WRF &amp;&amp; git checkout release-v4.5

# apply a patch that includes ACfL compiler options
wget https://raw.githubusercontent.com/aws/aws-graviton-getting-started/main/HPC/scripts-wrf/WRF-v45-patch-acfl.diff
git apply WRF-v45-patch-acfl.diff

# choose option '12. (dm+sm)   armclang (armflang/armclang): Aarch64' and '1=basic'
./configure
sed -i 's/(WRF_NMM_CORE)$/(WRF_NMM_CORE)  -Wno-error=implicit-function-declaration -Wno-error=implicit-int/g'  configure.wrf
./compile -j 1 em_real 2&gt;&amp;1 | tee compile_wrf.out
</code></pre>
<p>You will get the following message if the WRF build is successful.</p>
<pre><code>==========================================================================
build started:   Fri May 12 17:32:14 UTC 2023
build completed: Fri May 12 18:10:12 UTC 2023

---&gt;                  Executables successfully built                  &lt;---

-rwxrwxr-x 1 ubuntu ubuntu 47804664 May 12 18:10 main/ndown.exe
-rwxrwxr-x 1 ubuntu ubuntu 47553704 May 12 18:10 main/real.exe
-rwxrwxr-x 1 ubuntu ubuntu 47167056 May 12 18:10 main/tc.exe
-rwxrwxr-x 1 ubuntu ubuntu 52189632 May 12 18:09 main/wrf.exe

==========================================================================
</code></pre>
<h4 id="setup-the-runtime-configuration-download-and-run-the-benchmark"><a class="header" href="#setup-the-runtime-configuration-download-and-run-the-benchmark">Setup the runtime configuration, download and run the benchmark</a></h4>
<p>WRF uses shared memory and distributed memory programming model. It is recommended to use 8 threads per rank and setting threads affinity to be "compact" to reduce communication overhead and achieve better performance. The following is <a href="HPC/scripts-wrf/sbatch-wrf-v45-acfl.sh">an example Slurm script</a> that will download the WRF CONUS 12km model and run on a single Hpc7g instance with 8 ranks and 8 threads for each rank. You can submit the Slurm job by running this command <code>sbatch sbatch-wrf-v45-acfl.sh</code>. At the end of the WRF log file from rank 0 (rsl.error.0000), you will see the following message if the job completes successfully.</p>
<pre><code>Timing for main: time 2019-11-26_23:58:48 on domain   1:    0.46453 elapsed seconds
Timing for main: time 2019-11-27_00:00:00 on domain   1:    0.46581 elapsed seconds
 mediation_integrate.G         1242 DATASET=HISTORY
 mediation_integrate.G         1243  grid%id             1  grid%oid
            2
Timing for Writing wrfout_d01_2019-11-27_00:00:00 for domain        1:    0.97232 elapsed seconds
wrf: SUCCESS COMPLETE WRF
</code></pre>
<p>You can view WRF output model using <a href="https://aws.amazon.com/hpc/dcv/">Nice DCV</a> and <a href="http://meteora.ucsd.edu/~pierce/ncview_home_page.html">Ncview</a>. Typically the elapsed time spent on the computing steps is used to measure the performance of the WRF simulation on a system.</p>
<pre><code>num_compute_time_steps=$( grep "Timing for main" rsl.error.0000 | awk 'NR&gt;1' | wc -l )
time_compute_steps=$( grep "Timing for main" rsl.error.0000 | awk 'NR&gt;1' | awk '{ sum_comp += $9} END { print sum_comp }' )
echo $time_compute_steps
</code></pre>
<h4 id="build-wps-45-with-acfl-on-graviton"><a class="header" href="#build-wps-45-with-acfl-on-graviton">Build WPS 4.5 with ACFL on Graviton</a></h4>
<p>After compiling WRF 4.5, use <a href="HPC/scripts-wrf/scripts-wps/0-install-jasper.sh">this script</a> with command <code>./scripts-wps/0-install-jasper.sh</code> to install the required tools, jasper. Then, use <a href="HPC/scripts-wps/compile-wps.sh">this script</a> with command <code>./scripts-wps/compile-wps.sh</code> to configure and compile WPS.</p>
<pre><code># get WPS source 4.5
wget https://github.com/wrf-model/WPS/archive/refs/tags/v4.5.tar.gz
tar xf v4.5.tar.gz
cd WPS-4.5

# apply a patch that includes ACfL compiler options
cat &gt;&gt; arch/configure.defaults &lt;&lt; EOL
########################################################################################################################
#ARCH Linux aarch64, Arm compiler OpenMPI # serial smpar dmpar dm+sm
#
COMPRESSION_LIBS    = CONFIGURE_COMP_L
COMPRESSION_INC     = CONFIGURE_COMP_I
FDEFS               = CONFIGURE_FDEFS
SFC                 = armflang
SCC                 = armclang
DM_FC               = mpif90
DM_CC               = mpicc -DMPI2_SUPPORT
FC                  = CONFIGURE_FC
CC                  = CONFIGURE_CC
LD                  = $(FC)
FFLAGS              = -ffree-form -O -fconvert=big-endian -frecord-marker=4 -ffixed-line-length-0 -Wno-error=implicit-function-declaration -Wno-error=implicit-int -Wno-error=incompatible-function-pointer-types
F77FLAGS            = -ffixed-form -O -fconvert=big-endian -frecord-marker=4 -ffree-line-length-0 -Wno-error=implicit-function-declaration -Wno-error=implicit-int -Wno-error=incompatible-function-pointer-types
FCSUFFIX            =
FNGFLAGS            = $(FFLAGS)
LDFLAGS             =
CFLAGS              = -Wno-error=implicit-function-declaration -Wno-error=implicit-int -Wno-error=incompatible-function-pointer-types
CPP                 = /usr/bin/cpp -P -traditional
CPPFLAGS            = -D_UNDERSCORE -DBYTESWAP -DLINUX -DIO_NETCDF -DBIT32 -DNO_SIGNAL CONFIGURE_MPI
RANLIB              = ranlib
EOL

# configure (with option 2), and compile
./configure &lt;&lt;&lt; 2
sed -i 's/-lnetcdf/-lnetcdf -lnetcdff -lgomp /g' configure.wps
./compile | tee compile_wps.log
</code></pre>
<p>You will see the geogrid.exe, metgrid.exe, and ungrib.exe files in your directory if the WPS build is successful.</p>
<h3 id="openfoam"><a class="header" href="#openfoam">OpenFOAM</a></h3>
<p>OpenFOAM is a free, open-source CFD software released and developed by OpenCFD Ltd since 2004. OpenFOAM has a large user base and is used for finite element analysis (FEA) in a wide variety of industries, including aerospace, automotive, chemical manufacturing, petroleum exploration, etc.</p>
<h4 id="install-and-build-openfoam-v2112-on-graviton-instances-with-acfl"><a class="header" href="#install-and-build-openfoam-v2112-on-graviton-instances-with-acfl">Install and Build OpenFOAM v2112 on Graviton instances with ACfL</a></h4>
<p>Use <a href="HPC/scripts-openfoam/compile-openfoam-acfl.sh">this script</a> with command <code>./scripts-openfoam/compile-openfoam-acfl.sh</code> to compile OpenFOAM with ACfL.</p>
<pre><code>mkdir -p /shared/tools/openfoam-root &amp;&amp; cd /shared/tools/openfoam-root
export PATH=/shared/openmpi-4.1.4-acfl/bin:$PATH
export LD_LIBRARY_PATH=/shared/openmpi-4.1.4-acfl/lib:$LD_LIBRARY_PATH
module use /shared/arm/modulefiles 
module load acfl armpl

[ -d openfoam ] || git clone -b OpenFOAM-v2112 https://develop.openfoam.com/Development/openfoam.git
[ -d ThirdParty-common ] || git clone -b v2112 https://develop.openfoam.com/Development/ThirdParty-common.git

pushd ThirdParty-common
scotch_version="6.1.0"
git clone -b v${scotch_version} https://gitlab.inria.fr/scotch/scotch.git scotch_${scotch_version}
popd
cd openfoam

# a patch required for ACfL or GCC-12 (https://develop.openfoam.com/Development/openfoam/-/commit/91198eaf6a0c11b57446374d97a079ca95cf1412)
wget https://raw.githubusercontent.com/aws/aws-graviton-getting-started/main/HPC/scripts-openfoam/openfoam-v2112-patch.diff
git apply openfoam-v2112-patch.diff

sed -i -e "s/WM_COMPILER=Gcc/WM_COMPILER=Arm/g" etc/bashrc
source etc/bashrc || echo "Non-zero exit of source etc/bashrc"
./Allwmake -j 
</code></pre>
<p>You will see the following message if the installation is successful.</p>
<pre><code>========================================
Done OpenFOAM applications
========================================
========================================
prefix = /shared/tools/openfoam-root/openfoam/platforms/linuxARM64ArmDPInt32Opt

    ignoring possible compilation errors
    make certain to check the output file


2023-05-12 21:03:31 +0000
========================================
  openfoam
  Arm system compiler
  linuxARM64ArmDPInt32Opt, with SYSTEMOPENMPI sys-openmpi

  api   = 2112
  patch = 0
  bin   = 263 entries
  lib   = 120 entries

========================================
</code></pre>
<h4 id="setup-the-runtime-configuration-and-run-the-benchmark"><a class="header" href="#setup-the-runtime-configuration-and-run-the-benchmark">Setup the runtime configuration and run the benchmark</a></h4>
<p>Use <a href="HPC/scripts-openfoam/sbatch-openfoam-acfl.sh">this script</a> with command <code>sbatch ./sbatch-openfoam-acfl.sh</code> to set up the environment parameters, perform domain decomposition, generate meshes, and run the OpenFOAM motorBike 70M benchmark, included in OpenFOAM 2112 package, on a single instance with 64 ranks.</p>
<h4 id="sample-output"><a class="header" href="#sample-output">Sample output</a></h4>
<p>If the simulation has succeeded, you should see the final model statistics at the end of the log file, <code>/shared/data-openfoam/motorBike-70M/motorBike/log/simpleFoam.log</code>, like below. You can also use Paraview and Nice DCV to visualize the OpenFOAM output model.</p>
<pre><code>streamLine streamLines write:
    seeded 20 particles
    Tracks:20
    Total samples:18175
    Writing data to "/shared/data-openfoam/motorBike-70M/motorBike/postProcessing/sets/streamLines/500"
forceCoeffs forces execute:
    Coefficients
        Cd       : 0.438588     (pressure: 0.412171     viscous: 0.0264166)
        Cs       : 0.00672088   (pressure: 0.00631824   viscous: 0.000402645)
        Cl       : -0.0259146   (pressure: -0.0215873   viscous: -0.00432727)
        CmRoll       : 0.00360773       (pressure: 0.0034373    viscous: 0.000170428)
        CmPitch       : 0.228219        (pressure: 0.215858     viscous: 0.0123609)
        CmYaw       : 0.00165442        (pressure: 0.00162885   viscous: 2.55688e-05)
        Cd(f)    : 0.222901
        Cd(r)    : 0.215686
        Cs(f)    : 0.00501486
        Cs(r)    : 0.00170602
        Cl(f)    : 0.215262
        Cl(r)    : -0.241177
End

Finalising parallel run
</code></pre>
<h3 id="gromacs"><a class="header" href="#gromacs">Gromacs</a></h3>
<p>Gromacs is a widely used molecular dynamics software package. Gromacs is a computation heavy software, and can get better performance with the modern processors' SIMD (single instruction multiple data) capabilities. We recommend using Gromacs 2022.4 or later releases because they implement performance critical routines using the SVE instruction set found on Hpc7g/C7gn.</p>
<h4 id="build-gromacs-20224"><a class="header" href="#build-gromacs-20224">Build Gromacs 2022.4</a></h4>
<p>Use <a href="HPC/scripts-gromacs/compile-gromacs-acfl.sh">this script</a> with command <code>./scripts-gromacs/compile-gromacs-acfl.sh</code> to build Gromacs with ACfL</p>
<pre><code># note: Gromacs supports 3 different programming interfaces for FFT:
# "fftw3", "mkl" and "fftpack". The ArmPL FFT library has the same 
# programming interface as FFTW, so, setting "-DGMX_FFT_LIBRARY=fftw3" and 
# "-DFFTWF_LIBRARY=${ARMPL_LIBRARIES}/libarmpl_lp64.so" enables the 
# ArmPL FFT library for Gromacs.
cmake .. -DGMX_BUILD_OWN_FFTW=OFF \
-DREGRESSIONTEST_DOWNLOAD=ON \
-DCMAKE_C_FLAGS="-mcpu=neoverse-512tvb —param=aarch64-autovec-preference=4 -g" \
-DCMAKE_CXX_FLAGS="-mcpu=neoverse-512tvb —param=aarch64-autovec-preference=4 -g" \
-DCMAKE_C_COMPILER=$(which mpicc) \
-DCMAKE_CXX_COMPILER=$(which mpicxx) \
-DGMX_OMP=ON \
-DGMX_MPI=ON \
-DGMX_SIMD=ARM_SVE \
-DGMX_BUILD_MDRUN_ONLY=OFF \
-DGMX_DOUBLE=OFF \
-DCMAKE_INSTALL_PREFIX=${CURDIR} \
-DBUILD_SHARED_LIBS=OFF \
-DGMX_FFT_LIBRARY=fftw3 \
-DFFTWF_LIBRARY=${ARMPL_LIBRARIES}/libarmpl_lp64.so \
-DFFTWF_INCLUDE_DIR=${ARMPL_INCLUDES} \
\
-DGMX_BLAS_USER=${ARMPL_LIBRARIES}/libarmpl_lp64.so \
-DGMX_LAPACK_USER=${ARMPL_LIBRARIES}/libarmpl_lp64.so \
\
-DGMXAPI=OFF \
-DGMX_GPU=OFF

make
make install
</code></pre>
<p>You will see the following message if the installation is successful.</p>
<pre><code>-- Installing: /shared/gromacs-2022.4-acfl/bin/gmx_mpi
-- Up-to-date: /shared/gromacs-2022.4-acfl/bin
-- Installing: /shared/gromacs-2022.4-acfl/bin/gmx-completion.bash
-- Installing: /shared/gromacs-2022.4-acfl/bin/gmx-completion-gmx_mpi.bash
</code></pre>
<h4 id="run-the-benchmark"><a class="header" href="#run-the-benchmark">Run the benchmark</a></h4>
<p>To get the best performance for benchRIB, a benchmark from <a href="https://www.mpinat.mpg.de/grubmueller/bench">Max Planck Institute</a>, we recommend a single core for each rank and 64 ranks per instance. Below is <a href="HPC/scripts-gromacs/sbatch-gromacs-acfl.sh">an example Slurm script</a> for running Gromacs job on a single instance. You can start the Slurm job by <code>sbatch sbatch-gromacs-acfl.sh</code>.</p>
<h4 id="sample-output-1"><a class="header" href="#sample-output-1">Sample output</a></h4>
<p>At the end of benchRIB output log, <code>/shared/data-gromacs/benchRIB/benchRIB.log</code>, you can find a section showing the performance of the simulation. Below is an example of the output file on a single Hpc7g instance. The performance is measured by ns/day (higher is better), which means the number of nanoseconds of the system's dynamics that can be simulated in 1 day of computing.</p>
<pre><code>               Core t (s)   Wall t (s)        (%)
       Time:    17989.180      281.082     6400.0
                 (ns/day)    (hour/ns)
Performance:        6.149        3.903
Finished mdrun on rank 0 Fri May 12 22:18:17 2023
</code></pre>
<h3 id="code-saturne"><a class="header" href="#code-saturne">Code Saturne</a></h3>
<p>code_saturne is a general-purpose computational fluid dynamics free computer software package. Developed since 1997 at Électricité de France R&amp;D, code_saturne is distributed under the GNU GPL licence.</p>
<h4 id="build-code-saturne-802"><a class="header" href="#build-code-saturne-802">Build Code Saturne 8.0.2</a></h4>
<p>Use <a href="HPC/scripts-code_saturne/install-codesaturne-gcc-mpi4.sh">this script</a> with command <code>./scrpits-code_saturne/install-codesaturne-gcc-mpi4.sh</code> to build Code Saturne with GCC. The configuration below uses BLAS library from ArmPL. The default multi-grid solver is cs_sles_solve_native. Users can change the solver and solver settings (n_max_iter_coarse_solver, min_g_cells) by updating ./src/user/cs_user_parameters.c.  <a href="HPC/scripts-code_saturne/cs_user_parameters.c">This user parameters file example</a> shows a use case to use CS_SLES_P_SYM_GAUSS_SEIDEL solver for better solver performance.</p>
<pre><code>cd /shared/tools

module use /shared/arm/modulefiles
module load armpl
export PATH=/shared/openmpi-4.1.6/bin:$PATH
export LD_LIBRARY_PATH=/shared/openmpi-4.1.6/lib:$LD_LIBRARY_PATH
export CC=mpicc
export CXX=mpicxx
export FC=mpif90
export F77=mpif90
export F90=mpif90

if [ ! -d code_saturne-8.0.2 ]; then
    wget https://www.code-saturne.org/releases/code_saturne-8.0.2.tar.gz
    tar xf code_saturne-8.0.2.tar.gz
fi
cd code_saturne-8.0.2

PREFIX=/shared/code_saturne_8.0-mpi4
mkdir build-mpi4
cd build-mpi4

../configure CC=${CC} CXX=${CXX} FC=${FC} \
    --with-blas=$ARMPL_LIBRARIES --prefix=$PREFIX \
    --disable-gui --without-med \
    --without-hdf5 --without-cgns \
    --without-metis --disable-salome \
    --without-salome --without-eos \
    --disable-static --enable-long-gnum \
    --enable-profile

make -j
make install
</code></pre>
<h4 id="run-bench_f128_02-benchmark"><a class="header" href="#run-bench_f128_02-benchmark">Run BENCH_F128_02 benchmark</a></h4>
<p>The code_saturne benchmark data can be generated using the following procedures.</p>
<pre><code>mkdir -p /shared/data-codesaturne &amp;&amp; cd /shared/data-codesaturne
git clone https://github.com/code-saturne/saturne-open-cases.git

cd /shared/data-codesaturne/saturne-open-cases/BUNDLE/BENCH_F128_PREPROCESS/DATA
$PREFIX/bin/code_saturne run --initialize
cd /shared/data-codesaturne/saturne-open-cases/BUNDLE/BENCH_F128_PREPROCESS/RESU/extrude_128
./run_solver

cd /shared/data-codesaturne/saturne-open-cases/BUNDLE/BENCH_F128_02/DATA
$PREFIX/bin/code_saturne run --initialize
</code></pre>
<p>After that you can use <a href="HPC/scripts-code_saturne/submit-F128-2-hpc7g-gcc-mpi4.sh">the following slurm batch script</a> with command <code>sbatch scripts-code_saturne/submit-F128-2-hpc7g-gcc-mpi4.sh</code> to run the benchmark.</p>
<h4 id="code-saturne-benchmark-sample-output"><a class="header" href="#code-saturne-benchmark-sample-output">Code Saturne benchmark sample output</a></h4>
<p>At the end of the benchmark run, you will find <code>run_solver.log</code> and <code>performance.log</code> in the run directory. These logs contain the correctness and performance information of the run. You can find the Elapsed time for the job in <code>performance.log</code> and one of the sample can be found below.</p>
<pre><code>Calculation time summary:

  User CPU time:            294.198 s
  System CPU time:           13.001 s
  Total CPU time:         57958.255 s

  Elapsed time:             318.384 s
  CPU / elapsed time          0.965
</code></pre>
<h2 id="mpi-application-profiling"><a class="header" href="#mpi-application-profiling">MPI application profiling</a></h2>
<p>Ideally, as you add more resources, the runtime of HPC applications should reduce linearly. When scaling is sub-linear or worse, it is usually because of the non-optimal communication patterns. To debug these cases, open-source tools such as the <a href="http://www.cs.uoregon.edu/research/tau/home.php">Tau Performance System</a>, can generate profiling and tracing reports to help you locate the bottlenecks.</p>
<h3 id="tau-performance-system"><a class="header" href="#tau-performance-system">Tau Performance System</a></h3>
<p>Configure and build Tau as follows (shown here for an AWS EC2 instance launched in ParallelCluster setup):</p>
<pre><code>$ ./configure -prefix=/shared/TauOpenMPI \
  -mpi \
  -mpiinc=/opt/amazon/openmpi/include \
  -mpilib=/opt/amazon/openmpi/lib
</code></pre>
<p>After having built/installed the profiler, collect a profile by executing the command below:</p>
<pre><code>$ mpirun tau_exec mpiApplication &gt; ./output.log 2&gt;&amp;1
</code></pre>
<p>A successful collection of a Tau profile would cause the creation of <code>profile.*</code> files. You can visualize the results using <code>paraprof</code> or <code>pprof</code> utilities in Tau. Shown below is a summary profile using command <code>pprof -s</code>.</p>
<pre><code>FUNCTION SUMMARY (mean):
---------------------------------------------------------------------------------------
%Time    Exclusive    Inclusive       #Call      #Subrs  Inclusive Name
              msec   total msec                          usec/call 
---------------------------------------------------------------------------------------
100.0        0.556     2:11.067           1           1  131067754 .TAU application
100.0     1:09.130     2:11.067           1      478495  131067198 taupreload_main
 27.9       14,889       36,577      171820      171820        213 MPI_Allreduce() 
 16.8       22,037       22,037      172288           0        128 MPI Collective Sync 
  9.7       12,708       12,708       94456           0        135 MPI_Waitall() 
  2.8        3,624        3,624           1           0    3624935 MPI_Finalize() 
  2.7        3,518        3,518           1           0    3518172 MPI_Init_thread() 
  2.2        2,920        2,920     3597.37           0        812 MPI_Recv() 
  1.1        1,475        1,475     438.314           0       3367 MPI_Probe() 
</code></pre>
<h2 id="appendix-1"><a class="header" href="#appendix-1">Appendix</a></h2>
<h3 id="list-of-hpc-compilers-for-graviton"><a class="header" href="#list-of-hpc-compilers-for-graviton">List of HPC compilers for Graviton</a></h3>
<p>The table below has a list of HPC compilers and options that you can for Graviton instance:
Compiler | Minimum version	| Target: Graviton3 and up |	Enable OpenMP	| Fast Math
----------|---------|-------------------|-------------|--------
GCC	| 11	| -O3 -mcpu=neoverse-v1	| -fopenmp	| -ffast-math
CLang/LLVM |	14 | -O3 -mcpu=neoverse-512tvb	| -fopenmp	| -ffast-math
Arm Compiler for Linux |	23.04 |	-O3 -mcpu=neoverse-512tvb |	-fopenmp |	-ffast-math
Nvidia HPC SDK |	23.1	| -O3 -tp=neoverse-v1	| -mp |	-fast</p>
<h3 id="common-hpc-applications-on-graviton"><a class="header" href="#common-hpc-applications-on-graviton">Common HPC Applications on Graviton</a></h3>
<p>Below is a list of some common HPC applications that run on Graviton.
ISV                   | Application   | Release of support  | Additional Notes
----------------------|---------------|---------------------|-----------------
Ansys                 | Fluent        | v221                | <a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/">Graviton Applications (AWS)</a>
Ansys                 | LS-Dyna       | 12.1                | <a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/">Graviton Applications (AWS)</a>, <a href="https://rescale.com/blog/rescale-automates-the-deployment-of-ansys-ls-dyna-and-ansys-fluent-workloads-on-amazon-ec2-hpc7g-instances/">ANYS Deployment (Rescale)</a>
Ansys                 | RedHawk-SC    | 2023R1              | <a href="https://www.ansys.com/content/dam/amp/2023/june/quick-request/arm-64-processor-support-announcement-june-2023.pdf">Release Notes</a>
Fritz Haber Institute | FHIaims       | 21.02               | <a href="https://aws.amazon.com/blogs/hpc/quantum-chemistry-calculation-on-aws/">Quantum Chemistry (AWS)</a>
National Center for Atmospheric Research | WRF | WRFV4.5    | <a href="https://aws.amazon.com/blogs/hpc/numerical-weather-prediction-on-aws-graviton2/">Weather on Graviton (AWS)</a>, <a href="https://community.arm.com/arm-community-blogs/b/high-performance-computing-blog/posts/assessing-aws-graviton2-for-running-wrf">WRF on Graviton2 (ARM)</a>
OpenFOAM Foundation / ESI | OpenFOAM  | OpenFOAM7           | <a href="https://aws.amazon.com/blogs/hpc/getting-the-best-openfoam-performance-on-aws/">Getting Best Performance (AWS)</a>, <a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/">Graviton Applications (AWS)</a>, <a href="https://github.com/aws/aws-graviton-getting-started/tree/main/HPC#openfoam">Instructions (AWS)</a>
Sentieon              | DNAseq , TNseq, DNAscope | 202112.02 | <a href="https://support.sentieon.com/manual/appendix/releasenotes/#release-202112-02">Release Notes</a>, <a href="https://aws.amazon.com/blogs/hpc/cost-effective-and-accurate-genomics-analysis-with-sentieon-on-aws/">Cost Effective Genomics (AWS)</a>
Siemens               | StarCCM++     | 2023.2              | <a href="https://blogs.sw.siemens.com/simcenter/simcenter-star-ccm-2302-released/#section_3">Release Notes</a>
Université de Genève  | Palabos       | 2010                | <a href="https://aws.amazon.com/blogs/hpc/lattice-boltzmann-simulation-with-palabos-on-aws-using-graviton-based-amazon-ec2-hpc7g-instances/">Lattice-Boltzmann Palabos (AWS)</a>
Altair Engineering  | OpenRadioss       | 20231204                | <a href="https://www.openradioss.org/presentations-aachen270623/?wvideo=3ox1rtpco8">Presentations-Aachen270623 - OpenRadioss</a>, <a href="https://openradioss.atlassian.net/wiki/spaces/OPENRADIOSS/pages/47546369/HPC+Benchmark+Models">Instructions</a>
Électricité de France   | Code Saturne       | 8.0.2                | https://www.code-saturne.org/cms/web/documentation/Tutorials
HEXAGON               | Cradle CFD           | 2024.1               | <a href="https://simcompanion.hexagon.com/customers/s/article/What-s-New-in-Cradle-CFD-2024-1">Release Notes</a></p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="compile-instructions-on-an-ec2"><a class="header" href="#compile-instructions-on-an-ec2">Compile instructions on an EC2</a></h2>
<p>If you are a developer who want to build and test your applications on Graviton. You can get started by launching a Ubuntu 20.04 C7g instance (4xlarge or larger) from the console. Follow the procedures below to set up the tools:</p>
<pre><code># get the build tools and upgrade GCC
sudo apt update -y
sudo apt install build-essential environment-modules cmake m4 zlib1g zlib1g-dev csh unzip flex -y

sudo add-apt-repository -y ppa:ubuntu-toolchain-r/test
sudo apt install -y gcc-11 g++-11 gfortran-11 -y
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 100 --slave /usr/bin/g++ g++ /usr/bin/g++-11 --slave /usr/bin/gcov gcov /usr/bin/gcov-11 --slave /usr/bin/gfortran gfortran /usr/bin/gfortran-11
</code></pre>
<p>You can check by <code>gcc --version</code> to confirm that you have gcc 11.4.0 installed.</p>
<pre><code># install Arm Compiler for Linux, Arm Performance Libraries under /shared
sudo mkdir -p /shared/tools/
sudo chown -R ubuntu: /shared

# check Arm's website for the latest link
cd /shared/tools
wget -O arm-compiler-for-linux_23.04.1_Ubuntu-20.04_aarch64.tar 'https://developer.arm.com/-/media/Files/downloads/hpc/arm-compiler-for-linux/23-04-1/arm-compiler-for-linux_23.04.1_Ubuntu-20.04_aarch64.tar'
tar xf arm-compiler-for-linux_23.04.1_Ubuntu-20.04_aarch64.tar
./arm-compiler-for-linux_23.04.1_Ubuntu-20.04/arm-compiler-for-linux_23.04.1_Ubuntu-20.04.sh \
-i /shared/arm -a --force
</code></pre>
<p>You can check if the Arm Compiler and Armpl are installed and loaded properly by the following commands:</p>
<pre><code>source /etc/profile.d/modules.sh
module use /shared/arm/modulefiles
module av
module load acfl armpl
module list
</code></pre>
<p>You should be getting the following messages if the installation is successful.</p>
<pre><code>Currently Loaded Modulefiles:
 1) binutils/12.2.0   2) acfl/23.04.1   3) armpl/23.04.1
</code></pre>
<p>After that, you need to install EFA driver which is not installed on an EC2 instance by default and <a href="HPC/README.html#open-mpi">Open MPI</a>.</p>
<pre><code># install EFA, Open MPI under /shared
cd /shared/tools
curl -O https://efa-installer.amazonaws.com/aws-efa-installer-1.25.0.tar.gz
tar xf aws-efa-installer-1.25.0.tar.gz
cd aws-efa-installer
sudo ./efa_installer.sh -y
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ml-inference-on-graviton-cpus-with-pytorch"><a class="header" href="#ml-inference-on-graviton-cpus-with-pytorch">ML inference on Graviton CPUs with PyTorch</a></h1>
<p><strong>Introduction</strong></p>
<p>PyTorch is an open-source machine learning framework based on the Torch library, used for applications such as computer vision and natural language processing. It can be used across training and inference of deep neural networks. This document covers how to use PyTorch based machine learning inference on Graviton CPUs, what runtime configurations are important and how to debug any performance issues. The document also covers instructions for source builds and how to enable some of the downstream features.</p>
<h1 id="how-to-use-pytorch-on-graviton-cpus"><a class="header" href="#how-to-use-pytorch-on-graviton-cpus">How to use PyTorch on Graviton CPUs</a></h1>
<p>There are multiple levels of software package abstractions available: AWS DLC (Deep Learning Container, comes with all the packages installed), Python wheel (easier option for integrating pytorch inference into an existing service), AWS DJL (Deep Java Library, provides JAVA interface along with native torch libraries), and the Docker hub images (comes with downstream experimental features). Examples of using each method are below.</p>
<p><strong>AWS Graviton PyTorch DLC</strong></p>
<p>AWS <a href="https://github.com/aws/deep-learning-containers">Deep Learning Containers</a> are pre-built Docker images that make it easier to run popular deep learning frameworks and tools on AWS. Pytorch 2.6.0 is the latest version that DLCs cater to. The list of available images is in <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md">available_images.md</a></p>
<p>These DLCs continue to deliver the best performance on Graviton for Bert and RoBerta sentiment analysis and fill-mask models, making Graviton the most cost-effective CPU platform on the AWS cloud for these models.</p>
<pre><code>sudo apt-get update
sudo apt-get -y install awscli docker

# Login to ECR to avoid image download throttling
aws ecr get-login-password --region us-east-1 \
| docker login --username AWS \
  --password-stdin 763104351884.dkr.ecr.us-east-1.amazonaws.com

# Pull the AWS DLC for pytorch
docker pull 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference-arm64:2.6.0-cpu-py312-ubuntu22.04-ec2
</code></pre>
<p><strong>Using Python wheels</strong></p>
<pre><code># Install Python
sudo apt-get update
sudo apt-get install -y python3 python3-pip

# Upgrade pip3 to the latest version
python3 -m pip install --upgrade pip

# Install PyTorch and extensions
python3 -m pip install torch
python3 -m pip install torchvision torchaudio
</code></pre>
<p><strong>Using AWS Deep Java Library (DJL)</strong></p>
<p>DJL is a high level JAVA API for Machine Learning and Deep Learning frameworks. It supports aarch64 linux platform with PyTorch backend. The DJL binaries package the native torch libraries from official libtorch distribution which are optimized for AWS Graviton3. The following sections explain how to install, enable runtime configuration, and benchmark pytorch inference with DJL interface on AWS Graviton3 instances. Please refer to <a href="https://github.com/deepjavalibrary/djl/blob/master/docs/development/inference_performance_optimization.md#pytorch">djl pytorch documentation</a> for more details.</p>
<pre><code>sudo snap install openjdk
curl -O https://publish.djl.ai/djl-serving/djl-serving_0.25.0-1_all.deb
sudo dpkg -i djl-serving_0.25.0-1_all.deb
</code></pre>
<p><strong>Using Docker hub container</strong></p>
<p>4Q'23 Docker hub images from armswdev are based on PyTorch 2.0.0, but also include additional downstream optimizations and experimental features. These are avaiable for trying out the experimental downstream features and provide early feedback.</p>
<pre><code># Pull pytorch docker container with onednn-acl optimizations enabled
docker pull armswdev/pytorch-arm-neoverse:r23.10-torch-2.0.0-onednn-acl

# Launch the docker image
docker run -it --rm -v /home/ubuntu/:/hostfs armswdev/pytorch-arm-neoverse:r23.10-torch-2.0.0-onednn-acl
</code></pre>
<h1 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h1>
<ol>
<li>It is highly recommended to use the AMIs based on Linux Kernel 5.10 and beyond for the best PyTorch inference performance on Graviton3 instances</li>
<li>Python 3.8 is the minimum supported python version starting PyTorch 2.0.0. For more details, please refer to PyTorch 2.0 <a href="https://github.com/pytorch/pytorch/releases/tag/v2.0.0">release note</a></li>
</ol>
<h1 id="runtime-configurations-for-optimal-performance"><a class="header" href="#runtime-configurations-for-optimal-performance">Runtime configurations for optimal performance</a></h1>
<p>AWS DLCs come with all the optimizations enabled, so, there are no additional runtime configurations required. Where as for the python wheels and the docker hub images, enable the below runtime configurations to achieve the best performance.</p>
<pre><code># Graviton3(E) (e.g. c7g, c7gn and Hpc7g instances) and later generations support BF16 format for ML acceleration. This can be enabled in oneDNN by setting the below environment variable
grep -q bf16 /proc/cpuinfo &amp;&amp; export DNNL_DEFAULT_FPMATH_MODE=BF16

# Enable primitive caching to avoid the redundant primitive allocation
# latency overhead. Please note this caching feature increases the
# memory footprint. Tune this cache capacity to a lower value to
# reduce the additional memory requirement.
export LRU_CACHE_CAPACITY=1024

# Enable Transparent huge page allocations from PyTorch C10 allocator
export THP_MEM_ALLOC_ENABLE=1

# Make sure the openmp threads are distributed across all the processes for multi process applications to avoid over subscription for the vcpus. For example if there is a single application process, then num_processes should be set to '1' so that all the vcpus are assigned to it with one-to-one mapping to omp threads

num_vcpus=$(getconf _NPROCESSORS_ONLN)
num_processes=&lt;number of processes&gt;
export OMP_NUM_THREADS=$((1 &gt; ($num_vcpus/$num_processes) ? 1 : ($num_vcpus/$num_processes)))
export OMP_PROC_BIND=false
export OMP_PLACES=cores
</code></pre>
<p>We also recommend using <code>torch.compile()</code> when possible, as this has been shown to accelerate inference by up to 2x as described in our blog: <a href="https://aws.amazon.com/blogs/machine-learning/accelerated-pytorch-inference-with-torch-compile-on-aws-graviton-processors/">Accelerated PyTorch inference with torch.compile on AWS Graviton processors</a>.</p>
<p>While running inference using Convolutional Neural Networks (CNNs), we recommend converting the model to the channels-last format for optimal performance.</p>
<pre><code>import torch
import torchvision.models as models

model = models.mobilenet_v2(pretrained=True).eval()
model = model.to(memory_format=torch.channels_last)

sample_input = torch.randn(1, 3, 224, 224)
with torch.inference_mode():
    outputs = model(sample_input)
</code></pre>
<h1 id="evaluate-performance-with-pytorch-benchmark"><a class="header" href="#evaluate-performance-with-pytorch-benchmark">Evaluate performance with PyTorch benchmark</a></h1>
<ol>
<li>Resnet50 benchmarking</li>
</ol>
<pre><code># Clone PyTorch benchmark repo
git clone https://github.com/pytorch/benchmark.git

# Setup Resnet50 benchmark
cd benchmark
python3 install.py resnet50

# Install the dependent wheels
python3 -m pip install numba

# Run Resnet50 inference in torchscript mode. On successful completion of the inference runs,
# the script prints the inference latency and accuracy results

# Batch mode, the default batch size is 32
python3 run.py resnet50 -d cpu --backend torchscript -t eval --use_cosine_similarity

# Single inference mode
python3 run.py resnet50 -d cpu --backend torchscript -t eval --use_cosine_similarity --bs 1

</code></pre>
<ol start="2">
<li>Bert benchmarking</li>
</ol>
<pre><code># Clone PyTorch benchmark repo
git clone https://github.com/pytorch/benchmark.git

# Setup Bert benchmark
cd benchmark
python3 install.py bert

# Run BERT_pytorch inference in torchscript mode. On successful completion of the inference runs,
# the script prints the inference latency and accuracy results

# Batch mode
python3 run.py BERT_pytorch -d cpu --backend torchscript -t eval --use_cosine_similarity --bs 32

# Single inference mode
python3 run.py BERT_pytorch -d cpu --backend torchscript -t eval --use_cosine_similarity --bs 1
</code></pre>
<ol start="3">
<li>Resnet benchmarking with PyTorch Java</li>
</ol>
<pre><code># Install djl-bench
curl -O https://publish.djl.ai/djl-bench/0.26.0/djl-bench_0.26.0-1_all.deb
sudo dpkg -i djl-bench_0.26.0-1_all.deb

git clone https://github.com/deepjavalibrary/djl-serving.git
cd djl-serving

# Make sure the runtime env for Bfloat16 and LRU Cache are setup before running the inference tests
# Run Resnet18 with batch size of 16. On successful completion of the inference runs, the script
# prints the inference results, throughput and latencies
djl-bench -e PyTorch -u https://alpha-djl-demos.s3.amazonaws.com/model/djl-blockrunner/pytorch_resnet18.zip -n traced_resnet18 -c 10 -s 16,3,600,600

# Run Resnet50 ssd with batch size of 16
djl-bench -e PyTorch -c 2 -s 16,3,300,300 -u djl://ai.djl.pytorch/ssd/0.0.1/ssd_300_resnet50

</code></pre>
<h1 id="troubleshooting-performance-issues"><a class="header" href="#troubleshooting-performance-issues">Troubleshooting performance issues</a></h1>
<p>The below steps help debugging performance issues with pytorch inference.</p>
<ol>
<li>Run inference with DNNL and openmp verbose logs enabled to understand which backend is used for the tensor ops execution.</li>
</ol>
<pre><code>export DNNL_VERBOSE=1
export OMP_DISPLAY_ENV=VERBOSE
</code></pre>
<p>If there are no OneDNN logs on the terminal, this could mean either the ops are executed with OpenBLAS or XLA backend. For OneDNN accelerated ops, there should be a flow of DNN logs with details about the shapes, prop kinds and execution times. Inspect the logs to see if there are any ops and shapes not executed with the ACL gemm kernel, instead executed by cpp reference kernel. See below example dnnl logs to understand how the ACL gemm and reference cpp kernel execution traces look like.</p>
<pre><code># ACL gemm kernel
dnnl_verbose,exec,cpu,convolution,gemm:acl,forward_training,src_f32::blocked:acdb:f0 wei_f32::blocked:acdb:f0 bia_f32::blocked:a:f0 dst_f32::blocked:acdb:f0,post_ops:'eltwise_relu;';,alg:convolution_direct,mb1_ic256oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0

# OneDNN cpp reference kernel
dnnl_verbose,exec,cpu,convolution,gemm:ref,forward_training,src_f32::blocked:abcd:f0 wei_f32::blocked:abcde:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,post_ops:'eltwise_bounded_relu:6;';,alg:convolution_direct,mb1_g64ic64oc64_ih112oh56kh3sh2dh0ph0_iw112ow56kw3sw2dw0pw0
</code></pre>
<p>If the tensor ops and shapes are still not executed with ACL gemm kernels, please raise an issue on <a href="https://github.com/ARM-software/ComputeLibrary">ACL github</a> with the operator and shape details.</p>
<ol start="2">
<li>
<p>Once the tensor ops are executed with ACL gemm kernels, enable fast math mode, 'export DNNL_DEFAULT_FPMATH_MODE=BF16', to pick bfloat16 hybrid gemm kernels.</p>
</li>
<li>
<p>Linux thp (transparent huge pages) improve the memory allocation latencies for large tensors. This is important especially for batched inference use cases where the tensors are typically larger than 30MB. To take advantage of the thp allocations, make sure PyTorch C10 memory allocator optimizations are enabled. Most of the Linux based OS distributions come with "madvise" as the default thp allocation mode, if it comes with "never" then set it to "madvise" mode first.</p>
</li>
</ol>
<pre><code># Check the default thp mode in kernel
cat /sys/kernel/mm/transparent_hugepage/enabled

# If the thp mode is [never], then set it to 'madvise'
echo madvise &gt; /sys/kernel/mm/transparent_hugepage/enabled
cat /sys/kernel/mm/transparent_hugepage/enabled
always [madvise] never

# Enable THP allocations from C10 memory allocator
export THP_MEM_ALLOC_ENABLE=1
</code></pre>
<ol start="4">
<li>Starting with the release of PyTorch 1.13.0, mkldnn (OneDNN) backend is enabled for 'matmul' operator. While mkldnn along with ACL provides the best performance across several tensor shapes, the runtime setup overhead may not be acceptable for smaller tensor shapes. For use cases with fewer input tokens, check the performance by disabling the mkldnn backend and switching to openBLAS gemm kernels. This has shown improvement for shapes like "12x12x64:12x64x12:12x12x12" and "12x16x16:12x16x64:12x16x64".</li>
</ol>
<pre><code>export TORCH_MKLDNN_MATMUL_MIN_DIM=1024
</code></pre>
<ol start="5">
<li>The above triaging steps cover typical issues due to the missing runtime configurations. If you are stuck with any of these steps or if the performance is still not meeting the target, please raise an issue on <a href="https://github.com/aws/aws-graviton-getting-started">aws-graviton-getting-started</a> github.</li>
</ol>
<h1 id="building-pytorch-from-source"><a class="header" href="#building-pytorch-from-source">Building PyTorch from source</a></h1>
<p>With AWS Graviton DLCs and python wheels available for every official PyTorch release, there may not be a need for customers to build PyTorch from sources. However, to make the user guide complete, this section provides instructions for building the torch wheels from sources.</p>
<p>For more automated single step building of pytorch wheels from a release tag, please use the pyTorch builder repo <a href="https://github.com/pytorch/builder/blob/main/aarch64_linux/build_aarch64_wheel.py">scripts</a>. For building torch wheel alone with the downstream experimental features, please use the below steps.</p>
<pre><code># This step is required if gcc-10 is not the default version on the OS distribution, e.g. Ubuntu 20.04
# Install gcc-10 and g++-10 as it is required for Arm Compute Library build.
sudo apt install -y gcc-10 g++-10
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 1
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-10 1

# Install the required tools
sudo apt install -y scons cmake

# Build Arm Compute Library (ACL)
cd $HOME
git clone https://github.com/ARM-software/ComputeLibrary.git
cd ComputeLibrary
git checkout v23.05.1
scons Werror=1 -j8 debug=0 neon=1 opencl=0 os=linux openmp=1 cppthreads=0 arch=armv8a multi_isa=1 build=native

# Build PyTorch from the tip of the tree
cd $HOME
git clone https://github.com/pytorch/pytorch.git
cd pytorch
git submodule sync
git submodule update --init --recursive

# Set the ACL root directory and enable MKLDNN backend
export ACL_ROOT_DIR=$HOME/ComputeLibrary
export USE_MKLDNN=ON USE_MKLDNN_ACL=ON

python3 setup.py bdist_wheel

# Install the wheel
pip3 install &lt;dist/*.whl&gt;

</code></pre>
<h1 id="using-torch-xla-for-model-inference"><a class="header" href="#using-torch-xla-for-model-inference">Using Torch XLA for model inference</a></h1>
<p>While the PyTorch torchscript and the runtime backend covers majority of the networks, there are few scenarios where either the default optimizer can't optimize the generic graph or the runtime kernel launch overhead is simply not acceptable. XLA addresses these gaps by providing an alternative mode of running models: it compiles the PyTorch graph into a sequence of computation kernels generated specifically for the given model.</p>
<p><strong>How to build torach-xla wheel</strong></p>
<p>The scripts for building the torch and torch-xla wheels are avaiable in torch-xla repo. This section provides instructions for using those scripts.</p>
<pre><code># The PyTorch and torch-xla build scripts are available in torch-xla repo
git clone https://github.com/pytorch/xla.git
cd xla

# To build and install PyTorch from the tip of the tree
./scripts/build_torch_wheels.sh 3.8 nightly

</code></pre>
<p><strong>How to enable torch-xla for model inferece</strong></p>
<p>PyTorch/XLA creates a TensorFlow local server everytime a new PyTorch/XLA program is run. The XRT (XLA Runtime) client connects to the local TensorFlow server. So, to enable XLA device, set the XRT device map and worker as beolow</p>
<pre><code>export XRT_DEVICE_MAP="CPU:0;/job:localservice/replica:0/task:0/device:XLA_CPU:0"
export XRT_WORKERS="localservice:0;grpc://localhost:9002"
</code></pre>
<p>The below snippet highlights how easy it is to switch any  model to run on XLA. The model definition, dataloader, optimizer and training loop can work on any device. The only XLA-specific code is a couple lines that acquire the XLA device and mark the step. Calling xm.mark_step() at the end of each inference iteration causes XLA to execute its current graph and update the model’s parameters</p>
<pre><code class="language-python">import torch_xla
import torch_xla.core.xla_model as xm

# instead of the standard 'cuda' or 'cpu' device, create the xla device.
device = xm.xla_device()
.
.
.
xm.mark_step() # at the end of each inference iteration
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ml-inference-on-graviton-cpus-with-tensorflow"><a class="header" href="#ml-inference-on-graviton-cpus-with-tensorflow">ML inference on Graviton CPUs with TensorFlow</a></h1>
<p><strong>Introduction</strong></p>
<p>TensorFlow is an open-source software library for machine learning and artificial intelligence. It can be used across training and inference of deep neural networks. This document covers how to use TensorFlow based machine learning inference on Graviton CPUs, what runtime configurations are important and how to debug any performance issues.</p>
<h1 id="how-to-use-tensorflow-on-graviton-cpus"><a class="header" href="#how-to-use-tensorflow-on-graviton-cpus">How to use TensorFlow on Graviton CPUs</a></h1>
<p>There are multiple levels of software package abstractions available:</p>
<ul>
<li>AWS DLC (Deep Learning Container, gives the best performance, comes with additional optimizations on top of the official wheels, and all the packages installed)</li>
<li>Python wheel (easiest option to get the release features) and</li>
<li>Docker hub images (comes with downstream experimental features). Examples of using each method are below.</li>
</ul>
<p><strong>AWS Graviton TensorFlow DLC</strong></p>
<p>As of May 2024, AWS Graviton DLCs are based on TensorFlow 2.14.1. The DLCs enable the Graviton optimizations by default.</p>
<pre><code># Login and pull the AWS DLC for tensorflow
aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com

docker pull 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference-graviton:2.14.1-cpu-py310-ubuntu20.04-ec2

# Sample command to launch the tensorflow serving api with resnet50 model
docker run -p 8501:8501 --name tfserving_resnet --mount type=bind,source=/tmp/resnet,target=/models/resnet -e MODEL_NAME=resnet -t 763104351884.dkr.ecr.us-west-2.amazonaws.com/tensorflow-inference-graviton:2.14.1-cpu-py310-ubuntu20.04-ec2
</code></pre>
<p><strong>Using Python wheel</strong></p>
<pre><code>pip install tensorflow
</code></pre>
<p><strong>Using Docker hub container</strong></p>
<p>As of May 2024, Docker hub images from armswdev are based on TensorFlow 2.15.1, but also include additional downstream optimizations and experimental features. These are avaiable for trying out the experimental downstream features and provide early feedback.</p>
<pre><code># pull the tensorflow docker container with onednn-acl optimizations enabled
docker pull armswdev/tensorflow-arm-neoverse

# launch the docker image
docker run -it --rm -v /home/ubuntu/:/hostfs armswdev/tensorflow-arm-neoverse
</code></pre>
<h1 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h1>
<p>It is highly recommended to use the AMIs based on Linux Kernel 5.10 and beyond for the best TensorFlow inference performance on Graviton3 instances.</p>
<h1 id="runtime-configurations-for-optimal-performance-1"><a class="header" href="#runtime-configurations-for-optimal-performance-1">Runtime configurations for optimal performance</a></h1>
<p>AWS DLCs come with all the optimizations enabled, so, there are no additional runtime configurations required. Where as for the python wheels and the docker hub images, enable the below runtime configurations to achieve the best performance.</p>
<pre><code># For TensorFlow versions older than 2.14.0, the default runtime backend is Eigen, but typically onednn+acl provides better performance. To enable the onednn+acl backend, set the following TF environment variable
export TF_ENABLE_ONEDNN_OPTS=1

# Graviton3(E) (e.g. c7g, c7gn, and hpc7g instances) supports BF16 format for ML acceleration. This can be enabled in oneDNN by setting the below environment variable
grep -q bf16 /proc/cpuinfo &amp;&amp; export DNNL_DEFAULT_FPMATH_MODE=BF16

# Make sure the openmp threads are distributed across all the processes for multi process applications to avoid over subscription for the vcpus. For example if there is a single application process, then num_processes should be set to '1' so that all the vcpus are assigned to it with one-to-one mapping to omp threads

num_vcpus=$(getconf _NPROCESSORS_ONLN)
num_processes=&lt;number of processes&gt;
export OMP_NUM_THREADS=$((1 &gt; ($num_vcpus/$num_processes) ? 1 : ($num_vcpus/$num_processes)))
export OMP_PROC_BIND=false
export OMP_PLACES=cores
</code></pre>
<pre><code class="language-python"># TensorFlow inter and intra_op_parallelism_thread settings are critical for the optimal workload parallelization in a multi-threaded system.
# set the inter and intra op thread count during the session creation, an example snippet is given below.
session = Session(
                 config=ConfigProto(
                      intra_op_parallelism_threads=&lt;num. of vcpus&gt;,
                      inter_op_parallelism_threads=1,
                )
)
</code></pre>
<p>TensorFlow recommends the graph optimization pass for inference to remove training specific nodes, fold batchnorms and fuse operators. This is a generic optimizaion across CPU, GPU or TPU inference, and the optimization script is part of the TensorFlow python <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/optimize_for_inference_lib.py">tools</a>. For a detailed description, please refer to the TensorFlow <a href="https://www.tensorflow.org/guide/graph_optimization">Grappler</a> documentation. Below is a snippet of what libraries to import and how to invoke the Grappler passes for inference.</p>
<pre><code class="language-python">from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference

graph_def = tf.compat.v1.GraphDef()
with tf.compat.v1.gfile.FastGFile(model_path, "rb") as f:
     graph_def.ParseFromString(f.read())

optimized_graph_def = optimize_for_inference(graph_def, [item.split(':')[0] for item in inputs],
                    [item.split(':')[0] for item in outputs], dtypes.float32.as_datatype_enum, False)
g = tf.compat.v1.import_graph_def(optimized_graph_def, name='')
</code></pre>
<h1 id="evaluate-performance-with-the-standard-mlperf-inference-benchmarks"><a class="header" href="#evaluate-performance-with-the-standard-mlperf-inference-benchmarks">Evaluate performance with the standard MLPerf inference benchmarks</a></h1>
<ol>
<li>Setup MLPerf inference benchmarks and the required tools.</li>
</ol>
<pre><code>sudo apt install -y build-essential cmake libgl1-mesa-glx libglib2.0-0 libsm6 libxrender1 libxext6 python3-pip

git clone https://github.com/mlcommons/inference.git --recursive
cd inference
git checkout v2.0
cd loadgen
CFLAGS="-std=c++14" python3 setup.py bdist_wheel
pip install &lt;dist/*.whl&gt;
</code></pre>
<ol start="2">
<li>Benchmark image classification with Resnet50</li>
</ol>
<pre><code>sudo apt install python3-ck
ck pull repo:ck-env

# Download ImageNet's validation set
# These will be installed to ${HOME}/CK_TOOLS/
echo 0 | ck install package --tags=image-classification,dataset,imagenet,aux
echo 1 | ck install package --tags=image-classification,dataset,imagenet,val

# Copy the labels into the image location
cp ${HOME}/CK-TOOLS/dataset-imagenet-ilsvrc2012-aux-from.berkeley/val.txt ${HOME}/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min/val_map.txt

cd inference/vision/classification_and_detection
wget https://zenodo.org/record/2535873/files/resnet50_v1.pb

# Install the additional packages required for resnet50 inference
pip install opencv-python pycocotools psutil tqdm

# Set the data and model path
export DATA_DIR=${HOME}/CK-TOOLS/dataset-imagenet-ilsvrc2012-val-min
export MODEL_DIR=${HOME}/inference/vision/classification_and_detection

# Setup the tensorflow thread pool parameters via MLPerf env variables
export MLPERF_NUM_INTER_THREADS=1

num_vcpus=$(getconf _NPROCESSORS_ONLN)
num_processes=&lt;number of processes&gt;
export MLPERF_NUM_INTRA_THREADS=$((1 &gt; ($num_vcpus/$num_processes) ? 1 : ($num_vcpus/$num_processes)))

./run_local.sh tf resnet50 cpu --scenario=SingleStream
./run_local.sh tf resnet50 cpu --scenario=Offline
</code></pre>
<ol start="3">
<li>Benchmark natual language processing with Bert</li>
</ol>
<pre><code>pip install transformers boto3
cd inference/language/bert
make setup
python3 run.py --backend=tf --scenario=SingleStream
python3 run.py --backend=tf --scenario=Offline
</code></pre>
<h1 id="troubleshooting-performance-issues-1"><a class="header" href="#troubleshooting-performance-issues-1">Troubleshooting performance issues</a></h1>
<p>The below steps help debugging performance issues with any inference application.</p>
<ol>
<li>Run inference with DNNL and openmp verbose logs enabled to understand which backend is used for the tensor ops execution.</li>
</ol>
<pre><code>export DNNL_VERBOSE=1
export OMP_DISPLAY_ENV=VERBOSE
</code></pre>
<p>If there are no OneDNN logs on the terminal, this could mean the ops are executed with Eigen. To switch from Eigen to OneDNN+ACL backend, set 'TF_ENABLE_ONEDNN_OPTS=1' and rerun the model inference. There should be a flow of DNN logs with details about the shapes, prop kinds and execution times. Inspect the logs to see if there are any ops and shapes not executed with the ACL gemm kernel, instead executed by cpp reference kernel. See below example dnnl logs to understand how the ACL gemm and reference cpp kernel execution traces look like.</p>
<pre><code># ACL gemm kernel
dnnl_verbose,exec,cpu,convolution,gemm:acl,forward_training,src_f32::blocked:acdb:f0 wei_f32::blocked:acdb:f0 bia_f32::blocked:a:f0 dst_f32::blocked:acdb:f0,post_ops:'eltwise_relu;';,alg:convolution_direct,mb1_ic256oc64_ih56oh56kh1sh1dh0ph0_iw56ow56kw1sw1dw0pw0

# OneDNN cpp reference kernel
dnnl_verbose,exec,cpu,convolution,gemm:ref,forward_training,src_f32::blocked:abcd:f0 wei_f32::blocked:abcde:f0 bia_f32::blocked:a:f0 dst_f32::blocked:abcd:f0,post_ops:'eltwise_bounded_relu:6;';,alg:convolution_direct,mb1_g64ic64oc64_ih112oh56kh3sh2dh0ph0_iw112ow56kw3sw2dw0pw0
</code></pre>
<p>If there are any shapes not going to ACL gemm kernels, the first step is to make sure the graph has been optimized for inference via Grappler passes.</p>
<pre><code class="language-python">from tensorflow.python.tools.optimize_for_inference_lib import optimize_for_inference

graph_def = tf.compat.v1.GraphDef()
with tf.compat.v1.gfile.FastGFile(model_path, "rb") as f:
     graph_def.ParseFromString(f.read())

optimized_graph_def = optimize_for_inference(graph_def, [item.split(':')[0] for item in inputs],
                    [item.split(':')[0] for item in outputs], dtypes.float32.as_datatype_enum, False)
g = tf.compat.v1.import_graph_def(optimized_graph_def, name='')
</code></pre>
<p>If the tensor ops and shapes are still not executed with ACL gemm kernels, please raise an ssue on <a href="https://github.com/ARM-software/ComputeLibrary">ACL github</a> with the operator and shape details.</p>
<ol start="2">
<li>
<p>Once the tensor ops are executed with ACL gemm kernels, enable fast math mode, 'export DNNL_DEFAULT_FPMATH_MODE=BF16', to pick bfloat16 hybrid gemm kernels.</p>
</li>
<li>
<p>Verify the TensorFlow inter and intra thread pool settings are optimal as recommended in the runtime configurations section. Then, inspect the OMP environment to make sure the vcpu resources are not over subscribed for multi process applications. A typical openmp environment for a 64 vcpu, single process application looks like the one below.</p>
</li>
</ol>
<pre><code>OPENMP DISPLAY ENVIRONMENT BEGIN
  _OPENMP = '201511'
  OMP_DYNAMIC = 'FALSE'
  OMP_NESTED = 'FALSE'
  OMP_NUM_THREADS = '64'
  OMP_SCHEDULE = 'DYNAMIC'
  OMP_PROC_BIND = 'FALSE'
  OMP_PLACES = ''
  OMP_STACKSIZE = '0'
  OMP_WAIT_POLICY = 'PASSIVE'
  OMP_THREAD_LIMIT = '4294967295'
  OMP_MAX_ACTIVE_LEVELS = '1'
  OMP_CANCELLATION = 'FALSE'
  OMP_DEFAULT_DEVICE = '0'
  OMP_MAX_TASK_PRIORITY = '0'
  OMP_DISPLAY_AFFINITY = 'FALSE'
  OMP_AFFINITY_FORMAT = 'level %L thread %i affinity %A'
  OMP_ALLOCATOR = 'omp_default_mem_alloc'
  OMP_TARGET_OFFLOAD = 'DEFAULT'
  GOMP_CPU_AFFINITY = ''
  GOMP_STACKSIZE = '0'
  GOMP_SPINCOUNT = '300000'
OPENMP DISPLAY ENVIRONMENT END
</code></pre>
<ol start="4">
<li>The above triaging steps cover typical issues due to the missing compiler or runtime configurations. If you are stuck with any of these steps or if the performance is still not meeting the target, please raise an issue on <a href="https://github.com/aws/aws-graviton-getting-started">aws-graviton-getting-started</a> github.</li>
</ol>
<h1 id="building-tensorflow-from-sources"><a class="header" href="#building-tensorflow-from-sources">Building TensorFlow from sources</a></h1>
<p>We recommend using the official distributions of TensorFlow for Graviton, but there are cases developers may want to compile TensorFlow from source. One such case would be to experiment with new features or develop custom features. This section outlines the recommended way to compile TensorFlow from source. Please note that "--config=mkl_aarch64_threadpool" is the recommended bazel config for aarch64 builds. The following instructions cover both gcc and clang builds.</p>
<p><strong>Install the compiler tool chain</strong></p>
<p>For Clang builds, install the llvm version recommended in the TensorFlow release. For example, llvm-17.0.2 is recommended for TF v2.16</p>
<pre><code>wget https://github.com/llvm/llvm-project/releases/download/llvmorg-17.0.2/clang+llvm-17.0.2-aarch64-linux-gnu.tar.xz -O- | sudo tar xv -J --strip-component=1 -C /usr
</code></pre>
<p>For GCC builds, gcc10 or above version is required for building mkl_aarch64_threadpool configuration with Arm Compute Library. The following step is required if gcc-10+ is not the default version on the OS distribution, e.g. Ubuntu 20.04</p>
<pre><code>sudo apt install -y gcc-10 g++-10
sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 1
sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-10 1

</code></pre>
<p><strong>Install the required packages</strong></p>
<pre><code>pip3 install numpy packaging
sudo apt install patchelf

# Install bazel for aarch64
# Bazel version required depends on the TensorFlow version, please install the correct one
# https://github.com/tensorflow/tensorflow/blob/master/.bazelversion captures the bazel version details
# For example, tensorflow 2.16.1 build requires bazel 6.5.0
BAZEL=6.5.0  # modify version as needed
sudo wget https://github.com/bazelbuild/bazel/releases/download/$BAZEL/bazel-$BAZEL-linux-arm64 -O /usr/local/bin/bazel
sudo chmod +x /usr/local/bin/bazel
</code></pre>
<p><strong>Configure and build</strong></p>
<pre><code># Clone the tensorflow repository
git clone https://github.com/tensorflow/tensorflow.git
cd tensorflow
# Optionally checkout the stable version if needed
TF_VERSION=v2.16.1 # modify version as needed
git checkout $TF_VERSION

# Set the build configuration
export PYTHON_BIN_PATH=(which python)
export USE_DEFAULT_PYTHON_LIB_PATH=1
export TF_ENABLE_XLA=1
export TF_DOWNLOAD_CLANG=0
export TF_SET_ANDROID_WORKSPACE=0
export TF_NEED_MPI=0
export TF_NEED_ROCM=0
export TF_NEED_GCP=0
export TF_NEED_S3=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_CUDA=0
export TF_NEED_HDFS=0
export TF_NEED_OPENCL=0
export TF_NEED_JEMALLOC=1
export TF_NEED_VERBS=0
export TF_NEED_AWS=0
export TF_NEED_GDR=0
export TF_NEED_OPENCL_SYCL=0
export TF_NEED_COMPUTECPP=0
export TF_NEED_KAFKA=0
export TF_NEED_TENSORRT=0

# Configure the build setup.
# Leave the default option for everything except the compiler option. Enter "Y/n" depending on whether Clang or gcc build is required.
# Do you want to use Clang to build TensorFlow? [Y/n]:
./configure

# Issue bazel build command with 'mkl_aarch64' config to enable onednn+acl backend
bazel build --verbose_failures -s --config=mkl_aarch64_threadpool  //tensorflow/tools/pip_package:build_pip_package //tensorflow:libtensorflow_cc.so //tensorflow:install_headers

# Create and install the wheel
./bazel-bin/tensorflow/tools/pip_package/build_pip_package ./wheel-$TF_VERSION-aarch64

# The output wheel is generated in /home/ubuntu/tensorflow/wheel-$TF_VERSION-aarch64
pip install &lt;wheel-$TF_VERSION-aarch64/*.whl&gt;
</code></pre>
<h1 id="building-tensorflow-java-binaries-jar"><a class="header" href="#building-tensorflow-java-binaries-jar">Building TensorFlow Java binaries (JAR)</a></h1>
<p>This section outlines the recommended way to compile TensorFlow Java binaries. The build uses release python wheels for native libraries so, the tensorflow bazel build is not required for this step.</p>
<p>Note: TensorFlow jar distribution for aarch64 linux platform is blocked on CI support for <a href="https://github.com/tensorflow/java">tensorflow/java</a> repo.</p>
<pre><code>sudo apt-get install pkg-config ccache clang ant python3-pip swig git file wget unzip tar bzip2 gzip patch autoconf-archive autogen automake make cmake libtool bison flex perl nasm curl gfortran libasound2-dev freeglut3-dev libgtk2.0-dev libusb-dev zlib1g libffi-dev libbz2-dev zlib1g-dev

sudo apt install maven default-jdk

# Build and install tensorflow java bindings
git clone https://github.com/tensorflow/java.git
cd java
mvn install -X -T 16

</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="large-language-model-llm-inference-on-graviton-cpus-with-llamacpp"><a class="header" href="#large-language-model-llm-inference-on-graviton-cpus-with-llamacpp">Large Language Model (LLM) inference on Graviton CPUs with llama.cpp</a></h1>
<p><strong>Introduction</strong></p>
<p>The main goal of <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> is to enable LLM inference with minimal setup and state-of-the-art performance on a wide variety of hardware. It's a plain C/C++ implementation without any dependencies. It supports quantized general matrix multiply-add (GEMM) kernels for faster inference and reduced memory use. The quantized GEMM kernels are optimized for AWS Graviton processors using Arm Neon and SVE based matrix multiply-accumulate (MMLA) instructions. This document covers how to build and run llama.cpp efficiently for LLM inference on AWS Graviton based Amazon EC2 Instances.</p>
<h1 id="how-to-use-llamacpp-on-graviton-cpus"><a class="header" href="#how-to-use-llamacpp-on-graviton-cpus">How to use llama.cpp on Graviton CPUs</a></h1>
<p>Building from sources is the recommended way to use llama.cpp on Graviton CPUs, and for other hardware platforms too. This section provides the instructions on how to build llama.cpp from sources and how to install python bindings.</p>
<p><strong>Prerequisites</strong></p>
<p>Graviton3(E) (e.g. c7g/m7g/r7g, c7gn and Hpc7g Instances) and Graviton4 (e.g. r8g Instances) CPUs support BFloat16 format and MMLA instructions for machine learning (ML) acceleration. These hardware features are enabled starting with Linux Kernel version 5.10. So, it is highly recommended to use the AMIs based on Linux Kernel 5.10 and beyond for the best LLM inference performance on Graviton Instances. Use the following queries to list the AMIs with the recommended Kernel versions.</p>
<pre><code># For Kernel 5.10 based AMIs list
aws ec2 describe-images --owners amazon --filters "Name=architecture,Values=arm64" "Name=name,Values=*kernel-5.10*" --query 'sort_by(Images, &amp;CreationDate)[].Name'

# For Kernel 6.x based AMIs list
aws ec2 describe-images --owners amazon --filters "Name=architecture,Values=arm64" "Name=name,Values=*kernel-6.*" --query 'sort_by(Images, &amp;CreationDate)[].Name'
</code></pre>
<p><strong>Build llama.cpp from source</strong></p>
<pre><code># Clone llama.cpp
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

# build with cmake
mkdir build
cd build
cmake .. -DCMAKE_CXX_FLAGS="-mcpu=native" -DCMAKE_C_FLAGS="-mcpu=native"
cmake --build . -v --config Release -j `nproc`

</code></pre>
<p><strong>Install llama.cpp python bindings</strong></p>
<pre><code>CMAKE_ARGS="-DCMAKE_CXX_FLAGS='-mcpu=native' -DCMAKE_C_FLAGS='-mcpu=native'" pip3 install --no-cache-dir llama-cpp-python

</code></pre>
<h1 id="run-llm-inference-with-llamacpp"><a class="header" href="#run-llm-inference-with-llamacpp">Run LLM inference with llama.cpp</a></h1>
<p>llama.cpp provides a set of tools to (1) convert model binary file into GPT-Generated Unified Format (GGUF), (2) quantize single and half precision format models into one of the quantized formats, and (3) run LLM inference locally. For the steps on how to convert model binary into GGUF format and how to quantize them into low precision formats, please check <a href="https://github.com/ggerganov/llama.cpp/blob/master/README.md">llama.cpp README</a>.</p>
<p>The following instructions use Meta Llama-3 8B parameter model from <a href="https://huggingface.co/models">Hugging Face</a> models repository to demonstrate LLM inference performance on AWS Graviton based EC2 Instances. The model is already availble in multiple quantized formats which can be directly run on AWS Graviton processors.</p>
<pre><code># Download the model from Hugging Face model repo.
cd llama.cpp
wget https://huggingface.co/SanctumAI/Meta-Llama-3-8B-Instruct-GGUF/resolve/main/meta-llama-3-8b-instruct.Q4_0.gguf

</code></pre>
<p><strong>Using llama-cli</strong></p>
<pre><code># Now, launch llama-cli with the above model and a sample input prompt. The following command is using 64 threads.
# Change -t argument for running inference with lower thread count. On completion, the script prints throughput and latency metics
# for prompt encoding and response generation.
./build/bin/llama-cli -m meta-llama-3-8b-instruct.Q4_0.gguf -p "Building a visually appealing website can be done in ten simple steps:" -n 512 -t 64

# Launch the model in conversation (chatbot) mode using this command
./build/bin/llama-cli -m meta-llama-3-8b-instruct.Q4_0.gguf -p "You are a helpful assistant" -cnv --color

</code></pre>
<p><strong>Using llama.cpp python binding</strong></p>
<p>Note: Set the <code>n_threads</code> to number of vcpus explicitly while creating the Llama object. This is required to use all cores(vcpus) on Graviton instances. Without this set, the python bindings use half of the vcpus and the performance is not the best.</p>
<pre><code>import json
import argparse

from llama_cpp import Llama

parser = argparse.ArgumentParser()
parser.add_argument("-m", "--model", type=str, default="../models/7B/ggml-models.bin")
args = parser.parse_args()

# for example, for a .16xlarge instance, set n_threads=64
llm = Llama(model_path=args.model,
            n_threads=64)

output = llm(
"Question: How to build a visually appealing website in ten steps? Answer: ",
max_tokens=512,
echo=True,
)

</code></pre>
<h1 id="run-deepseek-r1-llm-inference-on-aws-graviton"><a class="header" href="#run-deepseek-r1-llm-inference-on-aws-graviton">Run DeepSeek R1 LLM Inference on AWS Graviton</a></h1>
<p>DeepSeek R1 is an open-source LLM for conversational AI, coding, and problem-solving tasks. The model can be readily deployed on AWS Graviton-based Amazon EC2 Instances for inference use cases. We recommend using <a href="https://github.com/ollama/ollama">ollama</a> service for the inference deployment. Ollama service is built on top of <a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a> which is highly optimized to achieve the best performance on AWS Graviton processors. This section shows how to install ollama service and run DeepSeek R1 inference.</p>
<pre><code># Launch Graviton3 or Graviton4 based EC2 instance (for example, c7g, m7g, c8g or m8g instances)

# Download and install the ollama service
curl -fsSL https://ollama.com/install.sh | sh
ollama --version
systemctl is-active ollama.service

# If the output is active, the service is running, and you can skip the next step. If it’s not, start it manually
sudo systemctl start ollama.service

# DeepSeek has released multiple versions of the R1 model, sizes from 1.5B parameters to 70B parameters.
# ollama supports deepseek-r1:1.5b/7b/8b/14b/32b/70b models
# Download and run the 8b model using the following command
ollama run deepseek-r1:8b

# To benchmark the prompt evaluation rate and response genetation rate, launch ollama with verbose option.
# At the end of the inference, the script prints the token eval and the token generation rates
ollama run deepseek-r1:8b "&lt;prompt&gt;" --verbose

</code></pre>
<h1 id="additional-resources-1"><a class="header" href="#additional-resources-1">Additional Resources</a></h1>
<p>Please refer to</p>
<ol>
<li><a href="https://community.arm.com/arm-community-blogs/b/infrastructure-solutions-blog/posts/best-in-class-llm-performance">Best-in-class LLM performance on Arm Neoverse V1 based AWS Graviton3 CPUs</a> to know the LLM inference performance measured on AWS Graviton3 based EC2 Instances.</li>
<li><a href="https://community.arm.com/arm-community-blogs/b/infrastructure-solutions-blog/posts/running-llama-3-70b-on-aws-graviton4">Running Llama 3 70B on the AWS Graviton4 CPU with Human Readable Performance</a> for LLM inference performance on AWS Graviton4 based EC2 Instances.</li>
<li><a href="https://dev.to/aws-heroes/intro-to-llama-on-graviton-1dc">Intro to Llama on Graviton</a> for a step by step guide on how to deploy an LLM model on AWS Graviton-based EC2 Instances. Note: This guide refers to llama.cpp version from July 2024. If you are using the latest llama.cpp version, please replace the <code>Q4_0_4_8</code> and <code>Q4_0_8_8</code> with <code>Q4_0</code> format.</li>
<li><a href="https://community.aws/content/2eazHYzSfcY9flCGKsuGjpwqq1B/run-llms-on-cpu-with-amazon-sagemaker-real-time-inference?lang=en">Run LLMs on CPU with Amazon SageMaker Real-time Inference</a> for running LLMs for real-time inference using AWS Graviton3 and Amazon SageMaker.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="large-language-model-llm-inference-on-graviton-cpus-with-vllm"><a class="header" href="#large-language-model-llm-inference-on-graviton-cpus-with-vllm">Large Language Model (LLM) inference on Graviton CPUs with vLLM</a></h1>
<p><strong>Introduction</strong></p>
<p>vLLM is a fast and easy-to-use library for LLM inference and serving. It provides an OpenAI-compatible API server and support NVIDIA GPUs, CPUs and AWS Neuron. vLLM has been adapted to work on ARM64 CPUs with NEON support, leveraging the CPU backend initially developed for the x86 platform. ARM CPU backend currently supports Float32, FP16 and BFloat16 datatypes.
This document covers how to build and run vLLM for LLM inference on AWS Graviton based Amazon EC2 Instances.</p>
<h1 id="how-to-use-vllm-on-graviton-cpus"><a class="header" href="#how-to-use-vllm-on-graviton-cpus">How to use vLLM on Graviton CPUs</a></h1>
<p>There are no pre-built wheels or images for Graviton CPUs, so you must build vLLM from source.</p>
<p><strong>Prerequisites</strong></p>
<p>Graviton3(E) (e.g. *7g instances) and Graviton4 (e.g. *8g instances) CPUs support BFloat16 format and MMLA instructions for machine learning (ML) acceleration. These hardware features are enabled starting with Linux Kernel version 5.10. So, it is highly recommended to use the AMIs based on Linux Kernel 5.10 and beyond for the best LLM inference performance on Graviton Instances. Use the following queries to list the AMIs with the recommended Kernel versions. New Ubuntu 22.04, 24.04, and AL2023 AMIs all have kernels newer than 5.10.</p>
<p>The following steps were tested on a Graviton3 R7g.4xlarge and Ubuntu 24.04.1</p>
<pre><code># For Kernel 5.10 based AMIs list
aws ec2 describe-images --owners amazon --filters "Name=architecture,Values=arm64" "Name=name,Values=*kernel-5.10*" --query 'sort_by(Images, &amp;CreationDate)[].Name'

# For Kernel 6.x based AMIs list
aws ec2 describe-images --owners amazon --filters "Name=architecture,Values=arm64" "Name=name,Values=*kernel-6.*" --query 'sort_by(Images, &amp;CreationDate)[].Name'
</code></pre>
<p><strong>Install Compiler and Python packages</strong></p>
<pre><code>sudo apt-get update  -y
sudo apt-get install -y gcc-13 g++-13 libnuma-dev python3-dev python3-virtualenv
</code></pre>
<p><strong>Create a new Python environment</strong></p>
<pre><code>virtualenv venv
source venv/bin/activate
</code></pre>
<p><strong>Clone vLLM project</strong></p>
<pre><code>git clone https://github.com/vllm-project/vllm.git
cd vllm
</code></pre>
<p><strong>Install Python Packages and build vLLM CPU Backend</strong></p>
<pre><code>pip install --upgrade pip
pip install "cmake&gt;=3.26" wheel packaging ninja "setuptools-scm&gt;=8" numpy
pip install -v -r requirements/cpu.txt --extra-index-url https://download.pytorch.org/whl/cpu

VLLM_TARGET_DEVICE=cpu python setup.py install
</code></pre>
<p><strong>Run DeepSeek Inference on AWS Graviton</strong></p>
<pre><code>export VLLM_CPU_KVCACHE_SPACE=40

vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B

curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{ "model": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B", "messages": [{"role": "system", "content": "You are a helpful assistant."},{"role": "user", "content": "Why is the sky blue?"}],"max_tokens": 100 }'
</code></pre>
<p>Sample output is as below.</p>
<pre><code>{"id":"chatcmpl-4c95b14ede764ab4a1338b0670ea839a","object":"chat.completion","created":1741351310,"model":"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B","choices":[{"index":0,"message":{"role":"assistant","reasoning_content":null,"content":"Okay, so I'm trying to understand why the sky appears blue. I've heard this phenomenon before, but I'm not exactly sure how it works. I think it has something to do with the mixing of light and particles in the atmosphere, but I'm not entirely clear on the details. Let me try to break it down step by step.\n\nFirst, there are a few factors contributing to why the sky looks blue. From what I remember, the atmosphere consists of gases and particles that absorb and refract light. Different parts of the sky are observed through different atmospheric layers, which might explain why the colors vary over long distances.\n\nI think the primary reason is that the atmosphere absorbs some of the red and blue light scattered from the sun. Red light has a longer wavelength compared to blue, so it is absorbed more easily because the molecules in the atmosphere absorb light based on its wavelength. Blue light has a shorter wavelength and doesn't get absorbed as much. As a result, the sky remains relatively blue because the blue light that hasn't been absorbed is still passing through the atmosphere and is refracted.\n\nAnother factor to consider is the angle of observation. Because the atmosphere travels through the sky, the observation of blue light can be messy at altitudes where the atmosphere is thinner.","tool_calls":[]},"logprobs":null,"finish_reason":"length","stop_reason":null}],"usage":{"prompt_tokens":17,"total_tokens":273,"completion_tokens":256,"prompt_tokens_details":null},"prompt_logprobs":null}
</code></pre>
<h1 id="additional-resources-2"><a class="header" href="#additional-resources-2">Additional Resources</a></h1>
<p>https://learn.arm.com/learning-paths/servers-and-cloud-computing/vllm/vllm-server/
https://docs.vllm.ai/en/latest/getting_started/installation/cpu/index.html?device=arm</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="ml-inference-on-graviton-cpus-with-open-neural-network-exchangeonnx"><a class="header" href="#ml-inference-on-graviton-cpus-with-open-neural-network-exchangeonnx">ML inference on Graviton CPUs with Open Neural Network Exchange(ONNX)</a></h1>
<p><strong>Introduction</strong></p>
<p>ONNX is an open-source machine learning framework that provides interoperability between different frameworks. ONNX Runtime is the runtime engine used for model inference and training with ONNX . This document covers how to use ONNX based machine learning inference on Graviton CPUs, what runtime configurations are important and how to run benchmarking scripts. The document also covers instructions for source builds to enable experimenting with downstream features.</p>
<h1 id="how-to-use-onnx-on-graviton-cpus"><a class="header" href="#how-to-use-onnx-on-graviton-cpus">How to use ONNX on Graviton CPUs</a></h1>
<p>Python wheels is the recommended option to use ONNX on Graviton, the default backend is optimized for Graviton CPU.</p>
<pre><code># Upgrade pip3 to the latest version
python3 -m pip install --upgrade pip

# Install ONNX and ONNX Runtime
python3 -m pip install onnx
python3 -m pip install onnxruntime
</code></pre>
<h1 id="prerequisites-2"><a class="header" href="#prerequisites-2">Prerequisites</a></h1>
<ol>
<li>It is highly recommended to use the AMIs based on Linux Kernel 5.10 and beyond for the best ONNX inference performance on Graviton3 instances. The following queries can be used to list down the AMIs with Kernel 5.10 and beyond.</li>
</ol>
<pre><code># For Kernel 5.10 based AMIs list
aws ec2 describe-images --owners amazon --filters "Name=architecture,Values=arm64" "Name=name,Values=*kernel-5.10*" --query 'sort_by(Images, &amp;CreationDate)[].Name'

# For Kernel 6.x based AMIs list
aws ec2 describe-images --owners amazon --filters "Name=architecture,Values=arm64" "Name=name,Values=*kernel-6.*" --query 'sort_by(Images, &amp;CreationDate)[].Name'
</code></pre>
<h1 id="runtime-configurations-for-optimal-performance-2"><a class="header" href="#runtime-configurations-for-optimal-performance-2">Runtime configurations for optimal performance</a></h1>
<p>Graviton3(E) (e.g. c7g/m7g/r7g, c7gn and Hpc7g instances) supports BFloat16 format and advanced Matrix Multiplication (MMLA) instructions for ML acceleration. Starting version v1.17.0, ONNX Runtime supports Bfloat16 accelerated SGEMM kernels and INT8 MMLA accelerated Quantized GEMM (QGEMM) kernels on Graviton3(E) CPU.</p>
<p>Note: The standard FP32 model inference can be accelerated with BFloat16 SGEMM kernels without model quantization.</p>
<p>MMLA QGEMM kernels are enabled by default, and to enable BF16 acceleration, set the onnxruntime session option as shown below</p>
<pre><code class="language-c++"># For C++ applications
SessionOptions so;
so.config_options.AddConfigEntry(
      kOrtSessionOptionsMlasGemmFastMathArm64Bfloat16, "1");
</code></pre>
<pre><code class="language-python"># For Python applications
sess_options = onnxruntime.SessionOptions()
sess_options.add_session_config_entry("mlas.enable_gemm_fastmath_arm64_bfloat16", "1")
</code></pre>
<h1 id="evaluate-performance-with-onnx-runtime-benchmark"><a class="header" href="#evaluate-performance-with-onnx-runtime-benchmark">Evaluate performance with ONNX Runtime benchmark</a></h1>
<p>ONNX Runtime repo provides inference benchmarking scripts for transformers based language models. The scripts support a wide range of models, frameworks and formats. The following section explains how to run BERT, RoBERTa and GPT model inference in fp32 and int8 quantized formats. Refer to <a href="https://github.com/microsoft/onnxruntime/blob/main/onnxruntime/python/tools/transformers/benchmark.py">ONNX Runtime Benchmarking script</a> for more details.</p>
<pre><code># Install onnx and onnx runtime
python3 -m pip install onnx onnxruntime

# Install the dependencies
python3 -m pip install transformers torch psutil

# Clone onnxruntime repo to get the benchmarking scripts
git clone --recursive https://github.com/microsoft/onnxruntime.git
cd onnxruntime/onnxruntime/python/tools/transformers

# The scripts download the models, export them to onnx format,
# quantize into int8 for int8 inference, run inference for
# different sequence lengths and batch sizes. Upon successful run,
# the scripts print the inference throughput in QPS (Queries/sec)
# and latency in msec along with system configuration

# Next run the benchmarks, select fp32 or int8 precision via -p argument
# To run bert-large
python3 benchmark.py -m bert-large-uncased -p &lt;fp32/int8&gt;

# To run bert-base
python3 benchmark.py -m bert-base-cased -p &lt;fp32/int8&gt;

# To run roberta-base
python3 benchmark.py -m roberta-base -p &lt;fp32/int8&gt;

# To run gpt2
python3 benchmark.py -m gpt2 -p &lt;fp32/int8&gt;

</code></pre>
<h1 id="building-onnx-runtime-from-source"><a class="header" href="#building-onnx-runtime-from-source">Building ONNX RunTime from source</a></h1>
<p>We recommend using the official python wheel distribution, but there are cases developers may want to compile ONNX Runtime from source. One such case would be to experiment with new features or develop custom features. This section outlines the recommended way to compile ONNX Runtime from source.</p>
<pre><code># Clone onnxruntime
git clone --recursive https://github.com/Microsoft/onnxruntime.git
cd onnxruntime

# Install cmake-3.27 or higher, one option is via pip installer.
python3 -m pip install cmake

# Build python wheel with Release configuration
./build.sh --config=Release --build_shared_lib --build_wheel --parallel 16

# the wheel is copied to build/Linux/Release/dist folder, so, to install
pip3 install &lt;build/Linux/Release/dist/onnxruntime_dnnl*.whl&gt;
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="graviton-performance-runbook"><a class="header" href="#graviton-performance-runbook">Graviton Performance Runbook</a></h1>
<h2 id="introduction-1"><a class="header" href="#introduction-1">Introduction</a></h2>
<p>This document is a reference for software developers who want to benchmark, debug, and optimize their application code on AWS Graviton based instances.  It contains checklists, best practices, examples, and tooling collected by the EC2 Graviton team to assist with the tasks of benchmarking, debugging, or optimizing code on Graviton.</p>
<p>This document covers many topics including how to benchmark, how to debug performance and which optimization recommendations.  It is not meant to be read beginning-to-end. Instead view it as a collection of checklists and best known practices to apply when working with Graviton instances that go progressively deeper into analyzing the system.  Please see the FAQ below to direct you towards the most relevant set of checklists and tools depending on your specific situation.</p>
<p>If after following these guides there is still an issue you cannot resolve with regards to performance on Graviton based instances, please do not hesitate to raise an issue on the <a href="https://github.com/aws/aws-graviton-getting-started/issues">AWS-Graviton-Getting-Started</a> guide or contact us at <a href="mailto:ec2-arm-dev-feedback@amazon.com">ec2-arm-dev-feedback@amazon.com</a>.  If there is something missing in this guide, please raise an issue or better, post a pull-request.</p>
<h2 id="pre-requisites"><a class="header" href="#pre-requisites">Pre-requisites</a></h2>
<p>To assist with some of the tasks listed in this runbook, we have created some helper-scripts for some of the tasks the checklists describe.  The helper-scripts assume the test instances are running an up-to-date AL2, AL2023 or Ubuntu 20.04LTS/22.04LTS distribution and the user can run the scripts using <code>sudo</code>. Follow the steps below to obtain and install the utilities on your test systems:</p>
<pre><code class="language-bash"># Clone the repository onto your systems-under-test and any load-generation instances
git clone https://github.com/aws/aws-graviton-getting-started.git
cd aws-graviton-getting-started/perfrunbook/utilities

# On AL2 or Ubuntu distribution
sudo ./install_perfrunbook_dependencies.sh

# All scripts expect to run from the utilities directory
</code></pre>
<h2 id="aperf-for-performance-analysis"><a class="header" href="#aperf-for-performance-analysis">APerf for performance analysis</a></h2>
<p>There is also a new tool aimed at helping move workloads over to Graviton called <a href="https://github.com/aws/aperf">APerf</a>, it bundles many of the capabilities of the individual tools present in this
runbook and provides a better presentation.  It is highly recommended to download this tool and use it to gather most of the same information in one test-run.</p>
<h2 id="sections"><a class="header" href="#sections">Sections</a></h2>
<ol>
<li><a href="perfrunbook/./intro_to_benchmarking.html">Introduction to Benchmarking</a></li>
<li><a href="perfrunbook/./defining_your_benchmark.html">Defining your benchmark</a></li>
<li><a href="perfrunbook/./configuring_your_loadgen.html">Configuring your load generator</a></li>
<li><a href="perfrunbook/./configuring_your_sut.html">Configuring your system-under-test environment</a></li>
<li>Debugging Performance
<ol>
<li><a href="perfrunbook/./debug_system_perf.html">Debugging performance — “What part of the system is slow?”</a></li>
<li><a href="perfrunbook/./debug_code_perf.html">Debugging performance — “What part of the code is slow?”</a></li>
<li><a href="perfrunbook/./debug_hw_perf.html">Debugging performance — “What part of the hardware is slow?”</a></li>
</ol>
</li>
<li><a href="perfrunbook/./optimization_recommendation.html">Optimizing performance</a></li>
<li><a href="perfrunbook/./appendix.html">Appendix — Additional resources</a></li>
<li><a href="perfrunbook/./references.html">References</a></li>
</ol>
<h2 id="faq"><a class="header" href="#faq">FAQ</a></h2>
<ul>
<li><strong>I want to benchmark Graviton but I have yet to port my application, where do I find information on helping port my application?</strong>
Our getting-started-guide has many resources to help with porting code to Graviton for a number of programming languages.  Start by reading those guides to understand minimum runtime, dependency and language requirements needed for Graviton.</li>
<li><strong>What benchmark should I run to determine if my application will be a good fit on Graviton?</strong>
No synthetic benchmark is a substitute for your actual production code.  The best benchmark is running your production application on Graviton with a load that approximates your production load.  Please refer to <a href="perfrunbook/./intro_to_benchmarking.html">Section 1</a> and <a href="perfrunbook/./defining_your_benchmark.html">2</a> on how to benchmark and on selecting a benchmark for more details.</li>
<li><strong>I ran micro-benchmark X from github project Y and it shows Graviton has worse performance, does that mean my application is not a good fit?</strong>
No.  Benchmarks only tell a limited story about performance, and unless this particular benchmark has been vetted as a good indicator for your application’s performance, we recommend running your production code as its own benchmark.  For more details, refer to <a href="perfrunbook/./intro_to_benchmarking.html">Section 1</a> and <a href="perfrunbook/./defining_your_benchmark.html">2</a> on how to define experiments and test Graviton for your needs.</li>
<li><strong>I benchmarked my service and performance on Graviton is slower compared to my current x86 based fleet, where do I start to root cause why?</strong>
Begin by verifying software dependencies and verifying the configuration of your Graviton and x86 testing environments to check that no major differences are present in the testing environment.  Performance differences may be due to differences in environment and not the due to the hardware.  Refer to the below chart for a step-by-step flow through this runbook to help root cause the performance regression:
<img src="perfrunbook/./images/performance_debug_flowchart.png" alt="" /></li>
<li><strong>What are the recommended optimizations to try with Graviton?</strong>
Refer to <a href="perfrunbook/./optimization_recommendation.html">Section 6</a> for our recommendations on how to make your application run faster on Graviton.</li>
<li><strong>I investigated every optimization in this guide and still cannot find the root-cause, what do I do next?</strong>
Please contact us at <a href="mailto:ec2-arm-dev-feedback@amazon.com">ec2-arm-dev-feedback@amazon.com</a> or talk with your AWS account team representative to get additional help.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="quick-introduction-to-benchmarking"><a class="header" href="#quick-introduction-to-benchmarking">Quick introduction to benchmarking</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>When designing an experiment to benchmark Graviton based instances against another instance type, it is key to remember the below 2 guiding principles:</p>
<ol>
<li>Always define a specific question to answer with your benchmark</li>
<li>Control your variables and unknowns within the benchmark environment</li>
</ol>
<h3 id="ask-the-right-question"><a class="header" href="#ask-the-right-question">Ask the right question</a></h3>
<p>The first bullet point is the most important; without a specific question to answer, any benchmarking data gathered will be hard to interpret and provide limited conclusions.  For example, a poor question is this:</p>
<blockquote>
<p>“How do Graviton instances compare to x86 based instances on Java?”</p>
</blockquote>
<p>This type of question is impossible to answer.  The primary problem is that there are too many variables present in that simple question, for instance: What Java version? What application? What load? What is the metric of interest?  What OS distribution? What software dependencies?  Because the scope of the question is so vast, any data gathered and any conclusion made from it will be immediately questioned and discarded as simple outliers from outside reviewers.  Leading to a great deal of wasted effort.</p>
<p>Instead formulate the question as tightly as possible:</p>
<blockquote>
<p>“How does a Graviton instance’s request throughput compare to current instances on my Java application at a P99 of 10ms for a mix of 60% GETS and 40% PUTS on Ubuntu 20.04LTS?</p>
</blockquote>
<p>This question is narrower in scope and defines a specific experiment that can be answered conclusively without a mountain of data.  Always ask specific questions so as to get specific answers and conclusions.</p>
<h3 id="less-moving-parts"><a class="header" href="#less-moving-parts">Less moving parts</a></h3>
<p>The second bullet follows from the first.  Once a specific question is posed for an experiment, then it is required to account for variables in the experimental environment as much as possible.</p>
<p>Variables for benchmarking an application can include:</p>
<ul>
<li>OS distribution</li>
<li>Linux kernel version</li>
<li>software dependency versions</li>
<li>instance size used</li>
<li>network placement group configuration</li>
<li>application setup</li>
<li>background daemons</li>
<li>traffic profiles</li>
<li>load generator behavior</li>
<li>etc.</li>
</ul>
<p>The more variables that are controlled for, more specific questions can be answered about the system.  Such as, if an instance is over-provisioned with disk and network bandwidth and it is known that this configuration will not pose a bottleneck, then experiments can be derived to test only the capability of the CPU and DRAM in the system.</p>
<p>It is recommended before running an experiment to fully understand all the ways the environment can vary and determine how and if they can be controlled for.  The above list can be used as a starting point.  Having a thorough understanding of all the variables present in an experiment will enable better analysis of results and reduce the number of experimental runs needed before settling on the final configurations that will enable performance debugging.</p>
<p>Now that you have a specific question to answer, and have a basic understanding of the variables to control for, lets get started with defining how to test your application to assist with debugging performance in <a href="perfrunbook/./defining_your_benchmark.html">Section 2.</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="system-load-and-compute-headroom"><a class="header" href="#system-load-and-compute-headroom">System Load and Compute Headroom</a></h1>
<p>Ideally, a system's performance should scale linearly as more vCPUs are used, up to 100% CPU load.
This ideal is not always realized due to how vCPUs are implemented on x86 and Graviton based EC2 instances.
Our x86 based EC instances in AWS EC2 execute two hardware threads per core, each thread representing a vCPU.
Graviton processors can execute only one hardware thread, or vCPU, per physical core.
The Linux kernel schedules tasks to cores first, then, when all physical cores are used, to hardware threads.
When hardware threads have to share one physical core, the per-thread performance decreases significantly.
This leads to x86 and Graviton systems to scale differently under increasing CPU load.
We recommend to always load test your application on Graviton to determine the CPU load limits your application
needs to maintain for quality of service, fail-over resilience etc., as they will likely be different between Graviton and x86.
To illustrate how Graviton behaves differently relative to CPU load, we provide two short case-studies below.</p>
<h3 id="experimental-setup"><a class="header" href="#experimental-setup">Experimental Setup</a></h3>
<p>The test systems are limited to eight cores to limit the maximum packet load needed to saturate them.
This is to exclude the influence of the networking infrastructure on the experiment.
All systems run Amazon Linux 2 with Linux kernel 5.10.118.
The test system are virtualized instances that use EBS networked storage.</p>
<h4 id="first-experiment"><a class="header" href="#first-experiment">First experiment</a></h4>
<p>The systems to be tested are an 8 vCPU Intel instance, c6i.2xlarge, and an 8 vCPU Graviton 3 instance, c7g.2xlarge.
OpenSSL source is placed and configured in eight separate directories.
The test then compiles OpenSSL one to eight times in parallel, limited to one vCPU per compile.
An ideal system would take the exact same time for each compile run.</p>
<p>Results:</p>
<p>The Graviton 3 achieves 96% of the single compile time performance, single CPU baseline when using 100% of the available CPUs.
The Intel system achieves 63% of the compile time performance of the single compile, single CPU, baseline when loaded to 100%.</p>
<p><img src="perfrunbook/images/system-load/c7g-compared-to-c6i.png" alt="" /></p>
<p>Note:
The average compile time of the c6i instance is 268s, close to the 250s it took on c7g.
The Linux scheduler picks cores over threads, when available, which shows nicely in the steep increase in compile time of the c6i beyond 50% load.
For the c6i to maintain the same response time as the c7g, it would need more vCPUs. It can stay on par until 5 of the 8 vCPUs are used.</p>
<h4 id="second-experiment"><a class="header" href="#second-experiment">Second experiment</a></h4>
<p>The second experiment adds a 64 core Graviton 3, c7g.16xlarge, that serves as a load generator running wrk2.
Test systems are an 8 CPU c7g.2xlarge Graviton 3 and an 8 vCPU c6i.2xlarge Intel Xeon.
Here a simple HTTP server, implemented using the Netty/Java framework, is the workload on the systems under test.
Latency vs actual packets processed is captured alongside the CPU load of the systems, as reported by /proc/stat.
Of particular interest is the performance at 50% and at maximum sustained load.
Maximum sustained load is where the response latency is no longer than two times the response time at &lt;20% CPU load.
This is the reason why the plots stop shy of 100%.</p>
<p>Results:</p>
<p><img src="perfrunbook/images/system-load/c7g.png" alt="" /></p>
<p>For this workload, the non-SMT Graviton 3 system performs better than linear:
644506 packet/s would be expected as maximum throughput but 736860 packets/s where handled, thus 14% better than expected.</p>
<p><img src="perfrunbook/images/system-load/c6i.png" alt="" /></p>
<p>The Intel SMT system degrades when loaded close to 100%. It achieves 86% of the performance it should have shown, based on the 50% number: 690276 packet/s would be expected, 591411 packets/s where actually handled.
At the 50% CPU load mark, a c6i can handle an additional 71% of the traffic it did up to 50% (345k,246k), whereas the c7g is able to serve another 130% of that (314k,423k).
The c6i would need 2 additional vCPUs to be on par with c7g in packets/s.</p>
<h4 id="conclusion"><a class="header" href="#conclusion">Conclusion</a></h4>
<p>Graviton instances compute performance increases near-linearly with CPU load, x86 performance increases less after 50% CPU load. This is because our x86 based EC2 instances employ symmetric multithreading, aka. 'Hyperthreading'. Based on the above, load balancer thresholds can, in many cases, be set higher on Graviton instances than on x86-type instances and thus lead to significant savings in the size of the required server fleet, as the Netty example shows.
Since every workload has different demands on the system, a full load sweep should done to determine best system type and at which threshold additional instances need to be added to maintain performance.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="defining-your-benchmark"><a class="header" href="#defining-your-benchmark">Defining your benchmark</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>To define a benchmark there are two things to consider, the software running on the System-under-test (SUT) and how to drive load.  We recommend the software running on the SUT should be your production application. There is no better benchmark to predict performance than the actual production code.  If a synthetic proxy must be used to break dependencies of your application on external services such as authentication layers, then that proxy should be derived from the production code as much as possible.  We recommend avoiding synthetic benchmarks not related to the production code.  They are generally poor at predicting performance for another application or helping optimize it as they can over-target specific attributes of a system or exercise different bottlenecks than your application code might.</p>
<p>Once the code to test is selected, the proper traffic load to the SUT must be defined.  There are three primary ways to drive load to measure performance and also to debug performance issues: via a synthetic load generator, live traffic redirection, or live traffic duplication. After determining how to send traffic to your SUT, the next step is to determine the load point for the SUT.  Picking the proper load point will help with evaluating and debugging your application code on Graviton.  For most cases, the proper load point should be what is defined as peak load.  Below we describe two methods to pick the proper load point: the maximum number of operations per second or maximum throughput, and breaking latency.</p>
<h2 id="maximum-throughput"><a class="header" href="#maximum-throughput">Maximum throughput</a></h2>
<p>Maximum throughput tests the limits of the SUT, with no regard for response latency.  To test maximum throughput, have the load-generator increase load until all vCPUs on the SUT are operating at 100% with as little OS idle time measured as possible. This can be verified by using <code>sysstat</code> or  <code>htop</code> to see CPU utilization and by measuring at what settings on the load generator achieve the maximum score. If reaching 100% CPU utilization on the SUT is not possible, then find the peak throughput point reported with the load generator and record the settings used. If there are any bottlenecks present, they will be magnified when running at maximum throughput. After each optimization attempted, the maximum throughput point will have to re-evaluated.</p>
<h2 id="throughput-at-breaking-latency"><a class="header" href="#throughput-at-breaking-latency">Throughput at breaking latency</a></h2>
<p>If latency and throughput have to be balanced, then a different methodology is needed.  In this case it is important to look for the breaking latency.  Breaking latency is the point when the machine can no longer serve more throughput and maintain acceptable response times to the load-generator and incrementally more throughput induces an exponential rise in latency.  An example of that exponential rise is below.</p>
<p><img src="perfrunbook/images/example_breaking_latency_chart.png" alt="" /></p>
<p>To find breaking latency, configure the load-generator to incrementally increase load while measuring latency and/or response failures to find the knee point in the latency/failure/throughput curve before it starts an exponential rise indicating the saturation point of the SUT. This usually requires a binary or linear search to refine where this point is located for each SUT.  Once these points are found, compare them and continue to use them for further experiments and debugging. After each successive optimization attempted, the saturation throughput point will have to be re-evaluated, and the goal is to shift the curve down and to the right, indicating more throughput and lower latency.</p>
<h2 id="synthetic-load-generators"><a class="header" href="#synthetic-load-generators">Synthetic load generators</a></h2>
<p>If you choose to use a synthetic load generator to drive your application, but need help finding a good candidate to use, please see the <a href="perfrunbook/./appendix.html">Appendix</a> to learn about the types of load generators and what we recommend to use.</p>
<p>With your benchmark fully defined, it is now time to move on to <a href="perfrunbook/./configuring_your_loadgen.html">Section 3</a>, and start verifying the test environment and accounting for any variables present.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuring-your-load-generator"><a class="header" href="#configuring-your-load-generator">Configuring your load generator</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>The load generator setup is important to understand and verify: it generates the load that is expected.  An unknown load-generation setup can lead to not measuring the expected experiment and getting results that are hard to interpret. Below is a checklist to step through and verify the load generator is working as expected.</p>
<ol>
<li>Ensure the load generator instance is large enough for driving traffic to the Systems-under-test (SUTs), we recommend using 12xl instances or larger.</li>
<li>When generating load, verify the load-generator instance is not using 100% CPU for load-generators that use blocking IO.  For load-generators that busy poll, verify that it is spending ~50% of its time in the busy poll loop by utilizing <code>perf</code>  to verify where the load generator code is spending time. See <a href="perfrunbook/./debug_code_perf.html">Section 5.b</a> on how to generate CPU time profiles to verify this.</li>
<li>If the load-generator is close to its limit, measurements taken may be measuring the load-generator's ability to generate load and not the SUT's ability to process that load.  A load-generator that is spending less than 50% of its time generating load is a good target to ensure you are measuring the SUT.</li>
<li>When generating load, verify the load-generator is receiving valid responses and not a large number of errors from the SUT.  For example, the example below shows the <a href="https://github.com/giltene/wrk2">Wrk2</a> load generator receiving many errors, meaning the test point is invalid:</li>
</ol>
<pre><code class="language-bash">Running 30s test @ http://127.0.0.1:80/index.html
2 threads and 100 connections
Thread calibration: mean lat.: 9747 usec, rate sampling interval: 21 msec
  Thread calibration: mean lat.: 9631 usec, rate sampling interval: 21 msec
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency     6.46ms    1.93ms  12.34ms   67.66%
    Req/Sec     1.05k     1.12k    2.50k    64.84%
  60017 requests in 30.01s, 19.81MB read
  Non-2xx or 3xx responses: 30131 
Requests/sec:   2000.15
Transfer/sec:    676.14KB
</code></pre>
<ol start="5">
<li>Check <code>dmesg</code> and logs in <code>/var/log</code> on the SUT and load generator for indications of errors occurring when under load</li>
<li>Verify load generators and SUTs are physically close to remove sensitivities to RTT (Round-Trip-Times) of messages. Differences in RTT from a load-generator to SUTs can show up as lower throughput, higher latency percentiles and/or increase in error rates, so it is important to control this aspect for testing. Check the following aspects in the order provided to verify your network setup follows best practices and reduces the influence of network factors on test results:
<ol>
<li>Verify on EC2 console <em>Placement Group</em> is filled in with the same placement for the SUTs and load generators and that placement group is set to <code>cluster</code>.</li>
<li>Check in the EC2 console that all load generators are in the same subnet (i.e. us-east-1a)</li>
<li>Run <code>ping</code> from your SUTs to the load generators and any back-end services your SUT will communicate with. Verify the average latencies are similar (10s of micro-seconds).  You can also use <code>traceroute</code> to see the number of hops between your instances as well, ideally it should be 3 or less.</li>
</ol>
</li>
<li>Check for ephemeral port exhaustion that may prevent load-generator from driving the desired traffic load:</li>
</ol>
<pre><code class="language-bash"># Check on both the load generator/SUT machines
%&gt; netstat -nt | awk '/ESTABLISHED|TIME_WAIT|FIN_WAIT2|FIN_WAIT1|CLOSE_WAIT/ {print $6}' | sort | uniq -c | sort -n
30000 TIME_WAIT
  128 ESTABLISHED

# If large numbers of connections are in TIME_WAIT, and number of ESTABLISHED is decreasing,
# port exhaustion is happening and can lead to decrease in driven load.  Use the below tips to
# fix the issue.

# Increase ephemeral port range on load generator/SUT
%&gt; sudo sysctl -w net.ipv4.ip_local_port_range 1024 65535
# If you application uses IPv6
%&gt; sudo sysctl -w net.ipv6.ip_local_port_range 1024 65535

# Allow kernel to re-use connections in load generator/SUT
%&gt; sysctl -w net.ipv4.tcp_tw_reuse=1
</code></pre>
<ol start="8">
<li>Check connection rates on SUT.  Do you see constant rate of new connections or bursty behavior? Does it match the expectations for the workload?</li>
</ol>
<pre><code class="language-bash"># Terminal on load-generator instance
%&gt; &lt;start test&gt;

# Terminal on SUT
%&gt; cd ~/aws-graviton-getting-started/perfrunbook/utilities
%&gt; python3 ./measure_and_plot_basic_sysstat_stats.py --stat new-connections --time 60
</code></pre>
<ol start="9">
<li>If higher than expected connections/s are being observed, the cause of these new connections can be determined by looking at a packet trace and determining which end is initiating and closing the connections.</li>
</ol>
<pre><code class="language-bash"># On load-generator
%&gt; &lt;start test&gt;
      
# On load-generator
# Grab tcpdump from network device that will recieve traffic, likely 
# eth0, but check your configuration.
%&gt; tcpdump -i eth&lt;N&gt; -s 128 -w dump.pcap
%&gt; &lt;stop tcpdump using Ctrl-C&gt;
</code></pre>
<ol start="10">
<li>Open the trace using a tool like <a href="https://www.wireshark.org/#download">Wireshark</a>.</li>
<li>Look for which side is closing connections unexpectedly by looking for <code>Connection: Close</code> in the HTTP headers, or <code>FIN</code> in the TCP headers.  Identify which system is doing this more than expected.</li>
<li>Verify setup of the SUT and/or load generator for connection establishment behavior.</li>
<li>Check packet rate, is it behaving as expected? i.e. constant rate of traffic, or bursty.</li>
</ol>
<pre><code class="language-bash"># Load generator terminal #1
%&gt; &lt;start load generator or benchmark&gt;
  
# Terminal #2 on load generator
%&gt; cd ~/aws-graviton-getting-started/perfrunbook/utilities
%&gt; python3 ./measure_and_plot_basic_sysstat_stats.py --stat tcp-out-segments --time 60
%&gt; python3 ./measure_and_plot_basic_sysstat_stats.py --stat tcp-in-segments --time 60
</code></pre>
<ol start="14">
<li>Check for hot connections (i.e. connections that are more heavily used that others) by running: <code>watch netstat -t</code>. The below example shows the use of <code>netstat -t</code> to watch multiple TCP connections. One connection is active and has a non-zero <code>Send-Q</code> value while all other connections have a <code>Send-Q</code> value of 0.</li>
</ol>
<pre><code class="language-bash">%&gt; watch netstat -t
Every 2.0s: netstat -t
ip-172-31-9-146: Tue Jan 12 23:01:35 2021
    
Active Internet connections (w/o servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State
tcp        0      0 ip-172-31-9-146.ec2:ssh 72-21-196-67.amaz:62519 ESTABLISHED
tcp        0 345958 ip-172-31-9-146.ec2:ssh 72-21-196-67.amaz:25884 ESTABLISHED
tcp        0      0 ip-172-31-9-146.ec2:ssh 72-21-196-67.amaz:18144 ESTABLISHED
</code></pre>
<ol start="15">
<li>Is the behavior expected?  If not, re-visit load-generator configuration.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuring-your-system-under-test-environment"><a class="header" href="#configuring-your-system-under-test-environment">Configuring your system-under-test environment</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>This section documents multiple checklists to use to verify your Graviton System-under-test (SUT) is up-to-date and as code-equivalent as possible to the systems and instances you are comparing against.   Please perform these tests on each SUT to vet your experimental setup and eliminate as many potential unknown variables as possible.</p>
<h2 id="initial-system-under-test-checks"><a class="header" href="#initial-system-under-test-checks">Initial system-under-test checks</a></h2>
<p>If you have more than one SUT, first verify there are no major differences in setup:</p>
<ol>
<li>Check all instances are running the same OS distribution:</li>
</ol>
<pre><code class="language-bash">## AL2
%&gt; sudo cat /etc/system-release
Amazon Linux release 2 (Karoo)
  
## Ubuntu
%&gt; sudo lsb_release -a
Distributor ID: Ubuntu
Description:    Ubuntu 20.04.2 LTS
Release:        20.04
Codename:       focal
</code></pre>
<ol start="2">
<li>Update your system packages to the latest for your AMI to pull in the most recent security and bug fixes.</li>
</ol>
<pre><code class="language-bash">## AL2
%&gt; sudo yum update
  
## Ubuntu
%&gt; sudo apt-get upgrade
</code></pre>
<ol start="3">
<li>Check all instances are running the same major and minor kernel versions.  It is recommended to upgrade to the newest kernel available to bring in the latest security and bug fixes.</li>
</ol>
<pre><code class="language-bash"># Example output on x86 SUT
%&gt; uname -r
4.14.219-161.340.amzn2.x86_64
  
# Example output on Graviton SUT
%&gt; uname -r
5.10.50-45.132.amzn2.aarch64
  
# To upgrade on AL2 for example to Linux 5.10:
%&gt; sudo amazon-linux-extras enable kernel-5.10
%&gt; sudo yum install kernel
  
# To update on Ubuntu
%&gt; sudo apt-cache search "linux-image"
# Find the newest kernel available
%&gt; sudo apt-get install linux-image-&lt;version&gt;-generic
  
# Restart your instance
%&gt; sudo reboot now
</code></pre>
<ol start="4">
<li>Check for suspicious error, exception or warning messages in system and service logs that are different between instances.</li>
</ol>
<pre><code class="language-bash"># Grep in all log locations, on most systems this is /var/log, but there
# may be additional log locations.
%&gt; egrep -Rni "ERROR|WARN|EXCEPTION" /var/log/*
# Example output
exampleservice.log:1000:`LoadError: Unsupported platform: unknown-linux
...
</code></pre>
<ol start="5">
<li>Check per process limitations on all systems under test are identical by running:</li>
</ol>
<pre><code class="language-bash">%&gt; ulimit -a
# Pay special attention to stacksize and open files limits
# If they are different, they can be changed temporarily.
# For example to change number of files allowed open:
%&gt; ulimit -n 65535
  
# Permanently changing the values requires editing /etc/security/limits.conf
</code></pre>
<ol start="6">
<li>Check ping latencies to load generators and downstream services that will be accessed from each system-under-test and verify latencies are similar.   A different of +/-50us is acceptable, differences of &gt;+/-100us can adversely affect testing results.  We recommend putting all testing environment instances inside a <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html#placement-groups-cluster">cluster placement group</a>, or at a minimum confirm that all instances are in the same subnet (i.e. us-east-1a).</li>
<li>Check the instance types used and verify IO devices are setup equivalently between the SUTs. I.e. m5d.metal and m6gd.metal have different disk configurations that may lead to differing performance measurements if your service is sensitive to disk performance.</li>
<li>We recommend using instances set to <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/dedicated-instance.html"><strong>dedicated tenancy</strong></a> first to establish a baseline for performance variance for your test application.  Knowing this baseline will help interpret results when testing on instances that are set to <strong>default</strong> tenancy.</li>
</ol>
<h2 id="check-for-missing-binary-dependencies"><a class="header" href="#check-for-missing-binary-dependencies">Check for missing binary dependencies</a></h2>
<p>Libraries for Python or Java can link in binary shared objects to provide enhanced performance.  The absence of these shared object dependencies does not prevent the application from running on Graviton, but the CPU will be forced to use a slow code-path instead of the optimized paths.  Use the checklist below to verify the same shared objects are available on all platforms.</p>
<ol>
<li>JVM based languages — Check for the presence of binary shared objects in the installed JARs and compare between Graviton and x86.</li>
</ol>
<pre><code class="language-bash">%&gt; cd ~/aws-getting-started-guide/perfrunbook/utilities
%&gt; sudo ./find_and_list_jar_with_so.sh
# Example output ...
# jars may place the shared object in different folders for the arch support
./com/sun/jna/linux-x86/libjnidispatch.so
./com/sun/jna/linux-x86-64/libjnidispatch.so
./com/sun/jna/linux-arm/libjnidispatch.so
./com/sun/jna/linux-armel/libjnidispatch.so
./com/sun/jna/linux-aarch64/libjnidispatch.so
./com/sun/jna/linux-ppc/libjnidispatch.so
./com/sun/jna/linux-ppc64le/libjnidispatch.so
# ... or append a suffix declaring the arch support
./META-INF/native/libnetty_tcnative_linux_x86_64.so
./META-INF/native/libnetty_tcnative_linux_aarch64.so
./META-INF/native/libnetty_transport_native_epoll_x86_64.so
./META-INF/native/libnetty_transport_native_epoll_aarch_64.so
# ... or very old jars will be x86 only if linux64/32 is specified
./META-INF/native/linux64/libjansi.so
./META-INF/native/linux32/libjansi.so
</code></pre>
<ol start="2">
<li>Python — Check for the presence of binary shared objects in your python version’s <code>site-packages</code> locations and compare between Graviton and x86:</li>
</ol>
<pre><code class="language-bash">%&gt; cd ~/aws-getting-started-guide/perfrunbook/utilites
%&gt; sudo ./find_and_list_pylib_with_so.sh 3.7 # takes python version as arg
# Example output ...
# ... Graviton
./numpy/core/_multiarray_tests.cpython-37m-aarch64-linux-gnu.so
./numpy/core/_struct_ufunc_tests.cpython-37m-aarch64-linux-gnu.so
./numpy/core/_rational_tests.cpython-37m-aarch64-linux-gnu.so
./numpy/core/_simd.cpython-37m-aarch64-linux-gnu.so
./numpy/core/_multiarray_umath.cpython-37m-aarch64-linux-gnu.so
./numpy/core/_umath_tests.cpython-37m-aarch64-linux-gnu.so
./numpy/core/_operand_flag_tests.cpython-37m-aarch64-linux-gnu.so
# ... x86
./numpy/core/_operand_flag_tests.cpython-37m-x86_64-linux-gnu.so
./numpy/core/_simd.cpython-37m-x86_64-linux-gnu.so
./numpy/core/_rational_tests.cpython-37m-x86_64-linux-gnu.so
./numpy/core/_umath_tests.cpython-37m-x86_64-linux-gnu.so
./numpy/core/_multiarray_tests.cpython-37m-x86_64-linux-gnu.so
./numpy/core/_multiarray_umath.cpython-37m-x86_64-linux-gnu.so
./numpy/core/_struct_ufunc_tests.cpython-37m-x86_64-linux-gnu.so
</code></pre>
<ol start="3">
<li>If you find any missing libraries, check for newer versions at:
<ol>
<li><a href="https://mvnrepository.com/">https://mvnrepository.com</a></li>
<li><a href="https://pypi.org/">https://pypi.org</a></li>
<li>Check the <a href="https://github.com/aws/aws-graviton-getting-started">AWS Graviton getting started guide</a> for additional guidance on known good versions of common libraries.</li>
</ol>
</li>
<li>Upgrade your dependencies to newer versions if able to get added support, but also security and bug fixes.</li>
</ol>
<h2 id="check-native-application-build-system-and-code"><a class="header" href="#check-native-application-build-system-and-code">Check native application build system and code</a></h2>
<p>For native compiled components of your application, proper compile flags are essential to make sure Graviton’s hardware features are being fully taken advantage of.  Follow the below checklist:</p>
<ol>
<li>Verify equivalent code optimizations are being made for Graviton as well as x86.  For example with C/C++ code built with GCC, make sure if builds use <code>-O3</code> for x86, that Graviton builds also use that optimization and not some basic debug setting like just <code>-g</code>.</li>
<li>Confirm when building for Graviton that <strong>one of the following flags</strong> are added to the compile line for GCC/LLVM12+ to ensure using Large System Extension instructions when able to speed up atomic operations.
<ol>
<li>Use <code>-moutline-atomics</code> for code that must run on all Graviton platforms</li>
<li>Use <code>-march=armv8.2-a -mcpu=neoverse-n1</code> for code that will run on Graviton2 or later and other modern Arm platforms</li>
</ol>
</li>
<li>When building natively for Rust, ensure that <code>RUSTFLAGS</code> is set to <strong>one of the following flags</strong>
<ol>
<li><code>export RUSTFLAGS="-Ctarget-features=+lse"</code> for code that will run on all Graviton2 and other Arm platforms that support LSE (Large System Extension) instructions.</li>
<li><code>export RUSTFLAGS="-Ctarget-cpu=neoverse-n1"</code> for code that will only run on Graviton2 and later platforms.</li>
</ol>
</li>
<li>Check for the existence of assembly optimized on x86 with no optimization on Graviton.  For help with porting optimized assembly routines, see <a href="perfrunbook/./optimization_recommendation.html">Section 6</a>.</li>
</ol>
<pre><code class="language-bash"># Check for any .S/.s files in a C/C++ application
find . -regex '.*\.[sS]' -type f -print
  
# Check for any inline assembly in files
egrep -Rn "__asm__|asm" *
  
# Search for intrinsics usage in the code base
egrep -Rn "arm_neon.h|mmintrin.h" *
</code></pre>
<h2 id="check-application-configuration-settings"><a class="header" href="#check-application-configuration-settings">Check application configuration settings</a></h2>
<p>Finally as part of checking the systems-under-test verify the application is configured properly on startup.</p>
<ol>
<li>Check that any startup scripts that attempt to do thread-pinning via <code>taskset</code> or <code>CGroups</code> are not making assumptions about the presence of SMT that is common on x86 servers.  There are no threads on Graviton servers and no assumptions about threads needs to be made.  This may be present as code taking the number of CPUs in the system and dividing by two, i.e. for shell run scripts you might see: <code>let physical_cores=$(nproc) / 2</code></li>
<li>Check daemon start scripts provide enough resources to the service as it starts on Graviton, such as specifying <code>LimitNOFILE</code>, <code>LimitSTACK</code>, or <code>LimitNPROC</code> in a systemd start script.</li>
<li>Check for debug flags in application start-up scripts that are enabled but should be disabled. Such as: <code>-XX:-OmitStackTraceInFastThrow</code> for Java which logs and generates stack traces for all exceptions, even if they are not considered fatal exceptions.</li>
<li>If using the Java Virtual Machine to execute your service, ensure it is a recent version based off at least JDK11. We recommend using <a href="https://docs.aws.amazon.com/corretto/latest/corretto-11-ug/downloads-list.html">Corretto11</a> or <a href="https://docs.aws.amazon.com/corretto/latest/corretto-15-ug/downloads-list.html">Corretto15</a>.  Corretto is a free and actively maintained OpenJDK distribution that contains optimizations for AWS Graviton based instances.</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="debugging-performance--what-part-of-the-system-is-slow"><a class="header" href="#debugging-performance--what-part-of-the-system-is-slow">Debugging performance — “What part of the system is slow?”</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>When debugging performance, start by measuring high level system behavior to pinpoint what part of the system performs differently when compared with a control instance.  Are the CPUs being saturated or under-saturated?  Is the network or disk behaving differently than expected?  Did a mis-configuration creep in that went undetected when validating the SUT application setup?</p>
<h2 id="check-cpu-usage"><a class="header" href="#check-cpu-usage">Check CPU-usage</a></h2>
<ol>
<li>Check the <code>cpu-iowait</code> time. High <code>cpu-iowait</code> indicates a bottleneck in disk operations. In this case, provision EBS volumes with more IOPs or use local disk instances to determine if the Graviton CPU is performing faster. Otherwise, high iowait time will lead to performance results that are similar between instances since disk operations, not CPU performance, is the bottleneck.</li>
</ol>
<pre><code class="language-bash"># Terminal one
%&gt; &lt;start load generator or benchmark&gt;
  
# Terminal two
%&gt; cd ~/aws-gravition-getting-started/perfrunbook/utilities
%&gt; python3 ./measure_and_plot_basic_sysstat_stats.py --stat cpu-iowait --time 60
</code></pre>
<ol start="2">
<li>Check the <code>cpu-user</code> and <code>cpu-kernel</code> time at your chosen load point. Check to see if Graviton2 has higher or lower cpu time  than the x86 comparison system-under-test.</li>
</ol>
<pre><code class="language-bash"># Terminal one
%&gt; &lt;start load generator or benchmark&gt;
  
# Terminal two
%&gt; cd ~/aws-gravition-getting-started/perfrunbook/utilities
%&gt; python3 ./measure_and_plot_basic_sysstat_stats.py --stat cpu-user --time 60
# To get kernel time, do cpu-kernel
  
# Example output
100 ++-----------------------+------------------------+------------------------+------------------------+------------------------+-----------------------++
    + ***       ***       ***+   *********         *******                     *** ****           **** ****           ****       +                      * +
    **   *******   *******   ****          ********       *  ***********      *   *    *****     *    *    *        **    ******    ***************    *  |
    |                                     *                **                *              *   *           *     **            *  *               *   *  |
    |                                                                   ** **                * *             *** *              * *                 * *   |
    |                                                                     *                   *                 *                *                   *    |
    |                                                                                                                                                     |
 80 ++                                                                                                                                                   ++
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
 60 ++                                                                                                                                                   ++
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
 40 ++                                                                                                                                                   ++
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
 20 ++                                                                                                                                                   ++
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    |                                                                                                                                                     |
    +                        +                        +                        +                        +                        +                        +
  0 ++-----------------------+------------------------+------------------------+------------------------+------------------------+-----------------------++
    0                        10                       20                       30                       40                       50                       60
                                                                           Time (s)
</code></pre>
<ol start="3">
<li>If CPU usage is higher or equal to the x86 system, proceed to profile for hot-functions in <a href="perfrunbook/./debug_code_perf.html">Section 5.b</a>.</li>
<li>If CPU usage is lower, proceed to <a href="perfrunbook/./debug_code_perf.html">Section 5.b</a> to profile which functions are putting threads to sleep and causing the CPU to go idle more than the x86 system.</li>
</ol>
<h2 id="check-system-memory-usage"><a class="header" href="#check-system-memory-usage">Check system memory usage</a></h2>
<p>It is also advisable to check memory consumption using <code>sysstat -r ALL</code> or <code>htop</code>.  Verify the system is not under memory pressure during testing.</p>
<h2 id="check-network-usage"><a class="header" href="#check-network-usage">Check network usage</a></h2>
<ol>
<li>Check for bursts of connections coming from the load-generator using</li>
</ol>
<pre><code class="language-bash"># On load-generator
%&gt; &lt;start test&gt;
  
# On SUT
%&gt; cd ~/aws-gravition-getting-started/perfrunbook/utilities
%&gt; python3 ./measure_and_plot_basic_sysstat_stats.py --stat new-connections --time 60
</code></pre>
<ol start="2">
<li>If seeing bursts, verify this is expected behavior for your load generator.  Bursts can cause performance degradation for each new connection, especially if it has to do an RSA signing operation for TLS connection establishment.</li>
<li>Check on SUT for hot connections (connections that are more heavily used than others) by running: <code>watch netstat -t</code></li>
<li>The example below shows the use of <code>netstat -t</code> to watch TCP connections with one being hot as indicated by its non-zero <code>Send-Q</code> value while all other connections have a value of 0. This can lead to one core being saturated by network processing on the SUT, bottlenecking the rest of the system.</li>
</ol>
<pre><code class="language-bash">%&gt; watch netstat -t
    Every 2.0s: netstat -t
    ip-172-31-9-146: Tue Jan 12 23:01:35 2021
      
    Active Internet connections (w/o servers)
    Proto Recv-Q Send-Q Local Address           Foreign Address         State
    tcp        0      0 ip-172-31-9-146.ec2:ssh 72-21-196-67.amaz:62519 ESTABLISHED
    tcp        0 345958 ip-172-31-9-146.ec2:ssh 72-21-196-67.amaz:25884 ESTABLISHED
    tcp        0      0 ip-172-31-9-146.ec2:ssh 72-21-196-67.amaz:18144 ESTABLISHED
</code></pre>
<ol start="5">
<li>Go back and verify the load generator is providing the expected traffic.</li>
<li>Check <code>sar -n DEV 1</code> to see throughput and packets per second, and overall packet rate the system is getting per device. Check and verify you are not hitting ENA throttles.</li>
<li><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/monitoring-network-performance-ena.html">Metrics in cloudwatch</a>: <code>bw_in_allowance_exceeded</code>, <code>bw_out_allowance_exceeded</code>, <code>conntrack_allowance_exceeded</code>, <code>linklocal_allowance_exceeded</code>, <code>pps_allowance_exceeded</code> for your instances can be inspected to see if networking throttles are being hit.  If they are, your network is the botteneck and should be increased.</li>
<li>If hitting ENA throttles, provision a larger instance to get more bandwidth if possible.  IO bottlenecks tend to mask any CPU performance gains.</li>
</ol>
<h2 id="check-runtime-behavior"><a class="header" href="#check-runtime-behavior">Check Runtime behavior</a></h2>
<p>Checking how your chosen runtime is behaving should be done before moving on to more in depth studies. Basic runtime tunables such as Garbage Collection need to be checked to ensure there is no unexpected behavior. We describe how to perform such standard checks for the Java runtime in the following.  Below we describe how to perform some standard checks for the Java runtime.  We are not able to provide recommended tools or methodologies for checking high level behavior of other popular runtimes such as Python or NodeJS at the moment.</p>
<h3 id="java-runtime"><a class="header" href="#java-runtime">Java Runtime</a></h3>
<p>When running Java applications, monitor for differences in behavior using JFR (Java Flight Recorder) to record JVM behavior and view with JMC (Java Mission Control). These can expose different execution behaviors as reasons for performance differences.</p>
<ol>
<li>Enable a JFR recording for your application by adding <code>-XX:+FlightRecorder -XX:StartFlightRecording=delay=&lt;X&gt;s,duration=&lt;X&gt;s,name=&lt;name&gt;,filename=path/to/recording.jfr</code> to the JVM command line.
<ol>
<li>Get <a href="https://www.oracle.com/java/technologies/javase/products-jmc8-downloads.html">Java Mission Control</a></li>
<li>Download the resulting JFR file to local machine</li>
<li>Open up the JFR file with Mission Control</li>
</ol>
</li>
<li>Check for Garbage Collection (GC) behavior
<ol>
<li>Longer collectionpauses,</li>
<li>Check if more objects/references are are live between collections compared to x86.</li>
<li>The image below shows JMC’s GC pane, showing pause times, heap size and references remaining after each collection.
<img src="perfrunbook/./images/jmc_example_image.png" alt="" /></li>
</ol>
</li>
<li>The same information can be gathered by enabling GC logging and then processing the log output. Enter <code>-Xlog:gc*,gc+age=trace,gc+ref=debug,gc+ergo=trace</code> on the Java command line and re-start your application.</li>
<li>If longer GC pauses are seen, this could be happening because objects are living longer on Graviton and the GC has to scan them.  To help debug this gather an off-cpu profile (<a href="perfrunbook/./debug_code_perf.html">see Section 5.b</a>) to look for threads that are sleeping more often and potentially causing heap objects to live longer.</li>
<li>Check for debug flags that are still enabled but should be disabled, such as: <code>-XX:-OmitStackTraceInFastThrow</code> which logs and generates stack traces for all exceptions, even if they are not fatal exceptions.</li>
<li>Check there are no major differences in JVM ergonomics between Graviton and x86, run:</li>
</ol>
<pre><code class="language-bash">%&gt; java -XX:+PrintFlagsFinal -version
# Capture output from x86 and Graviton and then diff the files
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="debugging-performance--what-part-of-the-code-is-slow"><a class="header" href="#debugging-performance--what-part-of-the-code-is-slow">Debugging performance — “What part of the code is slow?”</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>If after checking the system behavior with the sysstat tools the behavior of your code on the CPU is still different, then your next step is to generate code profiles. There are two primary types of profiles</p>
<h2 id="on-cpu-profiling"><a class="header" href="#on-cpu-profiling">On-cpu profiling</a></h2>
<p>If you see that Graviton is consuming more CPU-time than expected, on-cpu profiling can help find what is taking more time.  On-cpu profiling statistically measures what code is consuming time on the CPU by periodically sampling stack traces on the SUT.  We provide a utility to collect CPU profiles and emit a flamegraph, which is a graphical representation to show the percentage of execution time consumed by a particular stack-trace.  The x-axis represents percentage of execution time, and the y-axis is stack depth. These are particularly useful for finding differences in function execution times between two platforms.  To collect flamegraphs of on-cpu profiling, do the following:</p>
<ol>
<li>Verify native code is built with <code>-g -fno-omit-frame-pointer</code></li>
<li>Verify java code is started with <code>-XX:+PreserveFramePointer -agentpath:/usr/lib64/libperf-jvmti.so</code></li>
<li>Verify NodeJS code is started with <code>--perf-basic-prof</code></li>
<li>Collect the Flamegraph</li>
</ol>
<pre><code class="language-bash"># In terminal one on SUT/load-gen
%&gt; &lt;start load generator or benchmark&gt;
  
# In terminal two on SUT
cd ~/aws-graviton-getting-started/perfrunbook/utilities
sudo ./capture_flamegraphs.sh oncpu 300
  
# You will see a file saved in the following format:
# flamegraph_oncpu_&lt;instance id&gt;_&lt;instance type&gt;_&lt;date&gt;.svg
  
# In terminal three on your local machine
%&gt; scp "&lt;username&gt;@&lt;instance-ip&gt;:~/flamegraph_oncpu_*.svg" .
%&gt; open /path/to/flamegraph_oncpu_*.svg
</code></pre>
<ol start="5">
<li>Example on-cpu flamegraph:
<img src="perfrunbook/images/oncpu_example_flamgraph.png" alt="" /></li>
<li>Look for the most expensive functions and then compare with a flamegraph gathered from the x86 test system.</li>
<li>If you find an expensive function that is not expensive on your x86 system, proceed to <a href="perfrunbook/./optimization_recommendation.html">Section 6</a> for Optimization recommendations.</li>
</ol>
<h3 id="on-cpu-profiling-using-pseudo-non-maskable-interrupts-nmi"><a class="header" href="#on-cpu-profiling-using-pseudo-non-maskable-interrupts-nmi">On-cpu profiling using Pseudo Non-maskable-interrupts (NMI)</a></h3>
<p>If your on-cpu profiling reveals the hot code is in the kernel, you may see issues with measuring code-paths that run in non-preemptible sections of Linux.  This usually manifests as small functions such as <code>arch_local_irq_restore</code> in the kernel showing high overhead.  Functions like this are un-masking interrupts, such as the interrupt <code>perf</code> uses to trigger taking a sample stack trace, and since the interrupt may have
already been queued, <code>perf</code> will record IRQ unmasking functions as hot because that is when IRQs are re-enabled.  To collect profiles of kernel functions that are inside interrupt masked portions of code on Graviton you can enable <code>perf</code> to use a pseudo Non-maskable-interrupt to measure inside non-preemptible regions. To enable this, do the following:</p>
<ol>
<li>Ensure <code>CONFIG_ARM64_PSEUDO_NMI</code> is enabled in your kernel configuration</li>
<li>Enable <code>irqchip.gicv3_pseudo_nmi=1</code> on the kernel command line and reboot.</li>
<li>Collect a Flamegraph:</li>
</ol>
<pre><code class="language-bash"># In terminal on SUT
cd ~/aws-graviton-getting-started/perfrunbook/utilities
# Use a custome event from the PMU such as r11 (cycles) or r8 (instructions)
sudo ./capture_flamegraphs.sh r11 300
</code></pre>
<ol start="4">
<li>Double check you are using a PMU hardware-event, if you do not see a change in your profiles. Events that do not come from the cpu PMU, such as <code>cpu-clock</code>, can not utilize the pseudo-NMI feature on Graviton.</li>
<li>Be aware code using hardware events such as cycles (r11) will under-count idle time as Graviton uses a clock-gated sleep state during idle on Linux, meaning hardware events will not tick.</li>
</ol>
<p>You may see a small single-digit percent increase in overhead with pseudo-NMI enabled, but this is expected.  We recommend only turning on pseudo-NMI when needed.</p>
<h2 id="off-cpu-profiling"><a class="header" href="#off-cpu-profiling">Off-cpu profiling</a></h2>
<p>If Graviton is consuming less CPU-time than expected, it is useful to find call-stacks that are putting <em>threads</em> to sleep via the OS.  Lock contention, IO Bottlenecks, OS scheduler issues can all lead to cases where performance is lower, but the CPU is not being fully utilized.   The method to look for what might be causing more off-cpu time is the same as with looking for functions consuming more on-cpu time: generate a flamegraph and compare.  In this case, the differences are more subtle to look for as small differences can mean large swings in performance as more thread sleeps can induce milli-seconds of wasted execution time.</p>
<ol>
<li>Verify native (i.e. C/C++/Rust) code is built with <code>-fno-omit-frame-pointer</code></li>
<li>Verify java code is started with <code>-XX:+PreserveFramePointer -agentpath:/path/to/libperf-jvmti.so</code>
<ol>
<li>The <code>libperf-jvmti.so</code> library is usually provided when <code>perf</code> is installed.  If it is not, see <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/java.md#build-libperf-jvmtiso-on-amazon-linux-2">how to build the jvmti from source</a> in our getting-started guide.</li>
<li>Additional debugging information can be extracted by adding <code>-XX:+UnlockDiagnosticVMOptions -XX:+DebugNonSafepoints</code> to the java command line.</li>
</ol>
</li>
<li>Verify NodeJS code is started with <code>--perf-basic-prof</code></li>
<li>Collect the Flamegraph:</li>
</ol>
<pre><code class="language-bash"># In terminal one on SUT/load-gen
%&gt; &lt;start load generator or benchmark&gt;
  
# In terminal two on SUT
cd ~/aws-graviton-getting-started/perfrunbook/utilities
sudo ./capture_flamegraphs.sh offcpu 300
  
# You will see a file saved in the following format:
# flamegraph_oncpu_&lt;instance id&gt;_&lt;instance type&gt;_&lt;date&gt;.svg
  
# In terminal three on your local machine
%&gt; scp "&lt;username&gt;@&lt;instance-ip&gt;:~/flamegraph_oncpu_*.svg" .
%&gt; open /path/to/flamegraph_offcpu.svg
</code></pre>
<ol start="5">
<li>Inspect the flamegraphs for which code paths cause extra sleeps compared to x86:
<ol>
<li>Look at sleeps induced by to IO like <code>read</code>, <code>write</code>, and <code>epoll</code> system calls, do they happen more often?</li>
<li>Look at sleeps induced by callpaths that contain keywords such as: <code>lock</code>, <code>mutex</code>, <code>semaphore</code>, <code>synchronize</code>, <code>futex</code>. Determine if these are happening more often than on x86.</li>
<li>Disregard call-stacks that include <code>work_pending</code>. These indicate the CPU was involuntarily switched and can be ignored.</li>
</ol>
</li>
<li>If you find that there are code paths that are sleeping more than expected, proceed to <a href="perfrunbook/./optimization_recommendation.html">Section 6 for Optimization recommendations</a>.</li>
</ol>
<h2 id="additional-profiling-tips"><a class="header" href="#additional-profiling-tips">Additional profiling tips</a></h2>
<p>In our <code>capture_flamegraphs.sh</code> helper script, we use <code>perf record</code> to gather traces.  The script can be modified to collect different views of the same data to help dig deeper and root cause the reason for a detected performance issue on Graviton.</p>
<ol>
<li>To aide in spotting differences between two flamegraphs, you can use a flamegraph diff between <code>perf</code> data on x86 and Graviton</li>
</ol>
<pre><code class="language-bash"># On load generation instance
%&gt; &lt;start test&gt;
  
# x86 SUT
perf record -a -g -k 1 -F99 -e cpu-clock:pppH -- sleep
perf script -f -i perf.data &gt; x86_script.out
./FlameGraph/stackcollapse-perf.pl --kernel --jit x86_script.out &gt; x86_folded.out
  
# Graviton SUT
perf record -a -g -k 1 -F99 -e cpu-clock:pppH -- sleep
perf script -f -i perf.data &gt; grv_script.out
./FlameGraph/stackcollapse-perf.pl --kernel --jit grv_script.out &gt; grv_folded.out
# Copy x86_folded.out to Graviton SUT
./FlameGraph/difffolded.pl grv_folded.out x86_folded.out &gt; diff.out
./FlameGraph/flamegraph.pl --colors java diff.out &gt; flamegraph-diff.svg
</code></pre>
<ol start="2">
<li>View the diff — red regions indicate an increase in the proportion of execution time, blue a decrease. Note: diffing call-stacks between different architectures can lead to peculiar artifacts due to machine specific functions being named differently.</li>
<li>Create flame-graphs from <code>perf record</code>  that use different events than the cpu-clock to determine when to sample stack traces. This can help uncover different root causes and potential optimization opportunities.  Examples below:
<ol>
<li>Use <code>-e instructions</code> to generate a flame-graph of the functions that use the most instructions on average to identify a compiler or code optimization opportunity.</li>
<li>Use <code>-e cache-misses</code> to generate a flame-graph of functions that miss the L1 cache the most to indicate if changing to a more efficient data-structure might be necessary.</li>
<li>Use <code>-e branch-misses</code> to generate a flame-graph of functions that cause the CPU to mis-speculate.  This may identify regions with heavy use of conditionals, or conditionals that are data-dependent and may be a candidate for refactoring.</li>
</ol>
</li>
</ol>
<div style="break-before: page; page-break-before: always;"></div><h1 id="debugging-performance--what-part-of-the-hardware-is-slow"><a class="header" href="#debugging-performance--what-part-of-the-hardware-is-slow">Debugging performance — “What part of the hardware is slow?”</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>Sometimes, hardware, not code, is the reason for worse than expected performance. This may show up in the on-cpu profiles as every function is slightly slower on Graviton as more CPU time is consumed, but no obvious hot-spot function exists.  If this is the case, then measuring how the hardware performs can offer insight.  To do this requires counting special events in the CPU to understand which component of the CPU is bottlenecking the code from executing as fast as possible.</p>
<p>Modern server CPUs, whether they are from Intel, AMD or AWS all attempt to execute as many instructions as possible in a fixed amount of time by doing the following 4 fundamental operations in addition to executing at high frequencies:</p>
<ul>
<li>Executing instructions using multiple steps (pipelining)</li>
<li>Parallel instruction execution (out-of-order execution)</li>
<li>Predicting what instructions are needed next (speculation)</li>
<li>Predicting what values from DRAM should be cached nearby the processor (caching).</li>
</ul>
<p>The PMU (Performance Monitoring Unit) on modern CPUs have counters that can be programmed to count a set of events to give statistics on how aspects of those 4 fundamental operations are behaving.  When one aspect of the CPU starts becoming a bottleneck, for instance if caching starts to store the wrong values, then the CPU will execute instructions more slowly as it will be forced to access the correct values in main memory which is many times slower compared to a cache.</p>
<p>There are hundreds of events available to monitor in a server CPU today which is many times more than the 4 fundamental underpinnings of a modern CPU. This is because hundreds of components work together to enable a modern CPU to execute instructions quickly. The guide below describes the primary events to count with the PMU and explains their meaning to enable a first level root cause analysis of the observed performance issue.</p>
<h2 id="how-to-collect-pmu-counters"><a class="header" href="#how-to-collect-pmu-counters">How to Collect PMU counters</a></h2>
<p>Not all instance sizes support PMU event collection.  Generally
instance sizes which have an entire dedicated socket have full access
to all PMU events, and smaller instance sizes of the newer generations
have a reduced set of events suitable for most profiling needs.  For
older generations smaller instance sizes may not support any PMU event
collection.  The table below captures these details:</p>
<div class="table-wrapper"><table><thead><tr><th>Instance Family</th><th>Minimum Size for Full PMU Event Support</th><th>Basic Support at Smaller Sizes</th></tr></thead><tbody>
<tr><td>*8g</td><td>24xlarge</td><td>yes</td></tr>
<tr><td>*7a</td><td>24xlarge</td><td>yes</td></tr>
<tr><td>*7g</td><td>16xlarge</td><td>yes</td></tr>
<tr><td>*7i</td><td>24xlarge</td><td>yes</td></tr>
<tr><td>*6a</td><td>24xlarge</td><td>no</td></tr>
<tr><td>*6g</td><td>16xlarge</td><td>yes</td></tr>
<tr><td>*6i</td><td>16xlarge</td><td>no</td></tr>
<tr><td>*5</td><td>c5.9xlarge, *5.12xlarge</td><td>no</td></tr>
</tbody></table>
</div>
<p>To measure the standard CPU PMU events, do the following:</p>
<ol>
<li>Reserve SUT instances, i.e. a m6g.16xl and c5.12xl to get access to all events</li>
<li>Cut memory and vCPUs down to the size you need to represent your intended instance sizes</li>
</ol>
<pre><code class="language-bash"># Cut memory down to the required size
%&gt; ssh &lt;username&gt;@&lt;instance-ip&gt;
%&gt; cd ~/aws-graviton-getting-started/perfrunbook/utilities/
%&gt; sudo ./configure_mem_size.sh &lt;size in GB&gt;
%&gt; sudo reboot now
  
# Cut down vCPUs needed to the required size on x86
%&gt; sudo ./configure_vcpus.sh &lt;# vcpus&gt; threads
# Cut down vCPUs needed on Graviton
%&gt; sudo ./configure_vcpus.sh &lt;# vcpus&gt; cores
</code></pre>
<ol start="3">
<li>Measure individual hardware events or useful ratios (i.e. instruction commit event count over cycle tick counts to get instruction throughput per cycle) with our helper script. It will plot a time-series curve of the event count's behavior over time and provide geomean and percentile statistics.</li>
</ol>
<pre><code class="language-bash"># In terminal 1
%&gt; &lt;start load generator or benchmark&gt;
  
# In terminal 2
%&gt; cd ~/aws-graviton-getting-started/perfrunbook/utilities
# AMD (5a, 6a and 7a) instances not supported currently.
%&gt; sudo python3 ./measure_and_plot_basic_pmu_counters.py --stat ipc
  
# Example Output
1.6 ++-----------------------+------------------------+------------------------+-----------------------+------------------------+-----------------------++
     +                        +                        +                        +                       +                        +                        +
     |                                                                                           *                                                        |
1.55 ++                                           *           *                                 **                                  *                    ++
     |         *                                 * *         * *               **              * *              *                 ***                     |
     |        **      *                ******    * *  **     *  *     *    **** *     **      *  *             * *             ***   *                    |
     | ***   *  *    * *   ***  *    **         *   **  *  **   *   ** * **     *   **  *     *   *     ***** *   *       ** **      *                    |
 1.5 ++*  ***   *    *  * *    * *  *        ***         **      ***    *       *   *    *** *    *     *    *     *      * *        *     ***    **     ++
     |*         *   *    *    *   * *                                            *  *        *    *    *            *    *            *    *  **** *      |
     |*          ***               *                                             * *        *     *    *            *    *            *    *       *      |
1.45 +*          *                                                               * *              *   *              *   *            *    *       *     ++
     *                                                                           * *              *   *               * *              *  *        *      |
     *                                                                           * *              *   *               * *              *  *        *      |
 1.4 ++                                                                          * *              *  *                 *               *  *         *    ++
     |                                                                            **               * *                 *                * *         *     |
     |                                                                            *                **                                   * *         *     |
1.35 ++                                                                           *                **                                   * *         *    ++
     |                                                                            *                *                                    **          *     |
     |                                                                                                                                   *          *     |
     |                                                                                                                                   *          *     |
 1.3 ++                                                                                                                                             *    ++
     |                                                                                                                                              *     |
     |                                                                                                                                              *     |
1.25 ++                                                                                                                                             *    ++
     |                                                                                                                                              *     |
     |                                                                                                                                               *    |
 1.2 ++                                                                                                                                              *   ++
     |                                                                                                                                               *    |
     |                                                                                                                                               *    |
     |                                                                                                                                               *    |
1.15 ++                                                                                                                                              *   ++
     |                                                                                                                                                    |
     +                        +                        +                        +                       +                        +                        +
 1.1 ++-----------------------+------------------------+------------------------+-----------------------+------------------------+-----------------------++
     0                        10                       20                       30                      40                       50                       60
                                                         gmean:   1.50 p50:   1.50 p90:   1.50 p99:   1.61
</code></pre>
<ol>
<li>You can also measure all relevant ratios at once using our aggregate PMU measuring script if you do not need a time-series view.  It prints out a table of measured PMU ratios at the end and supports the same events.</li>
</ol>
<pre><code class="language-bash"># In terminal 1
%&gt; &lt;start load generator or benchmark&gt;
  
# In terminal 2
%&gt; cd ~/aws-graviton-getting-started/perfrunbook/utilities
# AMD (5a, 6a, and 7a) instances not supported currently.
%&gt; sudo python3 ./measure_aggregated_pmu_stats.py --timeout 300
|Ratio               |   geomean|       p10|       p50|       p90|       p95|       p99|     p99.9|      p100|
|ipc                 |      1.81|      1.72|      1.81|      1.90|      1.93|      1.95|      1.95|      1.95|
|branch-mpki         |      0.01|      0.01|      0.01|      0.01|      0.01|      0.02|      0.02|      0.02|
|code_sparsity       |      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|
|data-l1-mpki        |     11.24|     10.48|     11.25|     12.05|     12.21|     12.62|     12.62|     12.62|
|inst-l1-mpki        |      0.08|      0.07|      0.08|      0.09|      0.10|      0.15|      0.15|      0.15|
|l2-ifetch-mpki      |      0.06|      0.05|      0.06|      0.06|      0.07|      0.12|      0.12|      0.12|
|l2-mpki             |      0.71|      0.66|      0.70|      0.76|      0.77|      1.03|      1.03|      1.03|
|l3-mpki             |      0.49|      0.42|      0.49|      0.55|      0.63|      0.67|      0.67|      0.67|
|stall_frontend_pkc  |      1.97|      1.61|      1.90|      2.49|      2.68|      5.28|      5.28|      5.28|
|stall_backend_pkc   |    425.00|    414.64|    424.81|    433.63|    435.82|    441.57|    441.57|    441.57|
|inst-tlb-mpki       |      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|
|inst-tlb-tw-pki     |      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|
|data-tlb-mpki       |      1.49|      1.23|      1.55|      1.65|      1.66|      1.78|      1.78|      1.78|
|data-tlb-tw-pki     |      0.00|      0.00|      0.00|      0.01|      0.01|      0.01|      0.01|      0.01|
|inst-neon-pkc       |      0.31|      0.30|      0.30|      0.31|      0.31|      0.40|      0.40|      0.40|
|inst-scalar-fp-pkc  |      2.43|      2.37|      2.44|      2.49|      2.51|      2.52|      2.52|      2.52|
|stall_backend_mem_pkc|     90.73|     83.97|     90.71|     97.67|     97.98|    100.98|    100.98|    100.98|
|inst-sve-pkc        |    419.00|    409.92|    419.83|    426.79|    430.73|    433.08|    433.08|    433.08|
|inst-sve-empty-pkc  |      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|      0.00|
|inst-sve-full-pkc   |    180.89|    176.99|    181.24|    184.31|    185.91|    187.16|    187.16|    187.16|
|inst-sve-partial-pkc|      2.39|      2.27|      2.38|      2.50|      2.53|      2.58|      2.58|      2.58|
|flop-sve-pkc        |   1809.47|   1768.84|   1813.91|   1842.77|   1860.45|   1871.86|   1871.86|   1871.86|
|flop-nonsve-pkc     |      2.48|      2.41|      2.48|      2.54|      2.56|      2.57|      2.57|      2.57|
</code></pre>
<h2 id="top-down-method-to-debug-hardware-performance"><a class="header" href="#top-down-method-to-debug-hardware-performance">Top-down method to debug hardware performance</a></h2>
<p>This checklist describes the top-down method to debug whether the hardware is under-performing and what part is underperforming.  The checklist describes event ratios to check that are included in the helper-script.  All ratios are in terms of either misses-per-1000(kilo)-instruction or per-1000(kilo)-cycles.  This checklist aims to help guide whether a hardware slow down is coming from the front-end of the processor or the backend of the processor and then what particular part.  The front-end of the processor is responsible for fetching and supplying the instructions.  The back-end is responsible for executing the instructions provided by the front-end as fast as possible.  A bottleneck in either part will cause stalls and a decrease in performance.  After determining where the bottleneck may lie, you can proceed to <a href="perfrunbook/./optimization_recommendation.html">Section 6</a> to read suggested optimizations to mitigate the problem.</p>
<ol>
<li>Start by measuring <code>ipc</code> (Instructions per cycle) on each instance-type.  A higher IPC is better. A lower number for <code>ipc</code> on Graviton compared to x86 indicates <em>that</em> there is a performance problem.  At this point, proceed to attempt to root cause where the lower IPC bottleneck is coming from by collecting frontend and backend stall metrics.</li>
<li>Next, measure <code>stall_frontend_pkc</code> and <code>stall_backend_pkc</code> (pkc = per kilo cycle) and determine which is higher.  If stalls in the frontend are higher, it indicates the part of the CPU responsible for predicting and fetching the next instructions to execute is causing slow-downs.  If stalls in the backend are higher, it indicates the machinery that executes the instructions and reads data from memory is causing slow-downs</li>
</ol>
<h3 id="drill-down-front-end-stalls"><a class="header" href="#drill-down-front-end-stalls">Drill down front end stalls</a></h3>
<p>Front end stalls commonly occur if the CPU cannot fetch the proper instructions, either because it is speculating the wrong destination for a branch, or stalled waiting to get instructions from memory.  Below are steps to identify if this is the case.</p>
<ol>
<li>Measure <code>branch-mpki</code> which defines how many predicted branches are wrong and fetched the wrong instructions to execute next. Every time the CPU predicts incorrectly it has to flush the current set of instructions it was working on and start over by fetching new instructions from the correct place.  A <code>branch-mpki</code> average value of &gt;10 indicates the branch prediction logic is bottlenecking the processor.</li>
<li>Measure <code>inst-l1-mpki</code>.  A value &gt;20 indicates the working-set code footprint is large and is spilling out of the fastest cache on the processor and is potentially a bottleneck.</li>
<li>Measure <code>inst-tlb-mpki</code>. A value &gt;0 indicates the CPU has to do extra stalls to translate the virtual addresses of instructions into physical addresses before fetching them and the footprint is too large.</li>
<li>Measure <code>inst-tlb-tw-pki</code> . A value &gt;0 indicates the instruction footprint might be too large.</li>
<li>Measure <code>code-sparsity</code> . A number &gt;0.5 indicates the code being executed by the CPU is very sparse. This counter is only available on Graviton 16xlarge or metal instances. If the number is &gt;0.5 for the workload under test please see <a href="perfrunbook/./optimization_recommendation.html#optimizing-for-large-instruction-footprint">Optimizing For Large Instruction Footprints</a>.</li>
<li>If front-end stalls are the root cause, the instruction footprint needs to be made smaller, proceed to <a href="perfrunbook/./optimization_recommendation.html">Section 6</a> for suggestions on how to reduce front end stalls for your application..</li>
</ol>
<h3 id="drill-down-back-end-stalls"><a class="header" href="#drill-down-back-end-stalls">Drill down back-end stalls</a></h3>
<p>Backend stalls are caused when the CPU is unable to make forward progress executing instructions because a computational resource is full.  This is commonly due to lacking enough resources to execute enough memory operations in parallel because the data set is large and current memory requests are waiting for responses.  This checklist details how to gather information on how well the core is caching the data-set and identify if the backend is stalling because of it.</p>
<ol>
<li>Measure <code>data-l1-mpki</code> .  If this number is &gt;20, indicates the working set data footprint could be an issue.</li>
<li>Measure <code>l2-mpki</code>.  If this number is &gt;10, indicates the working set data footprint could be an issue.</li>
<li>Measure <code>l3-mpki</code>. If this number is &gt;10, indicates the working set data footprint is not fitting in L3 and data references are being served by DRAM.  The l3-mpki also indicates the DRAM bandwidth requirement of your application, a higher number means more DRAM bandwidth will be consumed, this may be an issue if your instance is co-located with multiple neighbors also consuming a measurable amount of DRAM bandwidth.</li>
<li>Measure <code>data-tlb-mpki</code> . A number &gt;0 indicates the CPU has to do extra stalls to translate the virtual address of load and store instructions into physical addresses the DRAM understands before issuing the load/store to the memory system.  A TLB (translation lookaside buffer) is a cache that holds recent virtual address to physical address translations.</li>
<li>Measure <code>data-tlb-tw-pki</code> . A number &gt;0 indicates the CPU has to do extra stalls to translate the virtual address of the load/store instruction into physical addresses the DRAM understands before issuing to the memory system. In this case the stalls are because the CPU must walk the OS built page-table, which requires <strong>extra memory references</strong> before the requested memory reference from the application can be executed.</li>
<li>If back-end stalls due to the cache-system and memory system are the problem, the data-set size and layout needs to be optimized.</li>
<li>Proceed to <a href="perfrunbook/./optimization_recommendation.html">Section 6</a> to view optimization recommendations for working with a large data-set causing backend stalls.</li>
</ol>
<h3 id="drill-down-vectorization"><a class="header" href="#drill-down-vectorization">Drill down Vectorization</a></h3>
<p>Vectorization is accomplished either by SVE or NEON instructions.  SVE vectorization will use 256-bit vectors on Graviton 3 processors, but the scalable nature of SVE makes both the code and binary vector-length agnostic.  NEON vectorization is always a 128-bit vector size, and does not have the predicate feature of SVE.</p>
<p>For SVE instructions there are metrics which describe how many SVE instructions had empty, full and partially-filled SVE predicates: <code>inst-sve-empty-pkc</code>, <code>inst-sve-partial-pkc</code>, and <code>inst-sve-full-pkc</code>.  These metrics apply to all SVE instructions (loads, stores, integer, and floating-point operations).  The <code>pkc</code> term indicates the counters are in units of "per kilo cycle".</p>
<p>A single SVE instruction can execute multiple (vectorized) floating point operations in the ALU.  These are counted individually by <code>flop-sve-pkc</code>.  For example: a single SVE <code>FMUL</code> instruction on 32-bit floats on Graviton 3's 256-bit vector will increment the <code>flop-sve-pkc</code> counter by eight because the operation is executed on the eight 32-bit floats that fit in the 256-bit vector.  Some instructions, such as <code>FMA</code> and (Fused Multiply Add) excute two floating point operations per item in the vector and increment the counter accordingly.  The <code>flop-sve-pkc</code> counter is incremented assuming a full SVE predicate.</p>
<p>Floating point operations for NEON and scalar instructions are counted together in the <code>flop-nonsve-pkc</code> counter.  For a single NEON <code>FMUL</code> instruction on 32-bit floats, the <code>inst-neon-pkc</code> counter will increment by one, and the <code>flop-nonsve-pkc</code> counter will increment by four (the number of 32-bit floats in a 128-bit NEON register).  For a single scalar <code>FMUL</code> instruction, the <code>flop-nonsve-pkc</code> counter will increment by one.  Some instructions (e.g., Fused Multiply Add) will increment the value by two.</p>
<p>The total number of floating-point instructions retired every 1000 cycles is <code>inst-scalar-fp-pkc + (inst-neon-pkc + inst-sve-pkc)*&lt;alpha&gt;</code>, where <code>&lt;alpha&gt;</code> is a user-provided estimate of the fraction of SVE and NEON instructions which task the floating-point ALU vs loads, stores, or integer operations.  The total number of floating-point operations executed by those instructions is <code>flop-nonsve-pkc + flop-sve-pkc*&lt;beta&gt;</code>, where <code>&lt;beta&gt;</code> is a user-provided estimate of how often the predicate was full.  To calculate a maximum expected value for these metrics, consult the ARM Software Optimization Guide and determine a FLOP per kilocycle from the instruction throughput and element size of the operation.  A code with no loads, stores, or dependencies performing <code>FMUL</code> entirely in L1 cache on 32-bit floats could theoretically observe <code>flop-sve-pkc</code> of 16,000 with SVE, or <code>flop-nonsve-pkc</code> of 16,000 with NEON SIMD, or <code>flop-nonsve-pkc</code> of 4,000 with scalar operations.</p>
<p>A footnote to readers of the ARM architecture PMU event description: SVE floating point operations are reported by hardware in units of "floating point operations per 128-bits of vector size", however the aggregation script we provide has already accounted for the Graviton 3 vector width before reporting.</p>
<h2 id="additonal-pmus-and-pmu-events"><a class="header" href="#additonal-pmus-and-pmu-events">Additonal PMUs and PMU events</a></h2>
<p>On metal instances, all available hardware PMUs and their events are exposed to instances and can be accessed so long as driver support by the OS in use is available.<br />
These extra PMUs help with diagnosing specific use cases, but are generally less applicable than the more widely used CPU PMU.
These PMU and events include: the Statistical Profiling Extension that provides precise data for sampled instructions, and
the Coherent Mesh Network PMU on Graviton instances to measure system level events such as system wide DRAM bandwidth use.
These PMUs will be covered in this section with links to documentation and Linux kernel support.</p>
<h3 id="capturing-statistical-profiling-extension-spe-hardware-events-on-graviton-metal-instances"><a class="header" href="#capturing-statistical-profiling-extension-spe-hardware-events-on-graviton-metal-instances">Capturing Statistical Profiling Extension (SPE) hardware events on Graviton metal instances</a></h3>
<p>The SPE PMU on Graviton enables cores to precisely trace events for individual instructions and record them to a memory buffer with the
linux <code>perf</code> tool.  It samples instructions from the executed instruction stream at random.  It is particularly useful for finding information
about particular loads that are always long latency, false sharing of atomic variables, or branches that are often mis-predicted and causing slow-downs.
Because SPE is precise, this information can be attributed back to individual code lines that need to be optimized.
SPE is enabled Graviton 2 and 3 metal instances.  The below table shows for which Linux distributions and kernel versions SPE is known to be
enabled.</p>
<div class="table-wrapper"><table><thead><tr><th>Distro</th><th>Kernel</th></tr></thead><tbody>
<tr><td>AL2</td><td>5.10</td></tr>
<tr><td>AL2023</td><td>&gt;=6.1.2</td></tr>
<tr><td>Ubuntu 20.04</td><td>&gt;=5.15</td></tr>
<tr><td>Ubuntu 22.04</td><td>&gt;=6.2</td></tr>
</tbody></table>
</div>
<p>On Amazon Linux 2 and 2023, the SPE PMU is available by default on Graviton metal instances, you can check for its existence by verifying:</p>
<pre><code># Returns the directory exists
ls /sys/devices/arm_spe_0
</code></pre>
<p>On Ubuntu to enable SPE requires four extra steps</p>
<pre><code># Install the arm_spe_pmu.ko module
sudo apt install linux-modules-extra-$(uname -r)

# Add kpti=off to the kernel boot command line: GRUB_CMDLINE_LINUX in /etc/default/grub, to set it for all the installed kernels.
# Note, the options passed to GRUB_CMDLINE_LINUX_DEFAULT will only propagate to the default kernel and not to all the installed kernels.
# Reboot instance

sudo modprobe arm_spe_pmu

# Verify exists
ls /sys/devices/arm_spe_0
</code></pre>
<p>SPE can be used via Linux <code>perf</code>. An example that samples every 1000'th branch on core 0 system wide is shown below:
<code>perf record -C0 -c1000 -a -e arm_spe_0/branch_filter=1,ts_enable=1,pct_enable=1,pa_enable=1,jitter=1/ -- sleep 30</code></p>
<p>Processing the data can be done with <code>perf report</code> to inspect hot functions and annotate assembly. If you want to look at
the samples directly, you can use the <a href="https://gitlab.arm.com/telemetry-solution/telemetry-solution/-/tree/spe-parser-prototype/tools/spe-parser">Arm SPE Parser</a>.</p>
<p>Be aware that SPE produces a large of volume of data (many GBs) if the sampling period is low and you collect the data over a long time.</p>
<h3 id="capturing-cache-coherence-issues"><a class="header" href="#capturing-cache-coherence-issues">Capturing cache coherence issues</a></h3>
<p>The <code>perf c2c</code> command can be used to analyze cache coherence issues and false sharing in multi-core systems. It needs SPE PMU available on Graviton metal instances.</p>
<p><code>perf c2c record ./application</code></p>
<h3 id="capturing-coherent-mesh-network-cmn-hardware-events-on-graviton-metal-instances"><a class="header" href="#capturing-coherent-mesh-network-cmn-hardware-events-on-graviton-metal-instances">Capturing Coherent Mesh Network (CMN) hardware events on Graviton metal instances</a></h3>
<p>On Graviton the CMN connects the CPUs to each other, to the memory controller, the I/O subsystem and provides the System Level Cache.
Its PMU counts events such as requests to SLC, DRAM (memory bandwidth), IO bus requests or coherence snoop events.
These metrics can be used to assess an application's utilization of such system level resources and if resources are used efficiently.
CMN counters are only accessible on Graviton metal-type instances and certain OSes and kernels.</p>
<div class="table-wrapper"><table><thead><tr><th>Distro</th><th>Kernel</th><th>Graviton2 (c6g)</th><th>Graviton3 (c7g)</th></tr></thead><tbody>
<tr><td>Ubuntu-20.04</td><td>5.15</td><td>yes</td><td>no</td></tr>
<tr><td>Ubuntu-20.04</td><td>&gt;=5.19</td><td>yes</td><td>yes</td></tr>
<tr><td>Ubuntu-22.04</td><td>5.15</td><td>no</td><td>no</td></tr>
<tr><td>Ubuntu-22.04</td><td>&gt;=5.19</td><td>yes</td><td>yes</td></tr>
<tr><td>AL2</td><td>5.10</td><td>no</td><td>no</td></tr>
<tr><td>AL2023</td><td>6.1.2</td><td>yes</td><td>yes</td></tr>
</tbody></table>
</div>
<p>General procedure on Ubuntu</p>
<pre><code>sudo apt install linux-modules-extra-aws
sudo modprobe arm-cmn
ls /sys/devices/arm_cmn_0/events
</code></pre>
<p>On AL2023/AL2:</p>
<pre><code>sudo modprobe arm_cmn
ls /sys/devices/arm_cmn_0/events
</code></pre>
<p>Examples for capturing events:</p>
<pre><code>sudo perf stat -C 0 -e /arm_cmn_0/hnf_mc_reqs/ sleep 15 #count of memory request
sudo perf stat -C 0 -e /arm_cmn_0/rnid_rxdat_flits/ sleep 15 #count AXI 'master' read requests
sudo perf stat -C 0 -e /arm_cmn_0/rnid_txdat_flits/ sleep 15 #count AXI 'master' write requests
</code></pre>
<p>Further information about capturing fabric events is available here:</p>
<p><a href="https://developer.arm.com/documentation/100180/0302/?lang=en">ARM documentation for Graviton2's CMN-600</a></p>
<p><a href="https://developer.arm.com/documentation/101481/0200/?lang=en">ARM documentation for Graviton3's CMN-650</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="optimizing-performance"><a class="header" href="#optimizing-performance">Optimizing performance</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>This section describes multiple different optimization suggestions to try on Graviton based instances to attain higher performance for your service.  Each sub-section defines some optimization recommendations that can help improve performance if you see a particular signature after measuring the performance using the previous checklists.</p>
<h2 id="optimizing-for-large-instruction-footprint"><a class="header" href="#optimizing-for-large-instruction-footprint">Optimizing for large instruction footprint</a></h2>
<ol>
<li>On C/C++ applications, <code>-flto</code>, <code>-Os</code>, and <a href="https://gcc.gnu.org/wiki/AutoFDO/Tutorial">Feedback Directed Optimization</a> can help with code layout using GCC.</li>
<li>On Java, <code>-XX:-TieredCompilation</code>, <code>-XX:ReservedCodeCacheSize</code> and <code>-XX:InitialCodeCacheSize</code> can be tuned to reduce the pressure the JIT places on the instruction footprint. The JDK defaults to setting up a 256MB region by default for the code-cache which over time can fill, become fragmented, and live code may become sparse.
<ol>
<li>We recommend setting the code cache initially to: <code>-XX:-TieredCompilation -XX:ReservedCodeCacheSize=64M -XX:InitialCodeCacheSize=64M</code> and then tuning the size up or down as required.</li>
<li>Experiment with setting <code>-XX:+TieredCompilation</code> to gain faster start-up time and better optimized code.</li>
<li>When tuning the code JVM code cache, watch for <code>code cache full</code> error messages in the logs indicating that the cache has been set too small.  A full code cache can lead to worse performance.</li>
</ol>
</li>
</ol>
<h2 id="optimizing-for-high-tlb-miss-rates"><a class="header" href="#optimizing-for-high-tlb-miss-rates">Optimizing for high TLB miss rates</a></h2>
<p>A TLB (translation lookaside buffer) is a cache that holds recent virtual address to physical address translations for the CPU to use.  Making sure this cache never misses can improve application performance.</p>
<ol>
<li>Enable Transparent Huge Pages (THP)
<code>echo always &gt; /sys/kernel/mm/transparent_hugepage/enabled</code> -or- <code>echo madvise &gt; /sys/kernel/mm/transparent_hugepage/enabled</code></li>
<li>On Linux kernels &gt;=6.9 Transparent Huge Pages (THP) has been extended with <a href="https://lwn.net/Articles/937239/">Folios</a> that create 16kB, and 64kB huge pages in addition to 2MB pages. This allows the Linux kernel to use huge pages in more places to increase performance by reducing TLB pressure.  All folio sizes can be set using <code>inherit</code> to use the setting of the top-level THP setting, or set independently to select the sizes to use.  Can also set each folio using <code>never</code>, <code>always</code> and <code>madvise</code>.
<ol>
<li>To use 16kB pages: <code>echo inherit &gt; /sys/kernel/mm/transparent_hugepage/hugepages-16kB/enabled</code></li>
<li>To use 64kB pages: <code>echo inherit &gt; /sys/kernel/mm/transparent_hugepage/hugepages-64kB/enabled</code></li>
<li>To use 2MB pages: <code>echo inherit &gt; /sys/kernel/mm/transparent_hugepage/hugepages-2048kB/enabled</code></li>
</ol>
</li>
<li>If your application can use pinned hugepages because it uses mmap directly, try reserving huge pages directly via the OS.  This can be done by two methods.
<ol>
<li>At runtime: <code>sysctl -w vm.nr_hugepages=X</code></li>
<li>At boot time by specifying on the kernel command line in <code>/etc/default/grub</code>: <code>hugepagesz=2M hugepages=512</code></li>
</ol>
</li>
<li>For Java, hugepages can be used for both the code-heap and data-heap by adding the below flags to your JVM command line
<ol>
<li><code>-XX:+UseTransparentHugePages</code> when THP is set to at least <code>madvise</code></li>
<li><code>-XX:+UseLargePages</code> if you have pre-allocated huge pages through <code>sysctl</code> or the kernel command line.</li>
</ol>
</li>
</ol>
<p>Using huge-pages should generally improve performance on all EC2 instance types, but there can be cases where using exclusively
huge-pages may lead to performance degradation.  Therefore, it is always recommended to fully test your application after enabling and/or
allocating huge-pages.</p>
<h2 id="porting-and-optimizing-assembly-routines"><a class="header" href="#porting-and-optimizing-assembly-routines">Porting and optimizing assembly routines</a></h2>
<ol>
<li>If you need to port an optimized routine that uses x86 vector instruction instrinsics to Graviton’s vector instructions (called NEON instructions), you can use the <a href="https://github.com/DLTcollab/sse2neon">SSE2NEON</a> library to assist in the porting.  While SSE2NEON won’t produce optimal code, it generally gets close enough to reduce the performance penalty of not using the vector intrinsics.</li>
<li>For additional information on the vector instructions used on Graviton
<ol>
<li><a href="https://developer.arm.com/architectures/instruction-sets/intrinsics/">Arm instrinsics guide</a></li>
<li><a href="https://developer.arm.com/documentation/pjdoc466751330-9707/2-0">Graviton2 core software optimization guide</a></li>
<li><a href="https://developer.arm.com/documentation/pjdoc466751330-9685/latest/">Graviton3 core software optimization guide</a></li>
<li><a href="https://developer.arm.com/documentation/PJDOC-466751330-593177/latest/">Graviton4 core software optimization guide</a></li>
</ol>
</li>
</ol>
<h2 id="optimizing-synchronization-heavy-optimizations"><a class="header" href="#optimizing-synchronization-heavy-optimizations">Optimizing synchronization heavy optimizations</a></h2>
<ol>
<li>Look for specialized back-off routines for custom locks tuned using x86 <code>PAUSE</code> or the equivalent x86 <code>rep; nop</code> sequence. Graviton2 should use a single <code>ISB</code> instruction as a drop in replacement, for an example and explanation see recent commit to the <a href="https://github.com/wiredtiger/wiredtiger/pull/6080/files#diff-08a92383c3904f531b067c488d6d6e34ddad0e3008313982b1b0712c0c3a7598">Wired Tiger storage layer</a>.</li>
<li>If a locking routine tries to acquire a lock in a fast path before forcing the thread to sleep via the OS to wait, try experimenting with modifying the fast path to attempt the fast path a few additional times before executing down the slow path. <a href="https://github.com/twitter/finagle/blob/develop/finagle-stats-core/src/main/scala/com/twitter/finagle/stats/NonReentrantReadWriteLock.scala">An example of this from the Finagle code-base where on Graviton2 we will spin longer for a lock before sleeping</a>.</li>
<li>If you do not intend to run your application on Graviton1, try compiling your code on GCC using <code>-march=armv8.2-a</code> instead of using <code>-moutline-atomics</code> to reduce overhead of using synchronization builtins.</li>
</ol>
<h2 id="network-heavy-workload-optimizations"><a class="header" href="#network-heavy-workload-optimizations">Network heavy workload optimizations</a></h2>
<ol>
<li>Check ENA device tunings with <code>ethtool -c ethN</code> where <code>N</code> is the device number and check <code>Adaptive RX</code> setting. By default on instances without extra ENI’s this will be <code>eth0</code>.
<ol>
<li>Set to <code>ethtool -C ethN adpative-rx off</code> for a latency sensitive workload</li>
<li>ENA tunings via <code>ethtool</code> can be made permanent by editing <code>/etc/sysconfig/network-scripts/ifcfg-ethN</code> files.</li>
</ol>
</li>
<li>Disable <code>irqbalance</code> from dynamically moving IRQ processing between vCPUs and set dedicated cores to process each IRQ.  Example script below:</li>
</ol>
<pre><code class="language-bash"># Assign eth0 ENA interrupts to the first N-1 cores
systemctl stop irqbalance
  
irqs=$(grep "eth0-Tx-Rx" /proc/interrupts | awk -F':' '{print $1}')
cpu=0
for i in $irqs; do
  echo $cpu &gt; /proc/irq/$i/smp_affinity_list
  let cpu=${cpu}+1
done
</code></pre>
<ol start="3">
<li>Disable Receive Packet Steering (RPS) to avoid contention and extra IPIs.
<ol>
<li><code>cat /sys/class/net/ethN/queues/rx-N/rps_cpus</code> and verify they are set to <code>0</code>. In general RPS is not needed on Graviton2 and newer.</li>
<li>You can try using RPS if your situation is unique.  Read the <a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt">documentation on RPS</a> to understand further how it might help. Also refer to <a href="https://aws.amazon.com/blogs/compute/optimizing-network-intensive-workloads-on-amazon-ec2-a1-instances/">Optimizing network intensive workloads on Amazon EC2 A1 Instances</a> for concrete examples.</li>
</ol>
</li>
</ol>
<h2 id="metal-instance-io-optimizations"><a class="header" href="#metal-instance-io-optimizations">Metal instance IO optimizations</a></h2>
<ol>
<li>If on Graviton2 and newer metal instances, try disabling the System MMU (Memory Management Unit) to speed up IO handling:</li>
</ol>
<pre><code class="language-bash">%&gt; cd ~/aws-gravition-getting-started/perfrunbook/utilities
# Configure the SMMU to be off on metal, which is the default on x86.
# Leave the SMMU on if you require the additional security protections it offers.
# Virtualized instances do not expose an SMMU to instances.
%&gt; sudo ./configure_graviton_metal_iommu.sh off
%&gt; sudo shutdown now -r
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="appendix-2"><a class="header" href="#appendix-2">Appendix:</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>This Appendix contains additional information for engineers that want to go deeper on a particular topic, such as using different PMU counters to understand how the code is executing on the hardware, discussion on load generators, and additional tools to help with code observability.</p>
<h2 id="useful-graviton-pmu-events-and-ratios"><a class="header" href="#useful-graviton-pmu-events-and-ratios">Useful Graviton PMU Events and ratios</a></h2>
<p>The following list of counter ratios has been curated to list events useful for performance debugging. The more extensive list of counters is contained in the following references:</p>
<ul>
<li><a href="https://developer.arm.com/documentation/102105/latest">Arm ARM</a></li>
<li><a href="https://developer.arm.com/documentation/100616/0400/debug-descriptions/performance-monitor-unit/pmu-events">Neoverse N1 TRM</a></li>
<li><a href="https://developer.arm.com/documentation/PJDOC-466751330-547673/r4p1?lang=en&amp;rev=0">Neoverse N1 PMU Guide</a></li>
<li><a href="https://developer.arm.com/documentation/101427/latest/">Neoverse V1 TRM</a></li>
<li><a href="https://developer.arm.com/documentation/109708/latest/">Neoverse V1 PMU Guide</a></li>
<li><a href="https://developer.arm.com/documentation/102375/latest/">Neoverse V2 TRM</a></li>
<li><a href="https://developer.arm.com/documentation/109528/0100">Neoverse V2 PMU Guide</a></li>
</ul>
<p>|METRIC	|Counter #1	|Counter #2	|Formula	|Description	|
|---	|---	|---	|---	|---	|
|IPC	|0x8 (INST_RETIRED)	|0x11 (CPU_CYCLES)	|#1 / #2	|Instructions per cycle, metric to understand how much parallelism in the core is left unused.	|
|Stalls Frontend PKC	|0x23 (STALL_FRONTEND)	|0x11 (CPU_CYCLES)	|(#1 / #2) * 1000	|Stalls per kilo-cycle caused by frontend not having an instruction available for dispatch stage to issue.	|
|Stalls Backend PKC	|0x24 (STALL_BACKEND)	|0x11 (CPU_CYCLES)	|(#1 / #2) * 1000	|Stalls per kilo-cycle caused by backend preventing instruction dispatch from frontend due to ALUs IQ full, LSQ full, MCQ full.	|
|Core Mem BW	|0x37	| 	|(#1 * 64) / (sample period)	|Read demand memory bandwidth estimate (excludes writes, prefetch, tablewalks) in Bytes / s from a core.	|
|LLC MPKI	|0x37 (LLC_CACHE_MISS_RD)	|0x8 (INST_RETIRED)	|(#1 / #2) * 1000	|From the cores perspective, is the LLC operating well, infer if it should be increased	|
|BMPKI	|0x10 (BR_MIS_PRED)	|0x8 (INST_RETIRED)	|(#1 / #2) * 1000	|Branch Miss per Kilo Instruction, metric to understand if branch prediction logic is on average performing well or not. Branches exist every ~4 instructions, BMPKI &gt; 10 is not good.	|
|L2C Inst MPKI	|0x108 (L2_REFILL_IFETCH)	|0x8	|(#1 / #2) * 1000	|Instruction fetch miss from L2 MPKI, infer if code size and fetch misses are stalling core	|
|L1C Inst MPKI	|0x1 (L1I_CACHE_REFILL)	|0x8	|(#1 / #2) * 1000	|Instruction fetch miss L1 MPKI, &gt;20 is generally an indicator of code size pressure	|
|L2 MPKI	|0x17 (L2D_CACHE_REFILL)	|0x8	|(#1 / #2) * 1000	|L2 Cache overall MPKI,	|
|L1C Data MPKI	|0x4 (L1_DATA_REFILL)	|0x8	|(#1 / #2) * 1000	|L1 Data cache overall MPKI.	|
|Old Atomics failures	|0x6E (STREX_FAIL_SPEC)	|0x8	|(#1 / #2) * 1000	|PKI of store-exclusive failures, indicates software not using LSE in production.	|
|L2 TLB MPKI	|0x2D	|0x8	|(#1 / #2) *1000	|L2 TLB MPKI	|
|L1I TLB MPKI	|0x2	|0x8	|(#1 / #2) * 1000	|Instruction L1 TLB MPKI	|
|L1I Page Walk PKI	|0x35	|0x8	|(#1 / #2) * 1000	|Instruction PKI that requires a page-table walk	|
|L1D TLB MPKI	|0x5	|0x8	|(#1 / #2) * 1000	|Data L1 TLB MPKI	|
|L1D Page Walk PKI	|0x34	|0x8	|(#1 / #2) * 1000	|Data PKI that requires a page-table walk	|</p>
<h2 id="load-generators"><a class="header" href="#load-generators">Load generators</a></h2>
<p>There are many load generators out there, and most if not all allow for the same basic configurations such as number of simulated clients (or number of connections), number of outstanding requests, target throughput, programmable way to define request mix etc. Pick one that is easy to work with, offers the features needed, is extensible for creating the tests desired, and allows fine grained control over how load is generated. After finding the ideal load generator, it is also important to understand how the load generator creates load to avoid easy measurement mistakes. Load generators fall into two main types: closed and open loop.</p>
<h3 id="closed-loop"><a class="header" href="#closed-loop"><strong>Closed loop</strong></a></h3>
<p>A closed loop load generator uses simple, synchronous IO calls for load generation in a similar fashion to below:</p>
<pre><code class="language-C">void generate_load(socket_t sock)
{
   int wret, rret = 0;
   const char *msg = "GENERATE SOME LOAD";
   char *buf = (char *)malloc(128);
   do {
     wret = write(sock, msg, strlen(msg));
     rret = read(sock, buf, 128);
   } while(wret &gt;= 0 &amp;&amp; rret);
}
</code></pre>
<p>This code is simple, and load is increased by creating more threads and connections that sit in a tight loop making requests to the SUT (System Under Test).  Closed loop testers can also be implemented with event loops that multiplex sockets among a few threads, but allow only 1 outstanding request per socket, which requires more connections to drive more concurrent load.  The <a href="https://github.com/wg/wrk">Wrk</a> load generator is an example of this. Some pitfalls with closed-loop testing:</p>
<ol>
<li>Maximum load achievable is coupled to the observed round-trip network latency between the driver and the SUT, if one SUT is further away than another from the load generator, it will see less throughput with the same number of connections.</li>
<li>Not good for testing non-client facing services, like middle-layer web-caches, that leverage request pipelining on small number of connections.</li>
<li>Can quickly saturate the CPU on the load generator because high loads require many threads and/or connections.</li>
</ol>
<p>Advantages of closed loop testers are: many to choose from, highly configurable and approximate client behavior well.
Closed loop load generator suggestions (we highly recommend the Wrk2 load generator):</p>
<ol>
<li><a href="https://github.com/giltene/wrk2">Wrk2</a></li>
<li><a href="https://github.com/wg/wrk">Wrk</a></li>
<li><a href="https://jmeter.apache.org/">JMeter</a></li>
</ol>
<h3 id="open-loop"><a class="header" href="#open-loop"><strong>Open loop</strong></a></h3>
<p>Open-loop generators generate load in an asynchronous fashion. They use non-blocking IO to send requests onto a small set of connections regardless of whether a response was returned for the previous request or not. Since open-loop generators send requests regardless of responses received, they can approximate a greater variety of load scenarios because the sending side will send requests according to a specified time schedule (i.e. every 100us on average following an exponential distribution) instead of waiting until responses from the SUT before generating new requests. This allows an open-loop tester to load the SUT more completely and be less sensitive to latency of the network connection to the SUT.</p>
<p>Some pitfalls of open-loop testing:</p>
<ol>
<li>Not common</li>
<li>Less configurable</li>
<li>Can not be used with SUTs that only allow synchronous connections.  For instance, many DBMS engines like <a href="https://www.postgresql.org/docs/current/libpq-async.html">Postgresql only allow a single command to be issued per connection</a>.</li>
</ol>
<p>Advantages of open loop testers are they are more accurate in measuring latency of an application than closed-loop testers.</p>
<p>Open loop load generator suggestions:</p>
<ol>
<li><a href="https://github.com/epfl-dcsl/lancet-tool">Lancet</a>, <a href="https://www.usenix.org/system/files/atc19-kogias-lancet.pdf">Usenix paper</a></li>
<li><a href="https://github.com/leverich/mutilate">Mutilate</a></li>
<li><a href="https://github.com/shaygalon/memcache-perf">MCPerf</a></li>
<li><a href="https://trex-tgn.cisco.com/">TRex</a> - This is for high performance stateless network testing</li>
</ol>
<h2 id="ebpf-extended-berkley-packet-filter--inspecting-fine-grained-code-behavior"><a class="header" href="#ebpf-extended-berkley-packet-filter--inspecting-fine-grained-code-behavior">eBPF (extended Berkley Packet Filter) — “Inspecting fine-grained code behavior”</a></h2>
<p>If you find a specific function that is mis-behaving, either putting the CPU to sleep more often, or taking longer to execute, but manual code inspection and flamegraphs are not yielding results, you can use eBPF to obtain detailed statistics on the function’s behavior. This can be leveraged for investigating reasons a function causes large latency spikes. Or measuring if it behaves differently on one machine compared to the other. These tools are useful for measuring a very specific issue, are extremely powerful and flexible. Warning: these tools can also have high overhead as they measure an event every time it happens compared to the sampling methods used in the previous sections.  If measuring something that happens millions of times a second, any injected eBPF will also run at that rate.</p>
<ol>
<li>Install the BCC tools</li>
</ol>
<pre><code class="language-bash"># AL2 
%&gt; sudo `amazon-linux-extras enable BCC
%&gt; sudo yum install bcc
  
# Ubuntu
%&gt; sudo apt-get install linux-headers-$(uname -r) bpfcc-tools`
</code></pre>
<ol start="2">
<li>Write an eBPF program in C that can be embedded in a python file</li>
<li>Run the eBPF program using the python to interface via the BCC tools and collect the statistics</li>
<li>Review the statistics and compare against the x86 SUT to compare.</li>
</ol>
<p>Example eBPF programs can be found: https://github.com/iovisor/bcc.</p>
<h3 id="capturing-coherent-mesh-network-hardware-event-counters"><a class="header" href="#capturing-coherent-mesh-network-hardware-event-counters">Capturing Coherent Mesh Network hardware event counters</a></h3>
<p>The CMN PMU counts events such as requests to DRAM (memory bandwidth), virtual memory manangement events,
I/O bus requests and coherence snoop events. These metrics can be used to assess an application's utilization
of such system level resources and if resources are used efficiently.
CMN counters are only accessible on metal-type instances and certain OSes and kernels.</p>
<div class="table-wrapper"><table><thead><tr><th>Distro</th><th>Kernel</th><th>Graviton2 (6g)</th><th>Graviton3 (7g)</th><th>Graviton4 (8g)</th></tr></thead><tbody>
<tr><td>Ubuntu-20.04</td><td>5.15</td><td>yes</td><td>no</td><td>no</td></tr>
<tr><td>Ubuntu-20.04</td><td>&gt;=5.19</td><td>yes</td><td>yes</td><td>no</td></tr>
<tr><td>Ubuntu-22.04</td><td>&gt;=5.19</td><td>yes</td><td>yes</td><td>no</td></tr>
<tr><td>Ubuntu-24.04</td><td>&gt;=6.8.0</td><td>yes</td><td>yes</td><td>yes</td></tr>
<tr><td>AL2023</td><td>6.1.2</td><td>yes</td><td>yes</td><td>no</td></tr>
</tbody></table>
</div>
<p>General procedure on Ubuntu</p>
<pre><code>sudo apt install linux-modules-extra-aws
sudo modprobe arm-cmn
ls /sys/devices/arm_cmn_0/events
</code></pre>
<p>On AL2023/AL2:</p>
<pre><code>sudo modprobe arm_cmn
ls /sys/devices/arm_cmn_0/events
</code></pre>
<p>Examples for capturing events:</p>
<pre><code>sudo perf stat -e /arm_cmn_0/hnf_mc_reqs/ sleep 15 #count of memory request
sudo perf stat -e /arm_cmn_0/rnid_rxdat_flits/ sleep 15 #count AXI 'master' read requests
sudo perf stat -e /arm_cmn_0/rnid_txdat_flits/ sleep 15 #count AXI 'master' write requests
</code></pre>
<p>For further information about specific events and useful ratios, please refer to:</p>
<p><a href="https://developer.arm.com/documentation/100180/0302/?lang=en">ARM documentation for Graviton2's CMN-600</a></p>
<p><a href="https://developer.arm.com/documentation/101481/0200/?lang=en">ARM documentation for Graviton3's CMN-650</a></p>
<p><a href="https://developer.arm.com/documentation/102308/latest/">ARM documentation for Graviton4's CMN-700</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="references"><a class="header" href="#references">References</a></h1>
<p><a href="perfrunbook/./README.html">Graviton Performance Runbook toplevel</a></p>
<p>Experimental design:</p>
<ul>
<li><a href="http://highscalability.com/blog/2015/10/5/your-load-generator-is-probably-lying-to-you-take-the-red-pi.html">Your load generator is probably lying to you</a></li>
<li><a href="https://www.itl.nist.gov/div898/handbook/pri/section3/pri3.htm">NIST Engineering Statistics: Choosing an experimental design</a></li>
</ul>
<p>Performance measurement tools:</p>
<ul>
<li><a href="https://drive.google.com/file/d/0B_SDNxjh2Wbcc0lWemFNSGMzLTA/view">Top-down performance analysis</a></li>
<li><a href="http://www.brendangregg.com/overview.html">Brendan Gregg's homepage</a></li>
<li><a href="https://github.com/brendangregg/FlameGraph">Flamegraph homepage</a></li>
<li>https://github.com/andikleen/pmu-tools</li>
<li><a href="https://linux.die.net/man/8/netstat">Netstat man-page</a></li>
<li><a href="https://linux.die.net/man/1/sar">Sar man-page</a></li>
<li><a href="https://linux.die.net/man/1/perf-stat">perf-stat man-page</a></li>
<li><a href="https://linux.die.net/man/1/perf-record">perf-record man-page</a></li>
<li><a href="https://linux.die.net/man/1/perf-annotate">perf-annotate man-page</a></li>
</ul>
<p>Optimization and tuning:</p>
<ul>
<li><a href="https://gcc.gnu.org/onlinedocs/gcc-10.2.0/gcc.pdf">GCC10 manual</a></li>
<li><a href="https://aws.amazon.com/blogs/compute/optimizing-network-intensive-workloads-on-amazon-ec2-a1-instances/">Optimizing network intensive workloads on Graviton1</a></li>
<li><a href="https://aws.amazon.com/blogs/compute/optimizing-nginx-load-balancing-on-amazon-ec2-a1-instances/">Optimizing NGINX on Graviton1</a></li>
<li><a href="https://github.com/amazonlinux/autotune/blob/master/src/ec2sys_autotune/ec2_instance_network_cfg_gen.py#L63-L64">sysctl tunings</a></li>
<li><a href="https://github.com/amazonlinux/autotune">AL2 auto-tuning</a></li>
</ul>
<p>Hardware reference manuals:</p>
<ul>
<li><a href="https://developer.arm.com/documentation/102105/latest">Arm64 Architecture Reference Manual</a></li>
<li><a href="https://developer.arm.com/documentation/100616/0400/debug-descriptions/performance-monitor-unit/pmu-events">Neoverse N1 Technical Reference Manual</a></li>
<li>Reference for Intel CPU PMU counters (c5/m5/r5): <a href="https://software.intel.com/content/dam/develop/external/us/en/documents-tps/253669-sdm-vol-3b.pdf">Intel 64 and IA-32 Architecture Software Developer’s Manual, Volume 3B Chapter 19</a></li>
<li>Reference for AMD CPU PMU counters (c5a): <a href="https://www.amd.com/system/files/TechDocs/56176_ppr_Family_17h_Model_71h_B0_pub_Rev_3.06.zip">Processor Programming Reference (PPR) for AMD Family 17h Model 71h, Revision B0 Processors Section 2.1.15</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="runtime-feature-detection-on-graviton"><a class="header" href="#runtime-feature-detection-on-graviton">Runtime Feature Detection on Graviton</a></h1>
<p>Different generations of Graviton support different features. For example, Graviton3 supports SVE but Graviton2 does
not. Graviton4 supports SVE2 and SVE. For some applications it may be advantageous to implement performance critical
kernels to make use of the highest performing feature set available which may not be known until runtime. For this, the
best practice is to consult HWCAPS. This guide covers the procedure for feature detection on Linux.</p>
<p>There are several existing libraries which can provide this runtime detection, one such example is <a href="https://github.com/pytorch/cpuinfo">PyTorch's
cpuinfo</a>. If you do not wish to use a library, continue reading to learn how to
implement this functionality yourself.</p>
<p>On Linux, the standard C library implements <code>getauxval</code> to read from the auxiliary vector. Reading the <code>AT_HWCAP</code> and
<code>AT_HWCAP2</code> values will return the hardware capability bitmaps which are filtered by the kernel to include only features
which the kernel also supports. The library function is defined in <code>sys/auxv.h</code> and the constants for masking the bitmap
are defined in <code>asm/hwcap.h</code>. Below is a <a href="sample-code/hwcaps-test.c">sample C program</a> which implements two versions of
<code>sum_all</code> with and without SVE2. The program then tests for support for SVE2 and then uses that function only when the
runtime system is detected to support it. This program can be compiled on any generation of Graviton and then executed
on any generation of Graviton and select the correct function at runtime. Indeed, when testing the code for this
article, the author compiled the program on Ubuntu 24.04 with GCC 13.3 on Graviton1 and then subsequently tested on all
generations, including Graviton4, which supports SVE2.</p>
<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;sys/auxv.h&gt;
#include &lt;asm/hwcap.h&gt;

#include &lt;arm_sve.h&gt;

#define sizeof_array(a) (sizeof(a) / sizeof((a)[0]))

uint64_t sum_all(uint32_t *values, int length)
{
    uint64_t sum = 0;
    for (int i = 0; i &lt; length; i++)
        sum += values[i];
    return sum;
}

#pragma GCC target("+sve2")
#pragma clang attribute push(__attribute__((target("sve2"))), apply_to = function)
uint64_t sum_all_sve2(uint32_t *values, int length)
{
    svuint64_t sum = svdup_u64(0);
    int i = 0;
    svbool_t predicate = svwhilelt_b32(i, length);
    do {
        svuint32_t a = svld1(predicate, (uint32_t *) &amp;values[i]);
        sum = svadalp_u64_x(predicate, sum, a);
        i += svcntw();
        predicate = svwhilelt_b32(i, length);
    } while (svptest_any(svptrue_b32(), predicate));
    return svaddv_u64(svptrue_b64(), sum);
}
#pragma clang attribute pop

void test() {
    uint32_t values[13] = {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13};

    int have_sve = !!(getauxval(AT_HWCAP2) &amp; HWCAP2_SVE2);
    uint64_t sum = 0;
    if (have_sve) {
        sum = sum_all_sve2(&amp;values[0], sizeof_array(values));
    } else {
        sum = sum_all(&amp;values[0], sizeof_array(values));
    }

    printf("sum: %lu, computed %s SVE2\n", sum, (have_sve) ? "with" : "without");
}

int main(int argc, char *argv[])
{
    test();
    return 0;
}
</code></pre>
<p>As shown, you will need to compile files which contain instructions or intrinsics for specific features with additional
compiler flags. This can be done by placing these functions in separate files and using your build infrastructure to add
additonal flags for those files, or it is also possible to embed these compiler directives directly on the functions to
which they apply. In an assembly file this can be accomplished with an assembler directive in the file. For example to
assemble with support for SVE instructions, include the following:</p>
<pre><code>.arch  armv8-2+sve
</code></pre>
<p>In a C or C++ file, adding a pragama can achieve something similar:</p>
<pre><code>#pragma GCC target("+sve")
#pragma clang attribute push(__attribute__((target("sve2"))), apply_to = function)
...
#pragma clang attribute pop
</code></pre>
<p>Once runtime detection is complete, use any polymorphism technique you wish to dispatch calls to the appropriate
versions of the functions, such as C++ classes, <a href="https://sourceware.org/glibc/wiki/GNU_IFUNC">ifuncs</a>, function
pointers, or a simpler control flow approach as shown above.</p>
<p>It is possible to consult system registers directly on the processor to detect support for different capabilities,
however this approach has some problems. For example, checking for SVE support by this method obscures the fact that the
kernel must also be configured for SVE support since the width of SVE registers can be different from NEON and so
context switching code must accommodate the different width. Without this kernel support, a context switch could result
in corruption of the content of SVE registers. Because of this, the processor is configured to trap executions of SVE
instructions by default and this trap must be disabled, a job done by the kernel if it is configured to support SVE.</p>
<h2 id="hwcap-by-generation"><a class="header" href="#hwcap-by-generation">HWCAP by Generation</a></h2>
<p><a href="sample-code/hwcaps.c">A simple program</a> can check and display the all capability bits on a system. This table shows the
results of the linked program executed on recent Linux Kernel. For more information on the meaning of each capability,
consult the <a href="https://developer.arm.com/documentation/ddi0487/latest/">Arm Architectural Reference Manual</a>. Most of the
features can be found by searching in the PDF for <code>FEAT_</code> + the symbol which follows <code>HWCAP_</code> in the table below.</p>
<div class="table-wrapper"><table><thead><tr><th>CAP</th><th>Graviton1</th><th>Graviton2</th><th>Graviton3</th><th>Graviton4</th></tr></thead><tbody>
<tr><td>HWCAP_FP</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_ASIMD</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_EVTSTRM</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_AES</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_PMULL</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_SHA1</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_SHA2</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_CRC32</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_ATOMICS</td><td></td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_FPHP</td><td></td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_ASIMDHP</td><td></td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_CPUID</td><td>*</td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_ASIMDRDM</td><td></td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_JSCVT</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_FCMA</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_LRCPC</td><td></td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_DCPOP</td><td></td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_SHA3</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_SM3</td><td></td><td></td><td>*</td><td></td></tr>
<tr><td>HWCAP_SM4</td><td></td><td></td><td>*</td><td></td></tr>
<tr><td>HWCAP_ASIMDDP</td><td></td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_SHA512</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_SVE</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_ASIMDFHM</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_DIT</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_USCAT</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_ILRCPC</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_FLAGM</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_SSBS</td><td></td><td>*</td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_SB</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP_PACA</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP_PACG</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP2_DCPODP</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP2_SVE2</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP2_SVEAES</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP2_SVEPMULL</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP2_SVEBITPERM</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP2_SVESHA3</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP2_SVESM4</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_FLAGM2</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP2_FRINT</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP2_SVEI8MM</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP2_SVEF32MM</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SVEF64MM</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SVEBF16</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP2_I8MM</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP2_BF16</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP2_DGH</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP2_RNG</td><td></td><td></td><td>*</td><td>*</td></tr>
<tr><td>HWCAP2_BTI</td><td></td><td></td><td></td><td>*</td></tr>
<tr><td>HWCAP2_MTE</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_ECV</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_AFP</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_RPRES</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_MTE3</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_I16I64</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_F64F64</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_I8I32</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_F16F32</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_B16F32</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_F32F32</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_FA64</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_WFXT</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_EBF16</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SVE_EBF16</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_CSSC</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_RPRFM</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SVE2P1</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME2</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME2P1</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_I16I32</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_BI32I32</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_B16B16</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SME_F16F16</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_MOPS</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_HBC</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_SVE_B16B16</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_LRCPC3</td><td></td><td></td><td></td><td></td></tr>
<tr><td>HWCAP2_LSE128</td><td></td><td></td><td></td><td></td></tr>
</tbody></table>
</div>
<h2 id="see-also"><a class="header" href="#see-also">See Also</a></h2>
<ul>
<li>https://www.kernel.org/doc/html/v5.4/arm64/elf_hwcaps.html</li>
<li>https://man7.org/linux/man-pages/man3/getauxval.3.html</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="dpdk-spdk-isa-l-supports-graviton"><a class="header" href="#dpdk-spdk-isa-l-supports-graviton">DPDK, SPDK, ISA-L supports Graviton</a></h1>
<p>Graviton2 and later CPUs are optimized for data path functions like networking and storage.  Users of <a href="https://github.com/dpdk/dpdk">DPDK</a> and <a href="https://github.com/spdk/spdk">SPDK</a> can download and compile natively on Graviton following the normal installation guidelines from the respective repositories linked above.</p>
<p><strong>NOTE</strong>: <em>Though DPDK precompiled packages are available from Ubuntu but we recommend building them from source.</em></p>
<p>SPDK relies often on <a href="https://github.com/intel/isa-l">ISA-L</a> which is already optimized for Arm64 and the CPU cores in Graviton2 and later processors.</p>
<h2 id="compile-dpdk-from-source"><a class="header" href="#compile-dpdk-from-source">Compile DPDK from source</a></h2>
<p><a href="https://doc.dpdk.org/guides/linux_gsg/build_dpdk.html">DPDK official guidelines</a> requires using <em>meson</em> and <em>ninja</em> to build from source code.</p>
<p>A native compilation of DPDK on top of Graviton will generate optimized code that take advantage of the CRC and Crypto instructions in Graviton2 and later cpu cores.</p>
<p><strong>NOTE</strong>: Some of the installations steps call "python" which may not be valid command in modern linux distribution,  you may need to install <em>python-is-python3</em> to resolve this.</p>
<h3 id="older-dpdk-version-with-makefile-based-build"><a class="header" href="#older-dpdk-version-with-makefile-based-build">Older DPDK version with makefile-based build</a></h3>
<p>If a developer is using the makefile-based build (vs the newer <em>meson</em>), the following <a href="https://www.mail-archive.com/dev@dpdk.org/msg179445.html">patch</a> will enable a Graviton2 optimized build.</p>
<h2 id="performance-consideration"><a class="header" href="#performance-consideration">Performance consideration</a></h2>
<h3 id="optimal-rte-settings"><a class="header" href="#optimal-rte-settings">Optimal RTE settings</a></h3>
<p>In some older releases (prior to DPDK 20.11), some default parameters are not optimal and developers should check the values:</p>
<ul>
<li>RTE_MAX_LCORE, should at least be 64</li>
<li>RTE_CACHE_LINE_SIZE=64</li>
</ul>
<h3 id="number-of-lcores-used-could-be-misconfigured"><a class="header" href="#number-of-lcores-used-could-be-misconfigured">Number of LCores used could be misconfigured</a></h3>
<p>Some application, written with the x86 architecture in mind, set the active dpdk threads or lcores to 1/2 number of vCPU to run single thread per physical core on x86 and disabling SMT.  However, in Graviton, every vCPU is a full CPU, and a developer can use more threads or lcores than same size x86-based instance.   For example, a c5.16xl has 64vCPU or 32 physical cores,  but some DPDK application would only run on 32 to guarantee one thread per physical core.   In c6g.16xl, developer can use 64 physical cores.</p>
<h2 id="known-issues"><a class="header" href="#known-issues">Known issues</a></h2>
<ul>
<li><strong>testpmd:</strong> The flowgen function of testpmd does not work correctly when compiled with GCC 9 and above. It generates IP packets with wrong checksum which are dropped when transmitted between AWS instances (including Graviton). This is a known issue and there is a <a href="https://patches.dpdk.org/patch/84772/">patch</a> that fixes it.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="taking-advantage-of-arm-advanced-simd-instructions"><a class="header" href="#taking-advantage-of-arm-advanced-simd-instructions">Taking advantage of Arm Advanced SIMD instructions</a></h1>
<h2 id="background"><a class="header" href="#background">Background</a></h2>
<p>Arm-v8 architecture include Advanced-SIMD instructions (NEON) helping boost performance for many applications that can take advantage of the wide registers.</p>
<p>A lot of the applications and libraries already taking advantage of Arm's Advanced-SIMD, yet this guide is written for developers writing new code or libraries. We'll guide on various ways to take advantage of these instructions, whether through compiler auto-vectorization or writing intrinsics.</p>
<p>Later we'll explain how to build portable code, that would detect at runtime which instructions are available at the specific cores, so developers can build one binary that supports cores with different capabilities. For example, to support one binary that would run on Graviton1, Graviton2, and arbitrary set of Android devices with Arm v8.x support.</p>
<h2 id="compiler-driven-auto-vectorization"><a class="header" href="#compiler-driven-auto-vectorization">Compiler-driven auto-vectorization</a></h2>
<p>Compilers keep improving to take advantage of the SIMD instructions without developers explicit guidance or specific coding style.</p>
<p>In general, GCC 9 has good support for auto-vectorization, while GCC 10 has shown impressive improvement over GCC 9 in most cases.</p>
<p>Compiling with <em>-fopt-info-vec-missed</em> is good practice to check which loops were not vectorized.</p>
<h3 id="how-minor-code-changes-improve-auto-vectorization"><a class="header" href="#how-minor-code-changes-improve-auto-vectorization">How minor code changes improve auto-vectorization</a></h3>
<p>The following example was run on Graviton2, with Ubuntu 20.04 and gcc 9.3.   Different combinations of server and compiler version may show different results</p>
<p>Starting code looked like:</p>
<pre><code>  1 // test.c 
...
  5 float   a[1024*1024];
  6 float   b[1024*1024];
  7 float   c[1024*1024];
.....
 37 for (j=0; j&lt;128;j++) { // outer loop, not expected to be vectorized
 38   for (i=0; i&lt;n ; i+=1){ // inner loop, ideal for vectorization
 39         c[i]=a[i]*b[i]+j;
 40   }
 41 }
</code></pre>
<p>and compiling:</p>
<pre><code>$ gcc test.c -fopt-info-vec-missed -O3
test.c:37:1: missed: couldn't vectorize loop
test.c:39:8: missed: not vectorized: complicated access pattern.
test.c:38:1: missed: couldn't vectorize loop
test.c:39:6: missed: not vectorized: complicated access pattern.
</code></pre>
<p>Line 37 is the outer loop and that's not expected to be vectorized
but the inner loop is prime candidate for vectorization, yet it was not done in this case</p>
<p>A small change to the code, to guarantee that the inner loop would always be aligned to 128-bit SIMD will be enough for gcc 9 to auto-vectorize it</p>
<pre><code>  1 // test.c 
...
  5 float   a[1024*1024];
  6 float   b[1024*1024];
  7 float   c[1024*1024];
...
 19 #if(__aarch64__)
 20 #define ARM_NEON_WIDTH  128
 21 #define VF32    ARM_NEON_WIDTH/32
 22 #else
 23 #define VF32    1
 33 #endif
...
 37 for (j=0; j&lt;128;j++) { // outer loop, not expected to be vectorized
 38   for (i=0; i&lt;( n - n%VF32 ); i+=1){ // forcing inner loop to multiples of 4 iterations
 39         c[i]=a[i]*b[i]+j;
 40   }
 41   // epilog loop
 42   if (n%VF32)
 43         for ( ; i &lt; n; i++) 
 44                 c[i] = a[i] * b[i]+j;
 45 }
</code></pre>
<p>The code above is forcing the inner loop to iterate multiples of 4 (128-bit SIMD / 32-bit per float). Results:</p>
<pre><code>$ gcc test.c -fopt-info-vec-missed -O3
test.c:37:1: missed: couldn't vectorize loop
test.c:37:1: missed: not vectorized: loop nest containing two or more consecutive inner loops cannot be vectorized
</code></pre>
<p>And the outer loop is still not vectorized as expected, but the inner loop is vectorized (and 3-4X faster).</p>
<p>Again, as compiler capabilities improve over time, the need for such techniques may no longer be needed. However, as long as target applications being built with gcc9 or older, this continues to be a good practice to follow.</p>
<h2 id="using-intrinsics"><a class="header" href="#using-intrinsics">Using intrinsics</a></h2>
<p>One way to build portable code is to use the intrinsic instructions defined by Arm and implemented by GCC and Clang.</p>
<p>The instructions are defined in <em>arm_neon.h</em>.</p>
<p>A portable code that would detect (at compile-time) an Arm CPU and compiler would look like:</p>
<pre><code>#if (defined(__clang__) || (defined(__GCC__)) &amp;&amp; (defined(__ARM_NEON) || defined(__aarch64__))
/* compatible compiler, targeting arm neon */
#include &lt;arm_neon.h&gt;
#include &lt;arm_acle.h&gt;
#endif
</code></pre>
<h2 id="runtime-detection-of-supported-simd-instructions"><a class="header" href="#runtime-detection-of-supported-simd-instructions">Runtime detection of supported SIMD instructions</a></h2>
<p>While Arm architecture version mandates specific instructions support, certain instructions are optional for a specific version of the architecture.</p>
<p>For example, a cpu core compliant with Arm-v8.4 architecture must support dot-product, but dot-products are optional in Arm-v8.2 and Arm-v8.3.
Graviton2 is Arm-v8.2 compliant, but supports both CRC and dot-product instructions.</p>
<p>A developer wanting to build an application or library that can detect the supported instructions in runtime, can follow this example:</p>
<pre><code>#include&lt;sys/auxv.h&gt;
......
  uint64_t hwcaps = getauxval(AT_HWCAP);
  has_crc_feature = hwcaps &amp; HWCAP_CRC32 ? true : false;
  has_lse_feature = hwcaps &amp; HWCAP_ATOMICS ? true : false;
  has_fp16_feature = hwcaps &amp; HWCAP_FPHP ? true : false;
  has_dotprod_feature = hwcaps &amp; HWCAP_ASIMDDP ? true : false;
  has_sve_feature = hwcaps &amp; HWCAP_SVE ? true : false;
</code></pre>
<p>The full list of arm64 hardware capabilities is defined in <a href="https://github.com/bminor/glibc/blob/master/sysdeps/unix/sysv/linux/aarch64/bits/hwcap.h">glibc header file</a> and in the <a href="https://github.com/torvalds/linux/blob/master/arch/arm64/include/asm/hwcap.h">Linux kernel</a>.</p>
<h2 id="porting-codes-with-sseavx-intrinsics-to-neon-1"><a class="header" href="#porting-codes-with-sseavx-intrinsics-to-neon-1">Porting codes with SSE/AVX intrinsics to NEON</a></h2>
<h3 id="detecting-arm64-systems"><a class="header" href="#detecting-arm64-systems">Detecting arm64 systems</a></h3>
<p>Projects may fail to build on arm64 with <code>error: unrecognized command-line option '-msse2'</code>, or <code>-mavx</code>, <code>-mssse3</code>, etc.  These compiler flags enable x86
vector instructions.  The presence of this error means that the build system may
be missing the detection of the target system, and continues to use the x86
target features compiler flags when compiling for arm64.</p>
<p>To detect an arm64 system, the build system can use:</p>
<pre><code># (test $(uname -m) = "aarch64" &amp;&amp; echo "arm64 system") || echo "other system"
</code></pre>
<p>Another way to detect an arm64 system is to compile, run, and check the return
value of a C program:</p>
<pre><code># cat &lt;&lt; EOF &gt; check-arm64.c
int main () {
#ifdef __aarch64__
  return 0;
#else
  return 1;
#endif
}
EOF

# gcc check-arm64.c -o check-arm64
# (./check-arm64 &amp;&amp; echo "arm64 system") || echo "other system"
</code></pre>
<h3 id="translating-x86-intrinsics-to-neon"><a class="header" href="#translating-x86-intrinsics-to-neon">Translating x86 intrinsics to NEON</a></h3>
<p>When programs contain code with x64 intrinsics, the following procedure can help
to quickly obtain a working program on Arm, assess the performance of the
program running on Graviton processors, profile hot paths, and improve the
quality of code on the hot paths.</p>
<p>To quickly get a prototype running on Arm, one can use
<a href="https://github.com/simd-everywhere/simde">SIMDe (SIMD everywhere)</a>
a translator of x64 intrinsics to NEON.</p>
<p>For example, to port code using AVX2 intrinsics to Graviton,
a developer could add the following code:</p>
<pre><code>#define SIMDE_ENABLE_NATIVE_ALIASES
#include "simde/x86/avx2.h"
</code></pre>
<p>SIMDe provides a quick starting point to port performance critical codes
to Arm.  It shortens the time needed to get an Arm working program that then
can be used to extract profiles and to identify hot paths in the code.
Once a profile is established, the hot paths can be rewritten directly with
NEON intrinsics to avoid the overhead of the generic translation.</p>
<h2 id="additional-resources-3"><a class="header" href="#additional-resources-3">Additional resources</a></h2>
<ul>
<li><a href="https://developer.arm.com/architectures/instruction-sets/intrinsics/">Neon Intrinsics</a></li>
<li><a href="https://developer.arm.com/documentation/102159/latest/">Coding for Neon</a></li>
<li><a href="https://developer.arm.com/architectures/instruction-sets/simd-isas/neon/neon-programmers-guide-for-armv8-a">Neon Programmer's Guide for Armv8-A</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="assembly-optimization-guide-for-graviton-arm64-processors"><a class="header" href="#assembly-optimization-guide-for-graviton-arm64-processors">Assembly Optimization Guide for Graviton Arm64 Processors</a></h1>
<h2 id="introduction-2"><a class="header" href="#introduction-2">Introduction</a></h2>
<p>This document is a reference for software developers who want to optimize code
for Graviton Arm64 processors at the assembly level. Some of the documented
transforms could be applied to optimize code written in higher level languages
like C or C++, and in that respect this document is not restrictive to assembly
level programming. This guide isn’t intended cover every aspect of writing in
assembly, such as assembler syntax or ABI details.</p>
<p>The code patterns in this document could also be useful to spot inefficient code
generated for the hot code. Profilers such as Linux perf allow the inspection of
the assembly level code. During the review of the hot code, software developers
can reference this guide to find better ways to optimize inner loops.</p>
<p>To develop code for AWS Gravition, we always recommend developing and testing on
Graviton itself to ensure that the optimizations you make are targeted toward
Graviton. The microarchitecture of other Arm64 cores may be different in ways
that lead to different optimizations.</p>
<p>Some techniques for writing optimized assembly:</p>
<ol>
<li><a href="arm64-assembly-optimization.html#instruction-level-parallelism">Be aware of instruction level parallelism</a></li>
<li><a href="arm64-assembly-optimization.html#split-data-dependency-chains">Split Data Dependency Chains</a></li>
<li><a href="arm64-assembly-optimization.html#modulo-scheduling">Modulo Scheduling</a></li>
<li><a href="arm64-assembly-optimization.html#test-everything">Test Everything</a></li>
<li><a href="arm64-assembly-optimization.html#specialize-functions-for-known-input-conditions">Specialize functions for known input conditions</a></li>
<li><a href="arm64-assembly-optimization.html#use-efficient-instructions">Use Efficient Instructions</a></li>
</ol>
<p>We will be adding more sections to this document soon, so check back!</p>
<h3 id="other-resources"><a class="header" href="#other-resources">Other resources</a></h3>
<p>This guide aims only to provide tips on optimizing assembly or intrinsics code.
There are other resources available on the web which can help you to learn
concepts and others which you can refer to as references. Here is a list of
helpful pages.</p>
<ul>
<li>https://mariokartwii.com/armv8/ - a full book on how to get start writing
ARMv8 assembly</li>
<li>https://github.com/pkivolowitz/asm_book - another guide to get starting
writing assembly</li>
<li>https://github.com/slothy-optimizer/slothy  - a tool which can optimize
instruction scheduling, register allocation, and software pipeline of assembly
functions</li>
<li>https://developer.arm.com/architectures/instruction-sets/intrinsics - a
reference of intrinsics functions when you want to make direct use of SIMD
instructions from C/C++</li>
<li>https://dougallj.github.io/asil/index.html - a reference of SVE instructions
and their intrinsic function names by SVE version and extension feature
availability</li>
<li>https://github.com/ARM-software/abi-aa/blob/main/aapcs64/aapcs64.rst#the-base-procedure-call-standard -
a reference to the calling convention for aarch64, which is particularly
helpful to know when registers to avoid or to spill to the stack when they are
needed for use</li>
<li>https://developer.arm.com/documentation/ddi0602/2024-03 - the complete
reference for instructions in the Arm A-profile A64 ISA, which can be
downloaded as a PDF</li>
</ul>
<h2 id="instruction-level-parallelism"><a class="header" href="#instruction-level-parallelism">Instruction Level Parallelism</a></h2>
<p>When writing in C, or any higher level language, the programmer writes a sequence
of statements which are presumed to execute in strict sequence. In the following
example, each statement logically occurs after the previous one.</p>
<pre><code class="language-c">int foo (int a, int b, int c)
{
    if (c &lt; 0) {
        int d = a + b;
        return d;
    } else {
        int e = a * b;
        return e;
    }
}
</code></pre>
<p>However when we compile this code with gcc version 7.3 with <code>-O1</code>, this is the output:</p>
<pre><code>foo(int, int, int):
    add     w3, w0, w1
    mul     w0, w0, w1
    cmp     w2, 0
    csel    w0, w0, w3, ge
    ret
</code></pre>
<p>Here we can see the add and the multiply instruction are executed independently
of the result of <code>c &lt; 0</code>. The compiler knows that the CPU can execute these
instructions in parallel. If we use the machine code analyzer from the LLVM
project, we can can see what is happening. We will use Graviton2 as our target
platform, which uses the Neoverse-N1 core.</p>
<p><code>llvm-mca -mcpu=neoverse-n1 -timeline -iterations 1 foo.s</code></p>
<pre><code>...
Timeline view:
Index     012345

[0,0]     DeER .   add  w3, w0, w1
[0,1]     DeeER.   mul  w0, w0, w1
[0,2]     DeE-R.   cmp  w2, #0
[0,3]     D==eER   csel w0, w0, w3, ge
[0,4]     DeE--R   ret
...
</code></pre>
<p>In this output we can see that all five instructions from this function are decoded
in parallel in the first cycle. Then in the second cycle the instructions for which
all dependencies are already resolved begin executing. There is only one instruction
which has dependencies: the conditional select, <code>csel</code>. The four remaining
instructions all begin executing, including the <code>ret</code> instruction at the end. In
effect, nearly the entire C function is executing in parallel. The
<code>csel</code> has to wait for the result of the comparison instruction to check
<code>c &lt; 0</code> and the result of the multiplication. Even the return has
completed by this point and the CPU front end has already decoded
instructions in the return path and some of them will already be
executing as well.</p>
<p>It is important to understand this when writing assembly. Instructions
do not execute in the order they appear. Their effects will be logically
sequential, but execution order will not be.</p>
<p>When writing assembly for arm64 or any platform, be aware that the CPU has more
than one pipeline of different types. The types and quantities are outlined in
the Software Optimization Guide, often abbreviated as SWOG. The guide
for each Graviton processor is linked from the <a href="README.html">main page of this
technical guide</a>. Using this knowledge, the programmer can arrange instructions of
different types next to each other to take advantage of instruction level
parallelism, ILP. For example, interleaving load instructions with vector or
floating point instructions can keep both pipelines busy.</p>
<h2 id="split-data-dependency-chains"><a class="header" href="#split-data-dependency-chains">Split Data Dependency Chains</a></h2>
<p>As we saw in the last section, Graviton has multiple pipelines or execution units which can execute instructions. The instructions may execute in parallel if all of the input dependencies have been met but a series of instructions each of which depend on a result from the previous one will not be able to efficiently utilize the resources of the CPU.</p>
<p>For example, a simple C function which takes 64 signed 8-bit integers and adds them all up into one value could be implemented like this:</p>
<pre><code>int16_t add_64(int8_t *d) {
    int16_t sum = 0;
    for (int i = 0; i &lt; 64; i++) {
        sum += d[i];
    }
    return sum;
}
</code></pre>
<p>We could write this in assembly using NEON SIMD instructions like this:</p>
<pre><code>add_64_neon_01:
    ld1     {v0.16b, v1.16b, v2.16b, v3.16b} [x0]   // load all 64 bytes into vector registers v0-v3
    movi    v4.2d, #0                               // zero v4 for use as an accumulator
    saddw   v4.8h, v4.8h, v0.8b                     // add bytes 0-7 of v0 to the accumulator
    saddw2  v4.8h, v4.8h, v0.16b                    // add bytes 8-15 of v0 to the accumulator
    saddw   v4.8h, v4.8h, v1.8b                     // ...
    saddw2  v4.8h, v4.8h, v1.16b
    saddw   v4.8h, v4.8h, v2.8b
    saddw2  v4.8h, v4.8h, v2.16b
    saddw   v4.8h, v4.8h, v3.8b
    saddw2  v4.8h, v4.8h, v3.16b
    addv    h0, v4.8h                               // horizontal add all values in the accumulator to h0
    fmov    w0, h0                                  // copy vector registor h0 to general purpose register w0
    ret                                             // return with the result in w0
</code></pre>
<p>In this example, we use a signed add-wide instruction which adds the top and bottom of each register to v4 which is used to accumulate the sum. This is the worst case for data dependency chains because every instruction depends on the result of the previous one which will make it impossible for the CPU to achieve any instruction level parallelism. If we use <code>llvm-mca</code> to evaluate it we can see this clearly.</p>
<pre><code>Timeline view:
                    0123456789
Index     0123456789          0123456

[0,0]     DeeER.    .    .    .    ..   movi    v4.2d, #0000000000000000
[0,1]     D==eeER   .    .    .    ..   saddw   v4.8h, v4.8h, v0.8b
[0,2]     D====eeER .    .    .    ..   saddw2  v4.8h, v4.8h, v0.16b
[0,3]     D======eeER    .    .    ..   saddw   v4.8h, v4.8h, v1.8b
[0,4]     D========eeER  .    .    ..   saddw2  v4.8h, v4.8h, v1.16b
[0,5]     D==========eeER.    .    ..   saddw   v4.8h, v4.8h, v2.8b
[0,6]     D============eeER   .    ..   saddw2  v4.8h, v4.8h, v2.16b
[0,7]     D==============eeER .    ..   saddw   v4.8h, v4.8h, v3.8b
[0,8]     D================eeER    ..   saddw2  v4.8h, v4.8h, v3.16b
[0,9]     D==================eeeeER..   addv    h0, v4.8h
[0,10]    D======================eeER   fmov    w0, h0
[0,11]    DeE-----------------------R   ret
</code></pre>
<p>One way to break data dependency chains is to use commutative property of addition and change the order which the adds are completed. Consider the following alternative implementation which makes use of pairwise add instructions.</p>
<pre><code>add_64_neon_02:
    ld1     {v0.16b, v1.16b, v2.16b, v3.16b} [x0]

    saddlp  v0.8h, v0.16b                   // signed add pairwise long
    saddlp  v1.8h, v1.16b
    saddlp  v2.8h, v2.16b
    saddlp  v3.8h, v3.16b                   // after this instruction we have 32 16-bit values

    addp    v0.8h, v0.8h, v1.8h             // add pairwise again v0 and v1
    addp    v2.8h, v2.8h, v3.8h             // now we are down to 16 16-bit values

    addp    v0.8h, v0.8h, v2.8h             // 8 16-bit values
    addv    h0, v4.8h                       // add 8 remaining values across vector
    fmov    w0, h0

    ret
</code></pre>
<p>In this example the first 4 instructions after the load can execute independently and then the next 2 are also independent of each other.  However, the last 3 instructions do have data dependencies on each other. If we take a look with <code>llvm-mca</code> again, we can see that this implementation takes 17 cycles (excluding the initial load instruction common to both implementations) and the original takes 27 cycles.</p>
<pre><code>Timeline view:
                    0123456
Index     0123456789

[0,0]     DeeER.    .    ..   saddlp    v0.8h, v0.16b
[0,1]     DeeER.    .    ..   saddlp    v1.8h, v1.16b
[0,2]     D=eeER    .    ..   saddlp    v2.8h, v2.16b
[0,3]     D=eeER    .    ..   saddlp    v3.8h, v3.16b
[0,4]     D==eeER   .    ..   addp      v0.8h, v0.8h, v1.8h
[0,5]     D===eeER  .    ..   addp      v2.8h, v2.8h, v3.8h
[0,6]     D=====eeER.    ..   addp      v0.8h, v0.8h, v2.8h
[0,7]     .D======eeeeeER..   addv      h0, v0.8h
[0,8]     .D===========eeER   fmov      w0, h0
[0,9]     .DeE------------R   ret
</code></pre>
<h2 id="modulo-scheduling"><a class="header" href="#modulo-scheduling">Modulo Scheduling</a></h2>
<p>When writing a loop in assembly that contains several dependent steps in each
iteration but there are no dependencies between iterations (or only superficial
dependencies), modulo scheduling can improve performance. Modulo scheduling is
the processes of combining loop unrolling with the interleaving of steps from
other iterations of the loop. For example, suppose we have a loop which has four
steps, A, B, C, and D. We can unroll that loop to execute 4 iterations in
parallel with vector instructions such that we complete steps like this: A₀₋₃,
B₀₋₃, C₀₋₃, D₀₋₃. However this construction still has a dependency chain which
the last section taught us to avoid. We can break that chain by scheduling the
steps with modulo scheduling.</p>
<pre><code>A1
B1 A2
C1 B2 A3
D1 C2 B3 A4
   D2 C3 B4
      D3 C4
         D4
</code></pre>
<p>Take the following example written in C. I noted step A through step D in the
source, but the lines in C don’t map cleanly onto the steps I selected for
implementation in assembly.</p>
<pre><code>void modulo_scheduling_example_c_01(int16_t *destination, int dest_len,
         int8_t *src, uint32_t *src_positions,
         int16_t *multiplier)
{
    int i;
    for (i = 0; i &lt; dest_len * 8; i++)
    {
        // step A: load src_position[i] and multiplier[i]
        int src_pos = src_positions[i];
        // step B: load src[src_pos] and copy the value to a vector register
        // step C: sign extend int8_t to int16_t, multiply, and saturate-shift right
        int32_t val = multiplier[i] * src[src_pos];
        destination[i] = min(val &gt;&gt; 3, (1 &lt;&lt; 15) - 1);
        // step D: store the result to destination[i]
    }
}
</code></pre>
<p>A first pass implementation of this function using Neon instructions to unroll
the loop could look like this:</p>
<details>
<summary>
 Click here to see the (long) assembly implementation.
</summary>
<pre><code>    // x0: destination
    // w1: dest_len
    // x2: src
    // x3: src_positions
    // x4: multiplier
modulo_scheduling_example_asm_01:
    stp x19, x20, [sp, #-16]!

1:
    // src_pos = src_positions[i]
    ldp w5, w6, [x3]
    ldp w7, w8, [x3, 8]
    ldp w9, w10, [x3, 16]
    ldp w11, w12, [x3, 24]
    add x3, x3, #32

    // src[src_pos]
    ldrb w13, [x2,  w5, UXTW]
    ldrb w14, [x2,  w6, UXTW]
    ldrb w15, [x2,  w7, UXTW]
    ldrb w16, [x2,  w8, UXTW]

    ldrb w17, [x2,  w9, UXTW]
    ldrb w18, [x2, w10, UXTW]
    ldrb w19, [x2, w11, UXTW]
    ldrb w20, [x2, w12, UXTW]

    // copy to vector reg
    ins v0.8b[0], w13
    ins v0.8b[1], w14
    ins v0.8b[2], w15
    ins v0.8b[3], w16

    ins v0.8b[4], w17
    ins v0.8b[5], w18
    ins v0.8b[6], w19
    ins v0.8b[7], w20

    // sign extend long, convert int8_t to int16_t
    sxtl v0.8h, v0.8b

    // multiplier[i]
    ld1 {v1.8h}, [x4], #16

    // multiply
    smull  v2.4s, v1.4h, v0.4h
    smull2 v3.4s, v1.8h, v0.8h

    // saturating shift right
    sqshrn  v2.4h, v2.4s, #3
    sqshrn2 v2.8h, v3.4s, #3

    st1 {v2.8h}, [x0], #16

    subs w1, w1, #1
    b.gt 1b

    ldp x19, x20, [sp], #16

    ret
</code></pre>
</details>
<p>Lets’s benchmark both implementations and see what the results are:</p>
<pre><code>modulo_scheduling_example_c_01 - runtime: 1.598776 s
modulo_scheduling_example_asm_01 - runtime: 1.492209 s
</code></pre>
<p>The assembly implementation is faster, which is great, but lets see how we can
do better. We could take this a step further and employ modulo scheduling. This
increases the complexity of the implementation, but for hot functions which
execute many iterations, this may be worth it. In order to make this work,
we have to prime the loop with a prologue section:</p>
<pre><code>A1
B1 A2
C1 B2 A3
</code></pre>
<p>Then we run a steady state loop:</p>
<pre><code>D1 C2 B3 A4
</code></pre>
<p>And finally an epilogue to complete the iterations:</p>
<pre><code>   D2 C3 B4
      D3 C4
         D4
</code></pre>
<p>In order to make this work, each step must consume all of the outputs of the
previous step and not use as temporary registers any of the inputs or outputs of
other steps. For this example, step A loads the value of <code>multiplier[i]</code> but it
is not consumed until step C, so in step B we simply copy its value with a <code>mov</code>
instruction. This implementation could look like this:</p>
<details>
<summary>
See the implementations of each step whicih are the same as above,
(hidden for brevity).
</summary>
<pre><code>.macro modulo_scheduling_example_asm_step_A
    // src_pos = src_positions[i]
    ldp w5, w6, [x3]
    ldp w7, w8, [x3, 8]
    ldp w9, w10, [x3, 16]
    ldp w11, w12, [x3, 24]
    add x3, x3, #32

    // multiplier[i]
    ld1 {v5.8h}, [x4], #16
.endm

.macro modulo_scheduling_example_asm_step_B
    mov v1.16b, v5.16b

    // src[src_pos]
    ldrb w13, [x2,  w5, UXTW]
    ldrb w14, [x2,  w6, UXTW]
    ldrb w15, [x2,  w7, UXTW]
    ldrb w16, [x2,  w8, UXTW]

    ldrb w17, [x2,  w9, UXTW]
    ldrb w18, [x2, w10, UXTW]
    ldrb w19, [x2, w11, UXTW]
    ldrb w20, [x2, w12, UXTW]

    // copy to vector reg
    ins v0.8b[0], w13
    ins v0.8b[1], w14
    ins v0.8b[2], w15
    ins v0.8b[3], w16

    ins v0.8b[4], w17
    ins v0.8b[5], w18
    ins v0.8b[6], w19
    ins v0.8b[7], w20
.endm

.macro modulo_scheduling_example_asm_step_C
    // sign extend long, convert int8_t to int16_t
    sxtl v0.8h, v0.8b

    // multiply
    smull  v2.4s, v1.4h, v0.4h
    smull2 v3.4s, v1.8h, v0.8h

    // saturating shift right
    // val = min(val &gt;&gt; 3, (1 &lt;&lt; 15) - 1)
    sqshrn  v4.4h, v2.4s, #3
    sqshrn2 v4.8h, v3.4s, #3
.endm

.macro modulo_scheduling_example_asm_step_D
    // step D
    // destination[i] = val
    st1 {v4.8h}, [x0], #16
.endm
</code></pre>
</details>
<pre><code>.global modulo_scheduling_example_asm_02
modulo_scheduling_example_asm_02:
    cmp w1, #4
    b.lt modulo_scheduling_example_asm_01

    stp x19, x20, [sp, #-16]!

    modulo_scheduling_example_asm_step_A // A1
    modulo_scheduling_example_asm_step_B // B1
    modulo_scheduling_example_asm_step_A // A2
    modulo_scheduling_example_asm_step_C // C1
    modulo_scheduling_example_asm_step_B // B2
    modulo_scheduling_example_asm_step_A // A3

1:  //// loop ////
    modulo_scheduling_example_asm_step_D
    modulo_scheduling_example_asm_step_C
    modulo_scheduling_example_asm_step_B
    modulo_scheduling_example_asm_step_A

    sub w1, w1, #1
    cmp w1, #3
    b.gt 1b
    //// end loop ////

    modulo_scheduling_example_asm_step_D // DN-2
    modulo_scheduling_example_asm_step_C // CN-1
    modulo_scheduling_example_asm_step_B // BN
    modulo_scheduling_example_asm_step_D // DN-1
    modulo_scheduling_example_asm_step_C // CN
    modulo_scheduling_example_asm_step_D // DN

    // restore stack
    ldp x19, x20, [sp], #16
    ret
</code></pre>
<p>If we benchmark these functions on Graviton3, this is the result:</p>
<pre><code>modulo_scheduling_example_c_01 - runtime: 1.598776 s
modulo_scheduling_example_asm_01 - runtime: 1.492209 s
modulo_scheduling_example_asm_02 - runtime: 1.583097 s
</code></pre>
<p>It’s still faster than the C implementation, but it’s slower than our first
pass. Let’s add another trick. If we check the software optimization guide for
Graviton2, the Neoverse N1, we see that the <code>ins</code> instruction has a whopping
5-cycle latency with only 1 instruction per cycle of total throughput. There are
8 <code>ins</code> instructions which will take at least 13 cycles to complete, the whole
time tying up vector pipeline resources.</p>
<p>If we instead outsource that work to the load and store pipelines, we can leave
the vector pipelines to process the math instructions in step C. To do this,
after each byte is loaded, we write them back to memory on the stack with <code>strb</code>
instructions to lay down the bytes in the order that we need them to be in the
vector register. Then in the next step we use a ld1 instruction to put those
bytes into a vector register. This approach probably has more latency (but I
haven’t calculated it), but since the next step which consumes the outputs is
three steps away, it leaves time for those instructions to complete before we
need those results. For this technique, we revise only the macros for steps B
and C:</p>
<pre><code>.macro modulo_scheduling_example_asm_03_step_B
    mov v4.16b, v5.16b

    strb w13, [sp, #16]
    strb w14, [sp, #17]
    strb w15, [sp, #18]
    strb w16, [sp, #19]

    strb w17, [sp, #20]
    strb w18, [sp, #21]
    strb w19, [sp, #22]
    strb w20, [sp, #23]
.endm

.macro modulo_scheduling_example_asm_03_step_C
    mov v1.16b, v4.16b
    ldr d0, [sp, #16]
.endm
</code></pre>
<p>This technique gets the best of both implementations and running the benchmark
again produces these results:</p>
<pre><code>modulo_scheduling_example_c_01 - runtime: 1.598776 s
modulo_scheduling_example_asm_01 - runtime: 1.492209 s
modulo_scheduling_example_asm_02 - runtime: 1.583097 s
modulo_scheduling_example_asm_03 - runtime: 1.222440 s
</code></pre>
<p>Nice. Now we have the fastest implementation yet with a 24% speed improvement.
But wait! What if the improvement is only from the trick where we save the
intermediate values on the stack in order to get them into the right order and
modulo scheduling isn’t helping. Let’s benchmark the case where we use this
trick without modulo scheduling.</p>
<pre><code>modulo_scheduling_example_asm_04 - runtime: 2.106044 s
</code></pre>
<p>Ooof. That’s the slowest implementation yet. The reason this trick works is
because it divides work up between vector and memory pipelines and stretches out
the time until the results are required to continue the loop. Only when we do
both of these things, do we achieve this performance.</p>
<p>I didn’t mention SVE gather load instructions on purpose. They would most
definitely help here, and we will explore that in the future.</p>
<h1 id="test-everything"><a class="header" href="#test-everything">Test Everything</a></h1>
<p>Test everything and make no assumptions. When you are writing hand optimized
assembly or intrinsics to achieve the best performance, speed can be impacted in
surprising and unintuitive ways even for experienced assembly developers, so
it’s important to create micro benchmarks for every function that you work on.
It’s also important to consider how the function will be called. For example, a
kernel in a video encoder for computing the sum-absolute-difference of two
frames of video will be called repeatedly in a tight loop with similar or
identical arguments which will give the branch predictor and instruction caches
plenty of opportunity to train themselves to peak performance. Microbenchmarks
to test these types of functions in loops with fixed arguments are very useful
to measure performance. However, something like mempcy is called on irregular
intervals possibly with different arguments for each call. Benchmarking a
function like this in a tight loop may lead to misleading results since a tight
loop will over-train the branch predictor leading the same path being taken each
time, whereas in real workloads data dependent branches will incur a more
significant penalty. To properly benchmark memcpy we would need to benchmark it
with a distribution of length arguments similar to what you expect in a
production workload, which will vary widely.</p>
<p>There is no one-size-fits all approach for benchmarking, so as you design your
test harness, consider how the function will be used and design accordingly. You
may even want several types of microbenchmarks to evaluate changes to your
function across different conditions. Most functions written in assembly should
avoid the need for knowledge about an external structure so the author does not
have to make assumptions about the layout of memory which could be changed by
the compiler. This also makes it easier to extract these functions as small
kernels for testing separately from a large application in a standalone test
harness. A standalone test harness is very convenient, since it is likely to
build much more quickly than a large application the assembly function is a part
of, making iterative development quicker. In addition to testing a wide range of
input conditions, which is the case for any function in any language, assembly
functions should also be tested to ensure they do not violate the calling
convention. For example, some registers should not be modified, or if they are,
their contents should be spilled to the stack and reloaded before returning. For
some functions, especially if the input affects locations of memory
dereferencing, it is necessary to fuzz the function by generating random input
arguments, or all permutations, if it is feasible.</p>
<p>While testing is always important in software development, it is especially
important for assembly programming. Choose a test method that works for your case
and make thorough use of it.</p>
<h2 id="specialize-functions-for-known-input-conditions"><a class="header" href="#specialize-functions-for-known-input-conditions">Specialize functions for known input conditions</a></h2>
<p>Functions that are important enough to optimize with assembly often take very
predictable or specific code paths that are the same every time they are called.
For example, a hot function written in C may take an argument which specifies an
array length which the C will use in a loop termination condition. The C may be
written to accept any possible input for that length. However when we write the
function in assembly, it can be advantageous to make assumptions about the
arguments, such as “it will always be divisible by 2, 4, 8, or 16.” We can
handle these cases by checking if any of these assumptions is true at the top of
the function and then branch to a specialized portion of the function. Then the
specialized assembly can omit more general code to handle any possible input and
make use of loop unrolling or SIMD instructions which divide evenly into the
array length.</p>
<p>Another technique is to call hot functions through a function pointer. Before
setting the function pointer, if the input arguments can be verified to always
satisfy some conditions, like the length is divisible 16 and always non-zero, we
can set the pointer to an assembly function which is optimized based on those
assumptions. Authors of video codecs will be very familiar with this technique.
In fact,
<a href="https://github.com/FFmpeg/FFmpeg/blob/master/libswscale/swscale.h">many</a>
<a href="https://bitbucket.org/multicoreware/x265_git/src/master/source/common/aarch64/asm-primitives.cpp">encoders</a>
have a great deal of infrastructure just to dispatch work to specialized
functions which match the input conditions.</p>
<p>Here is an example:</p>
<pre><code>int32_t sum_all(int8_t *values, int length)
{
   int32_t sum = 0;
   for (int i = 0; i &lt; length; i++)
      sum += values[i];
   return sum;
}
</code></pre>
<p>If this is only ever called when length is divisible by 16 and non-zero, we can
implement in assembly like this:</p>
<pre><code>// x0 - int8_t *values
// w1 - int length
sum_all_x16_asm:
    movi v2.4s, #0           // zero out v2 for use as an accumulator
1:  ld1 {v0.16b}, [x0], #16  // load 16 bytes from values[i]
    saddlp v1.8h, v0.16b     // pairwise-add-long (A, B, C, D, ...) =&gt; (A+B, C+D, ...)
    sadalp v2.4s, v1.8h      // pairwise-add-accumulate-long (same, but add to v2)
    subs w1, w1, #16         // check the loop counter
    b.gt 1b                  // branch back if more iterations remain
    addv s0, v2.4s           // add-across the accumulator
    fmov w0, s0              // copy to general purpose register
    ret
</code></pre>
<p>In this implementation, we can skip the initial check to see if length is zero
and we can skip any checks for any unconsumed values if length was not divisible
by 16. A more complex function may be able to be unrolled farther or make more
assumptions about type conversions which are knowable based on the inputs, which
can improve execution speed even more.</p>
<h2 id="use-efficient-instructions"><a class="header" href="#use-efficient-instructions">Use Efficient Instructions</a></h2>
<p>Make yourself aware of the various types of instructions available to use.
Choosing the right instructions can make the difference between a 5 or 10
instructions sequence or just one instruction that does exactly what you need.
For example, saturating arithmetic instructions can replace a series of
instructions with just one. A rounding-shift-right-narrow can compute an average
of two integers with just two instructions instead of three with a truncating
right shift and in fewer cycles than the very expensive divide instruction.</p>
<p>One way this is especially true is the use of vector or instructions. We have
already mentioned Neon instructions in this document, but to make it clearer,
these instructions operation on multiple data lanes in parallel. They are also
known as SIMD or Single Instruction Multiple Data instructions. The full details
on how to use this type of programming is beyond the scope of this guide, but
when it’s possible to process 16 bytes in parallel instead of just one at a
time, the speed up can be significant.</p>
<p>Another way to use efficient instructions is to consult the <a href="README.html#building-for-graviton2-graviton3-and-graviton3e">software
optimization guide</a>.
Many different combinations of instructions can accomplish the same result, and
some are obviously better than others. Some instructions, like the
absolute-difference-accumulate-long instructions (<code>sabal</code> and <code>uabal</code>) can only
be executed on half of the vector pipelines in Neoverse V1, which slashes the
overall throughput of a series of independent instructions to two per cycle.
Using instead a series of absolute-difference instructions (<code>sabd</code> or <code>uabd</code>),
which can execute on all four pipelines, can increase the parallelism. If there
are enough of these instructions, this is worth the trade off of requiring a
separate step to add up the accumulating absolute difference. There are other
instructions which this applies to, so consult the software optimization guide
to see what the throughput and latency numbers are for the instructions you are
using.</p>
<details>
<summary>
Expand to see a code sample using the two different approaches for calculating
sum-absolute-difference.
</summary>
The first function in this example uses `uabal` which is limited to half of the
NEON execution units. The second uses more instructions to complete the final
sum, but since the instructions can execute on all NEON execution units, it runs
25% faster on Graviton3.
<p>This is a C implementation of the functions below for illustration.</p>
<pre><code>int sum_absolute_difference_c(uint8_t *a, uint8_t *b, int h)
{
    int s = 0;
    for (int i = 0; i &lt; h*64; i++) {
        s += abs(a[i] - b[i]);
    }
    return s;
}
</code></pre>
<pre><code>.text
.global sad_asm_01
.global sad_asm_02

sad_asm_01:
    movi    v18.2d, #0                      // write zeros to the accumulation register
1:
    ld1     {v0.16b-v3.16b}, [x0], #64      // Load 64 bytes from uint8_t *a
    ld1     {v4.16b-v7.16b}, [x1], #64      // Load 64 bytes from uint8_t *b
    uabdl   v16.8h, v0.8b,  v4.8b           // sad long 0-7 (clears v16)
    uabal2  v16.8h, v0.16b, v4.16b          // sad accumulate long 8-15
    uabal   v16.8h, v1.8b,  v5.8b           // sad accumulate long 16-23
    uabal2  v16.8h, v1.16b, v5.16b          // sad accumulate long 24-31
    uabal   v16.8h, v2.8b,  v6.8b           // sad accumulate long 31-39
    uabal2  v16.8h, v2.16b, v6.16b          // sad accumulate long 40-47
    uabal   v16.8h, v3.8b,  v7.8b           // sad accumulate long 48-57
    uabal2  v16.8h, v3.16b, v7.16b          // sad accumulate long 58-63
    uadalp  v18.4s, v16.8h                  // add accumulate pairwise long into the 32 bit accumluator
    subs    w2, w2, #1                      // decrement the loop counter
    b.gt    1b                              // branch to the top of the loop

    addv    s0, v18.4s                      // add across vector to add up the 32 bit values in the accumlutor (v18)
    fmov    w0, s0                          // move vector register s0 to 32 bit GP register, w0 
    ret



sad_asm_02:
    movi    v21.2d, #0                      // write zeros to the accumulation registers
1:
    ld1     {v0.16b-v3.16b}, [x0], #64      // Load 64 bytes from uint8_t *a
    ld1     {v4.16b-v7.16b}, [x1], #64      // Load 64 bytes from uint8_t *b

    uabd    v17.16b, v0.16b, v4.16b         // sad 0-15
    uabd    v18.16b, v1.16b, v5.16b         // sad 16-31
    uabd    v19.16b, v2.16b, v6.16b         // sad 32-47
    uabd    v20.16b, v3.16b, v7.16b         // sad 48-63

    uaddlp  v17.8h, v17.16b                 // add long pairwise each of the result registers from above
    uaddlp  v18.8h, v18.16b                 // after these four instructions, we have converted 64 8-bit values to
    uaddlp  v19.8h, v19.16b                 // 32 16-bit sums
    uaddlp  v20.8h, v20.16b

    addp    v17.8h, v17.8h, v18.8h          // add pairwise again converting 32 16-bit values to 16 16-bit values
    addp    v18.8h, v19.8h, v20.8h          // second step to get to 16 values
    addp    v17.8h, v17.8h, v18.8h          // add pairwise again, converting 16 16-bit values to 8 16-bit values
    uaddlp  v17.4s, v17.8h                  // add pairwise long, converting 8 16-bit values to 4 32-bit values
    add     v21.4s, v21.4s, v17.4s          // accumulate these 4 values into v21 for the next iteration

    subs    w2, w2, #1                      // decrement the loop counter
    b.gt    1b                              // branch to the top of the loop

    addv    s0, v21.4s                      // add across vector to add up the 32 bit values in the accumluator (v21)
    fmov    w0, s0                          // move vector register s0 to the 32 bit GP register, w0
    ret
</code></pre>
<p>If the second approach is faster, why would we ever use <code>sabal</code> or <code>uabal</code>?
There is a trade off, of course. As you can see in the example, it takes more
instructions to avoid using <code>uabal</code> because you have to compute the accumulation
another way. It works well in this example because we can parallelize the SAD
computation across 64 bytes. If the amount of loop unrolling was more limited
(maybe because a result from the previous iteration is needed in the next), we
might not be able to parallelize enough SAD computations to keep 4 execution
units full. In that case it would probably make sense to use <code>uabal</code> since we
would already be unable to fill up the queues of 4 execution units.</p>
</details>
<p>Check to see what the compiler or several different compilers produce to see if
it can teach you any tricks. Sometimes just writing a short segment of code in C
in Compiler Explorer and seeing what the latest versions of GCC and Clang
produce with various levels of optimization can reveal tricks which you may not
have thought of. This doesn’t always produce useful results, especially since
writing in assembly suggests that the compiler’s output was not good enough, but
it can be a good way to explore ideas for implementing short segments of code.</p>
<p>Choosing the right instructions in an efficient way is something that takes
practice and familiarity with the instructions available in the ISA. It takes
time to get familiar with the broad array of options available to you as the
programmer and as an experienced engineer, you can produce some impressive
optimized code.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="monitoring-tools-for-aws-graviton"><a class="header" href="#monitoring-tools-for-aws-graviton">Monitoring tools for AWS Graviton</a></h1>
<p>Listed below are some monitoring and profiling tools supported on AWS Gravtion. Also listed are some differences when compared to the tools available on x86 processor architectures.</p>
<p>Some of the most commonly used tools such as <em>top, htop, iostat, lstopo, hwloc, dmidecode, lmbench, Linux perf</em> are supported on AWS Graviton processors. There are some tools such as Intel MLC, Intel VTune Profiler, PCM that are supported only on Intel processors and some tools such as <em>turbostat</em> supported on x86 processors.</p>
<h2 id="details"><a class="header" href="#details">Details</a></h2>
<h3 id="info-utilities"><a class="header" href="#info-utilities">Info utilities</a></h3>
<h4 id="lscpu-utility"><a class="header" href="#lscpu-utility"><em>lscpu</em> utility</a></h4>
<p>The <em>lscpu</em> utility shows details of the processor features such as architecture, number of cores, active CPUs, caches, NUMA, instruction support and optionally CPU frequency.</p>
<p><em>$ lscpu</em></p>
<div class="table-wrapper"><table><thead><tr><th>Attribute</th><th>Value</th></tr></thead><tbody>
<tr><td>Architecture</td><td>aarch64</td></tr>
<tr><td>CPU(s)</td><td>64</td></tr>
<tr><td>On-line CPU(s) list</td><td>0-63</td></tr>
<tr><td>L1d</td><td>4 MiB (64 instances)</td></tr>
<tr><td>...</td><td>...</td></tr>
<tr><td>NUMA node(s)</td><td>1</td></tr>
<tr><td>...</td><td>...</td></tr>
</tbody></table>
</div>
<h4 id="dmidecode-to-get-cpu-frequency-info"><a class="header" href="#dmidecode-to-get-cpu-frequency-info"><em>dmidecode</em> to get CPU frequency info</a></h4>
<p>The <em>dmidecode</em> utility is a tool for listing details of the system's hardware components.</p>
<p><em>$ sudo dmidecode | less</em></p>
<div class="table-wrapper"><table><thead><tr><th>Attribute</th><th>Value</th></tr></thead><tbody>
<tr><td>Socket Designation</td><td>CPU00</td></tr>
<tr><td>Type</td><td>Central Processor</td></tr>
<tr><td>Family</td><td>ARMv8</td></tr>
<tr><td>Manufacturer</td><td>AWS</td></tr>
<tr><td>...</td><td>...</td></tr>
<tr><td>Max Speed</td><td>2600 MHz</td></tr>
<tr><td>Current Speed</td><td>2600 MHz</td></tr>
<tr><td>...</td><td>...</td></tr>
</tbody></table>
</div>
<h4 id="aws-api-to-get-cpu-frequency-information"><a class="header" href="#aws-api-to-get-cpu-frequency-information">AWS API to get CPU frequency information</a></h4>
<p>The AWS EC2 API also allows to query an instance type processor maximum frequency.</p>
<p>Below is an example using the AWS CLI to query the processor frequency of a Graviton3-based c7g.4xlarge:</p>
<pre><code>$ aws ec2 describe-instance-types --instance-types c7g.4xlarge --query "InstanceTypes[].{InstanceType:InstanceType,SustainedClockSpeedInGhz:ProcessorInfo.SustainedClockSpeedInGhz}" --output json
[
    {
        "InstanceType": "c7g.4xlarge",
        "SustainedClockSpeedInGhz": 2.6
    }
]
</code></pre>
<h4 id="hwloc-and-lstopo-utilities"><a class="header" href="#hwloc-and-lstopo-utilities"><em>hwloc</em> and <em>lstopo</em> utilities</a></h4>
<p>Shown below is sample output for these utilities on a c6g.2xlarge instance.</p>
<p><em>$ hwloc-info</em></p>
<pre><code>depth 0:           1 Machine (type #0)
    depth 1:          1 Package (type #1)
        depth 2:         1 L3Cache (type #6)
            depth 3:        8 L2Cache (type #5)
                depth 4:       8 L1dCache (type #4)
                    depth 5:      8 L1iCache (type #9)
        ...
</code></pre>
<p><em>$ lstopo</em></p>
<pre><code>Machine (15GB total)
    Package L#0
        NUMANode L#0 (P#0 15GB)
            L3 L#0 (32MB)
            L2 L#0 (1024KB) + L1d L#0 (64KB) + L1i L#0 (64KB) + Core L#0 + PU L#0 (P#0)
            L2 L#1 (1024KB) + L1d L#1 (64KB) + L1i L#1 (64KB) + Core L#1 + PU L#1 (P#1)
            L2 L#2 (1024KB) + L1d L#2 (64KB) + L1i L#2 (64KB) + Core L#2 + PU L#2 (P#2)
    ...
</code></pre>
<h3 id="perf-monitoring-utilities"><a class="header" href="#perf-monitoring-utilities">Perf monitoring utilities</a></h3>
<p>On AWS Graviton processors, the <strong>Linux perf</strong> tool comes handy to collect an application execution profile, hardware perf counters. Much of the functionality provided by tools such as Intel <em>VTune Profiler</em> and <em>PCM</em> are supported in <em>Linux perf</em>. Below are some details of its usage/ syntax.</p>
<h4 id="collect-basic-cpu-statistics-for-the-specified-command-or-system-wide"><a class="header" href="#collect-basic-cpu-statistics-for-the-specified-command-or-system-wide">Collect basic CPU statistics for the specified command or system wide</a></h4>
<p><em>$ perf stat command</em></p>
<p>Shown below are <em>Linux perf</em> stats collected at system wide on a c6g.2xlarge instance.</p>
<p><em>$ perf stat</em></p>
<p>Performance counter stats for 'system wide':</p>
<pre><code>      87692.26 msec cpu-clock                 #    8.000 CPUs utilized
           441      context-switches          #    5.029 /sec
            13      cpu-migrations            #    0.148 /sec
             2      page-faults               #    0.023 /sec
      25115021      cycles                    #    0.000 GHz
      28853592      instructions              #    1.15  insn per cycle
         68126      branch-misses

  10.961122204 seconds time elapsed
</code></pre>
<h4 id="collect-basic-specific-cpu-hardware-counters-for-a-specific-command-or-system-wide-for-10-seconds"><a class="header" href="#collect-basic-specific-cpu-hardware-counters-for-a-specific-command-or-system-wide-for-10-seconds">Collect basic/ specific CPU hardware counters, for a specific command or system wide, for 10 seconds</a></h4>
<p>One can collect hardware events/ counters for an application, on a specific CPU, for a PID or system wide as follows:</p>
<p><em>$ perf stat -e cycles,instructions,cache-references,cache-misses,bus-cycles -a sleep 10</em></p>
<p>Performance counter stats for 'system wide':</p>
<pre><code>     161469308      cycles                                                        (80.01%)
     120685678      instructions              #    0.75  insn per cycle           (79.97%)
      42132948      cache-references                                              (80.01%)
       2001520      cache-misses              #    4.750 % of all cache refs      (80.02%)
     160016796      bus-cycles                                                    (60.00%)

  10.002896494 seconds time elapsed
</code></pre>
<h4 id="view-the-profile-using-perf-report-command"><a class="header" href="#view-the-profile-using-perf-report-command">View the profile using perf report command</a></h4>
<p><em>$ perf report</em>
|Overhead  |   Command  |       Shared Object   |       Symbol|
|---	|---	|---	|---    |
|72.44%   |       functionA |       functionA       |       classA::functionA|
|7.66%    |       functionB |       libB.so       |       classB::functionB|
|...      |                 |                       |                 |
|0.81%    |       functioA  |       libc-2.31.so    |       memcmp|</p>
<p>More details on how to use Linux perf utility on AWS Graviton processors is available <a href="https://github.com/aws/aws-graviton-getting-started/blob/main/optimizing.md#profiling-the-code">here</a>.</p>
<h2 id="summary-utilities-on-aws-graviton-vs-intel-x86-architectures"><a class="header" href="#summary-utilities-on-aws-graviton-vs-intel-x86-architectures">Summary: Utilities on AWS Graviton vs. Intel x86 architectures</a></h2>
<p>|Processor	|x86	|Graviton2,3, and 4	|
|---	|---	|---	|
|CPU frequency listing	|<em>lscpu, /proc/cpuinfo, dmidecode</em>	|<em>dmidecode</em>	|
|<em>turbostat</em> support	|Yes	|No	|
|<em>hwloc</em> support	|Yes	|Yes	|
|<em>lstopo</em> support	|Yes	|Yes	|
|<em>i7z</em> Works	|Yes	|No	|
|<em>lmbench</em>	|Yes	|Yes	|
|Intel <em>MLC</em>  |Yes    |No     |
|Performance monitoring tools	|<em><a href="https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html">VTune Profiler</a>, <a href="https://github.com/opcm/pcm">PCM</a>, <a href="https://www.brendangregg.com/perf.html">Linux perf</a>, <a href="https://github.com/aws/aperf">APerf</a></em>	|<em><a href="https://www.brendangregg.com/perf.html">Linux perf</a>, <a href="https://www.linaroforge.com/">Linaro Forge</a>, <a href="https://developer.arm.com/Tools%20and%20Software/Streamline%20Performance%20Analyzer">Arm Streamline CLI Tools</a>, <a href="https://github.com/aws/aperf">APerf</a></em>	|</p>
<p>Utilities such as <em>lmbench</em> are available <a href="http://lmbench.sourceforge.net/">here</a> and can be built for AWS Graviton processors to obtain latency and bandwidth stats.</p>
<p><strong>Notes</strong>:</p>
<p><strong>1.</strong> The ARM Linux kernel community has decided not to put CPU frequency in <em>/proc/cpuinfo</em> which can be read by tools such as <em>lscpu</em> or directly.</p>
<p><strong>2.</strong> On AWS Graviton processors, Turbo isn’t supported. So, utilities such as ‘turbostat’ aren’t supported/ relevant. Also, tools such as <em><a href="https://code.google.com/archive/p/i7z/">i7z</a></em> for discovering CPU frequency, turbo, sockets and other information are only supported on Intel architecture/ processors. Intel <em>MLC</em> is a memory latency checker utility that is only supported on Intel processors.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="how-to-resources"><a class="header" href="#how-to-resources">How to Resources</a></h1>
<p>If you are just getting started with AWS Graviton-based instances, or even if you have been using AWS Graviton for some time, below is a list of some of the tech postings from around the AWS community. The list includes blog postings, tech talks and some recent re:Invent sessions as well. It also covers topics including running applications on AWS, AI/ML, cost optimization, sustainability, and HPC.</p>
<h3 id="recent-events"><a class="header" href="#recent-events">Recent Events</a></h3>
<h4 id="aws-reinvent-2023"><a class="header" href="#aws-reinvent-2023"><a href="https://reinvent.awsevents.com/">AWS re:Invent 2023</a></a></h4>
<ul>
<li><a href="https://www.youtube.com/watch?v=9JZVomrx6uQ">CMP404: Migrating to AWS Graviton with AWS container services</a></li>
<li><a href="https://www.youtube.com/watch?v=9W0j__k5afg">CMP315: Migrating critical business applications to AWS Graviton with ease</a></li>
<li><a href="https://www.youtube.com/watch?v=T_hMIjKtSr4">CMP334: The best price performance for your AWS workloads (Graviton4 deep dive)</a></li>
</ul>
<h3 id="development-tools"><a class="header" href="#development-tools">Development Tools</a></h3>
<ul>
<li>Performance Analysis - <a href="https://github.com/aws/aperf">APerf</a></li>
<li>Source Code Analysis - <a href="https://github.com/aws/porting-advisor-for-graviton">Porting Advisor for Graviton</a></li>
</ul>
<h3 id="building--running-applications-on-aws-graviton"><a class="header" href="#building--running-applications-on-aws-graviton">Building / Running applications on AWS Graviton</a></h3>
<ul>
<li>Tech Talk - <a href="https://www.youtube.com/watch?v=yAf6-A8Zso4">AWS Graviton and EC2 - a bit of history</a></li>
<li>Blog - <a href="https://aws.amazon.com/blogs/devops/multi-architecture-container-builds-with-codecatalyst/">Multi-Architecture Container Builds with CodeCatalyst</a></li>
<li>Tech Talk - <a href="https://www.youtube.com/watch?v=ysmvoO4DgB8">Package management with Graviton</a></li>
<li>Tech Talk - <a href="https://youtu.be/mzDlszhJetI">AMIs for Graviton - Operating systems supporting Graviton, how to make the best choice</a></li>
<li>Twitch - <a href="https://www.linkedin.com/video/live/urn:li:ugcPost:7036455289579077633/">How to migrate your first application to Graviton</a></li>
<li>Blog - <a href="https://aws.amazon.com/cn/blogs/devops/unlock-the-power-of-ec2-graviton-with-gitlab-ci-cd-and-eks-runners/">Unlock the power of EC2 Graviton with GitLab CI/CD and EKS Runners</a></li>
<li>Twitch - <a href="https://www.twitch.tv/videos/1876233449?collection=ZbDCkIlpDhemjg">Measuring Performance - How to measure performance on Graviton based instances</a></li>
</ul>
<h3 id="cost-optimization"><a class="header" href="#cost-optimization">Cost Optimization</a></h3>
<ul>
<li>Virtual Workshop - <a href="https://www.youtube.com/watch?v=BfiEEx8k1lQ">Optimizing Cost with AWS Graviton Based Services</a></li>
<li>Blog - <a href="https://aws.amazon.com/blogs/aws-cloud-financial-management/optimize-aws-costs-without-architectural-changes-or-engineering-overhead/">Optimize AWS costs without architectural changes or engineering overhead</a></li>
<li>Tech Talk - <a href="https://pages.awscloud.com/Optimize-your-workloads-no-architectural-changes-needed_2023_0301-OTT-OD-NGI_OD">Optimize your workloads, no architectural changes needed</a></li>
</ul>
<h3 id="sustainability"><a class="header" href="#sustainability">Sustainability</a></h3>
<ul>
<li>Tech Talk - <a href="https://www.youtube.com/watch?v=TmHIROOQ1Mc">Building Sustainable Infrastructure with EC2 Graviton</a></li>
<li>Tech Talk - <a href="https://www.youtube.com/watch?v=pzSvcsduijM">How to optimize workloads for sustainability using AWS Graviton-based EC2 instances</a></li>
</ul>
<h3 id="programming-languages"><a class="header" href="#programming-languages">Programming Languages</a></h3>
<ul>
<li>Rust - Blog - <a href="https://community.aws/tutorials/building-rust-applications-for-aws-graviton">Building Rust Applications For AWS Graviton</a></li>
<li>.Net - Blog - <a href="https://aws.amazon.com/blogs/dotnet/net-workflows-for-arm64-with-codecatalyst-part-1/">.NET Workflows for arm64 with Amazon CodeCatalyst: Part 1</a></li>
<li>.Net - Blog - <a href="https://aws.amazon.com/blogs/dotnet/net-workflows-for-arm64-with-codecatalyst-part-2/">.NET Workflows for arm64 with Amazon CodeCatalyst: Part 2</a></li>
<li>Java - Tech Talk - <a href="https://www.youtube.com/watch?v=zANOBN4jZfI">Java on Graviton - How to use Corretto</a></li>
<li>Python - Twitch - <a href="https://www.twitch.tv/videos/1888177585?collection=ZbDCkIlpDhemjg">How to develop and build a python module with native extension for a multi-architecture deployment</a></li>
</ul>
<h3 id="hpc-2"><a class="header" href="#hpc-2">HPC</a></h3>
<ul>
<li>Blog - <a href="https://aws.amazon.com/blogs/hpc/application-deep-dive-into-the-graviton3e-based-amazon-ec2-hpc7g-instance/">Application deep-dive into the AWS Graviton3E-based Amazon EC2 Hpc7g instance</a></li>
</ul>
<h3 id="aiml"><a class="header" href="#aiml">AI/ML</a></h3>
<ul>
<li>Blog - <a href="https://aws.amazon.com/blogs/machine-learning/optimized-pytorch-2-0-inference-with-aws-graviton-processors/">Optimized PyTorch 2.0 inference with AWS Graviton processors</a></li>
<li>Blog - <a href="https://aws.amazon.com/blogs/machine-learning/reduce-amazon-sagemaker-inference-cost-with-aws-graviton/">Reduce Amazon SageMaker inference cost with AWS Graviton</a></li>
<li>Tech Talk - <a href="https://www.youtube.com/watch?v=c1Rl-vCmnT0">Deep Dive: PyTorch 2.0 on AWS Graviton</a></li>
<li>Blog - <a href="https://www.anaconda.com/blog/implementing-a-full-ml-lifecycle-with-anaconda-distribution-on-aws-graviton">Implementing a Full ML Lifecycle with Anaconda Distribution on AWS Graviton</a></li>
</ul>
<h3 id="porting-advisor-for-aws-graviton"><a class="header" href="#porting-advisor-for-aws-graviton">Porting Advisor for AWS Graviton</a></h3>
<ul>
<li>Twitch - <a href="https://www.twitch.tv/videos/1822190104">Porting Advisor for Graviton</a></li>
<li>Blog -  <a href="https://aws.amazon.com/blogs/compute/using-porting-advisor-for-graviton/">Using Porting Advisor for Graviton</a></li>
</ul>
<h3 id="databases"><a class="header" href="#databases">Databases</a></h3>
<ul>
<li>Blog - <a href="https://aws.amazon.com/blogs/database/choose-aws-graviton-and-cloud-storage-for-your-ethereum-nodes-infrastructure-on-aws/">Choose AWS Graviton and cloud storage for your Ethereum nodes infrastructure on AWS</a></li>
</ul>
<h3 id="customer-stories"><a class="header" href="#customer-stories">Customer Stories</a></h3>
<ul>
<li>Datadog - <a href="https://www.youtube.com/watch?v=bbchHOFVUuY">What to know before adopting Arm: Lessons learned at Datadog</a></li>
<li>Aerospike - <a href="https://www.youtube.com/watch?v=-9ul3j-fBpU">Building real-time applications to utilize AWS Graviton</a></li>
<li>Tehama - <a href="https://aws.amazon.com/blogs/industries/tehama-leverages-graviton-cost-efficiency-to-strengthen-business-core-competency/">Tehama leverages Graviton cost efficiency to strengthen business core competency</a></li>
<li>Zomato - <a href="https://aws.amazon.com/blogs/opensource/how-zomato-boosted-performance-25-and-cut-compute-cost-30-migrating-trino-and-druid-workloads-to-aws-graviton/">How Zomato Boosted Performance 25% and Cut Compute Cost 30% Migrating Trino and Druid Workloads to AWS Graviton</a></li>
<li>Stripe - <a href="https://youtu.be/lZkO-KelLnk?t=1858">AWS Graviton deep dive: The best price performance for AWS workloads</a></li>
<li>Snowflake - <a href="https://aws.amazon.com/blogs/apn/how-snowflake-optimized-its-virtual-warehouses-for-sustainability-using-aws-graviton/">How Snowflake Optimized its Virtual Warehouses for Sustainability Using AWS Graviton</a></li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
